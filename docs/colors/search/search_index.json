{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"A Practical Guide to Color Management in Audiovisual Production \u00b6 About us \u00b6 This guide is part of a collection of documents managed by RxLaboratory : RxDocs . RxLaboratory is an organization whose mission is to develop, distribute and promote free software and innovation in audiovisual production, as well as to facilitate mutual aid, sharing and cooperation between all actors in this field. RxDocs is a collaborative work of scientific, technical and artistic popularization in the field of audiovisual production in the broad sense: image, video, animation, sound \u2026 These documents are intended for anyone working in the field (phography, video, motion design, directing, animation, illustration, cinematography, editing \u2026) Note All names followed by an asterisk* are explained in the Glossary Introduction \u00b6 The methods and tools of color management in (digital) image processing often seem complicated and are sometimes misunderstood by people working in audiovisual production. The software are full of different parameters, and which seem to be very different from one to another. As a result, color management is not always mastered; very often the user lose control over colors and can\u2019t achieve the desired look. This field, certainly very technical, is however not as complicated as it seems, and this document aims to explain and vulgarize the problems related to color management in image treatment, and is intended for any person dealing with this problem in the production of digital images1. The goal is not to make theory but to arrive quickly at the concrete application of the explanations, and we will limit the historical and theoretical explanations to the bare minimum. Note Practically, the colorimetric spaces are intrinsically related to the data encoding formats, and we will also speak partly about these various formats. The first part will be theoretical, starting from light and color, and studying how we perceive it, then how to record it, represent it and reproduce it in a digital system. Then we will approach the practical aspect, explaining in concrete cases what are the useful and recommended parameters in your productions of all types, and in different applications. Tip This document is constantly evolving! If you find any errors, or wish to make improvements, contribute or report it to us ! I - Theory \u00b6 A - What is color? A.1 - Physical color: visible light A.2 - Following light : from emission to reception A.2.a - Emission, transmission, reflection, reception A.2.b - Breakdown of the received light B - Small inventory and classification of colors note : short intro on human perception B.1 - Black B.2 - Monochromatic lights B.3 - Purple B.4 - Grey and white B.4.a - Complementary colors B.4.b - Perception C - Perception of light and colors by the human eye B.1 - The eye B.2 - The nervous system B.3 - Implications B.3.a - On brightness and contrast B.3.b - On shading D - Color reproduction D.1 - Additive synthesis D.2 - Subtractive synthesis E - About white values : temperature E.1 - Black body E.2 - Planckian locus E.3 - White balance F - Objective representation of colors F.1 - Decomposing colors F.2 - Color Charts, CIE XYZ of 1931 and CIE xyZ F.3 - Other CIE spaces G - From real life to digital G.1 - Digitization and storage: converting to binary G.2 - The Color Spaces H - What is a color space? H.1 - What defines a color space H.1.a - Primaries and gamut H.1.b - White point H.1.c - Transfer curve H.2 - Other parameters H.2.a - Pixel format H.2.b - Depth H.3 - Why different color spaces? I - List of color spaces J - List of color space parameters K - Pixel format K.1 - RGB and YUV K.1.a - RGB K.1.b - YUV K.1.c - Comparison K.1.d - Others K.2 - YUV 4:4:4, 4:2:2, 4:2:0\u2026 Chrominance downsampling. K.2.a - 4:4:4 K.2.b - 4:2:2 K.2.c - 4:2:0 K.3 - Color depth (bits, bpc and bpp) K.3.a - RGB K.3.b - YUV K.3.c - Others K.4 - Full range / Limited / TV / PC ? K.4.a - Full range / PC K.4.b - Limited range / TV K.4.c - Practical implications K.4.c.a - Encoding K.4.c.b - Playback and display L - Transfer curves, linear space and gamma M - Les LUTs M.1 - Description M.2 - Use N - OpenColorIO and ACES N.1 - Compatible applications N.2 - ACES N.2.a - ACES color space N.2.a.a - ACES2065-1 N.2.a.b - ACEScg N.2.a.c - ACEScc II - Application \u00b6 Hint The list of applications is arranged in alphabetical order, with the publisher\u2019s name at the top. A - Practical application: choosing color spaces and formats A.1 - Theory A.1.a - Journey of a color A.2 - Workspace (scene referred) A.3 - Input A.4 - Display A.4.a - Screen space A.4.b - Settings and color profiles A.4.c - In the application A.5 - Color pickers A.6 - Intermediate output A.7 - Final output B - A few standards C - Screen calibration C.1 - Introduction C.2 - Environment C.3 - Calibration C.3.a - Choosing the color space of the screen and the calibration C.3.b - White point and brightness C.3.c - Calibration and application of the color profile C.3.c.1 - Calibration C.3.c.2 - Colorimetric profile D - Soft-proofing and simulation E - Setting up the production pipeline E.1 - 3D Animation with Blender, Filmic E.1.a - Textures et autres images 2D E.1.b - 3D rendering E.1.c - Compositing E.1.c.a - Filmic and After Effects E.1.d - Exports E.2 - 3D Animation with ACES E.2.a - Textures and other 2D images E.2.b - 3D rendering E.2.c - Compositing E.2.d - Exports E.3 - 2D Animation E.4 - Videos, live action and VFX F - Setting up the production pipeline with OpenColorIO F.1 - Setting up F.2 - Analysis of an OCIO configuration F.2.a - Metadata F.2.b - Roles F.2.c - Displays G - Color management : Adobe After Effects G.1 - Project Settings - Workspace G.2 - Interpr\u00e9tation des m\u00e9trages - Espaces d\u2019Input G.3 - View Options - Display Space and Simulations G.4 - Export Options - Output Spaces G.5 - OCIO G.5.a - Introduction G.5.b - Installing OCIO plugin G.5.c - Disabling color management in After Effects G.5.d - Managing G.5.e - Input and workspace G.5.f - Output G.5.g - Display H - Color management : Adobe Premiere and Media Encoder H.1 - Output space H.2 - Applying a LUT H.3 - Workspace and display I - Color management : Adobe Photoshop J - Color management : Autodesk Maya K - Color management : Blender K.1 - Default configuration : Filmic K.1.a - Render (scene referred) and display K.1.b - Output K.1.c - Input (textures) K.2 - Cha\u00eene de fabrication OCIO K.2.a - Changer la configuration OCIO K.3 - Utiliser ACES K.3.a - Rendu (scene referred) et affichage K.3.b - Input (textures) L - Color management : Darktable L.1 - Input and Workspace L.2 - Output L.3 - Soft-Proofing M - Color management : DuME M.1 - Workspace M.2 - Input M.3 - Output M.4 - Presets M.5 - LUT M.6 - OCIO N - Color management : Krita N.1 - Workspace N.2 - Display N.2.a - Screen N.2.b - Soft-proofing N.3 - Color pickers N.4 - Output N.5 - OCIO with Krita III - Appendixes \u00b6 In a Nutshell: a quick summary of how to get it right Common Errors and Misunderstandings, Problem Solving Glossary French-English Dictionary Downloads and Other Resources For the sake of simplicity, we will allow ourselves certain shortcuts and approximations, the aim being the concrete application and acquisition of know-how, and not the scientific study of the subject. We will also limit the explanation to the elements necessary for the concrete application and understanding of the parameters; we will avoid information of purely historical interest, with the intention of keeping this guide clear and as concrete as possible. We invite the reader to start by reading the [ Wikipedia ] articles (https://fr.wikipedia.org/wiki/Couleur) to learn more about the subject and to complete the theoretical information missing from this guide; there is no lack of resources on the subject. On each page, we regularly add and complete the sources and references to deepen what is explained. \u21a9","title":"Introduction"},{"location":"index.html#a-practical-guide-to-color-management-in-audiovisual-production","text":"","title":"A Practical Guide to Color Management in Audiovisual Production"},{"location":"index.html#about-us","text":"This guide is part of a collection of documents managed by RxLaboratory : RxDocs . RxLaboratory is an organization whose mission is to develop, distribute and promote free software and innovation in audiovisual production, as well as to facilitate mutual aid, sharing and cooperation between all actors in this field. RxDocs is a collaborative work of scientific, technical and artistic popularization in the field of audiovisual production in the broad sense: image, video, animation, sound \u2026 These documents are intended for anyone working in the field (phography, video, motion design, directing, animation, illustration, cinematography, editing \u2026) Note All names followed by an asterisk* are explained in the Glossary","title":"About us"},{"location":"index.html#introduction","text":"The methods and tools of color management in (digital) image processing often seem complicated and are sometimes misunderstood by people working in audiovisual production. The software are full of different parameters, and which seem to be very different from one to another. As a result, color management is not always mastered; very often the user lose control over colors and can\u2019t achieve the desired look. This field, certainly very technical, is however not as complicated as it seems, and this document aims to explain and vulgarize the problems related to color management in image treatment, and is intended for any person dealing with this problem in the production of digital images1. The goal is not to make theory but to arrive quickly at the concrete application of the explanations, and we will limit the historical and theoretical explanations to the bare minimum. Note Practically, the colorimetric spaces are intrinsically related to the data encoding formats, and we will also speak partly about these various formats. The first part will be theoretical, starting from light and color, and studying how we perceive it, then how to record it, represent it and reproduce it in a digital system. Then we will approach the practical aspect, explaining in concrete cases what are the useful and recommended parameters in your productions of all types, and in different applications. Tip This document is constantly evolving! If you find any errors, or wish to make improvements, contribute or report it to us !","title":"Introduction"},{"location":"index.html#i-theory","text":"A - What is color? A.1 - Physical color: visible light A.2 - Following light : from emission to reception A.2.a - Emission, transmission, reflection, reception A.2.b - Breakdown of the received light B - Small inventory and classification of colors note : short intro on human perception B.1 - Black B.2 - Monochromatic lights B.3 - Purple B.4 - Grey and white B.4.a - Complementary colors B.4.b - Perception C - Perception of light and colors by the human eye B.1 - The eye B.2 - The nervous system B.3 - Implications B.3.a - On brightness and contrast B.3.b - On shading D - Color reproduction D.1 - Additive synthesis D.2 - Subtractive synthesis E - About white values : temperature E.1 - Black body E.2 - Planckian locus E.3 - White balance F - Objective representation of colors F.1 - Decomposing colors F.2 - Color Charts, CIE XYZ of 1931 and CIE xyZ F.3 - Other CIE spaces G - From real life to digital G.1 - Digitization and storage: converting to binary G.2 - The Color Spaces H - What is a color space? H.1 - What defines a color space H.1.a - Primaries and gamut H.1.b - White point H.1.c - Transfer curve H.2 - Other parameters H.2.a - Pixel format H.2.b - Depth H.3 - Why different color spaces? I - List of color spaces J - List of color space parameters K - Pixel format K.1 - RGB and YUV K.1.a - RGB K.1.b - YUV K.1.c - Comparison K.1.d - Others K.2 - YUV 4:4:4, 4:2:2, 4:2:0\u2026 Chrominance downsampling. K.2.a - 4:4:4 K.2.b - 4:2:2 K.2.c - 4:2:0 K.3 - Color depth (bits, bpc and bpp) K.3.a - RGB K.3.b - YUV K.3.c - Others K.4 - Full range / Limited / TV / PC ? K.4.a - Full range / PC K.4.b - Limited range / TV K.4.c - Practical implications K.4.c.a - Encoding K.4.c.b - Playback and display L - Transfer curves, linear space and gamma M - Les LUTs M.1 - Description M.2 - Use N - OpenColorIO and ACES N.1 - Compatible applications N.2 - ACES N.2.a - ACES color space N.2.a.a - ACES2065-1 N.2.a.b - ACEScg N.2.a.c - ACEScc","title":"I - Theory"},{"location":"index.html#ii-application","text":"Hint The list of applications is arranged in alphabetical order, with the publisher\u2019s name at the top. A - Practical application: choosing color spaces and formats A.1 - Theory A.1.a - Journey of a color A.2 - Workspace (scene referred) A.3 - Input A.4 - Display A.4.a - Screen space A.4.b - Settings and color profiles A.4.c - In the application A.5 - Color pickers A.6 - Intermediate output A.7 - Final output B - A few standards C - Screen calibration C.1 - Introduction C.2 - Environment C.3 - Calibration C.3.a - Choosing the color space of the screen and the calibration C.3.b - White point and brightness C.3.c - Calibration and application of the color profile C.3.c.1 - Calibration C.3.c.2 - Colorimetric profile D - Soft-proofing and simulation E - Setting up the production pipeline E.1 - 3D Animation with Blender, Filmic E.1.a - Textures et autres images 2D E.1.b - 3D rendering E.1.c - Compositing E.1.c.a - Filmic and After Effects E.1.d - Exports E.2 - 3D Animation with ACES E.2.a - Textures and other 2D images E.2.b - 3D rendering E.2.c - Compositing E.2.d - Exports E.3 - 2D Animation E.4 - Videos, live action and VFX F - Setting up the production pipeline with OpenColorIO F.1 - Setting up F.2 - Analysis of an OCIO configuration F.2.a - Metadata F.2.b - Roles F.2.c - Displays G - Color management : Adobe After Effects G.1 - Project Settings - Workspace G.2 - Interpr\u00e9tation des m\u00e9trages - Espaces d\u2019Input G.3 - View Options - Display Space and Simulations G.4 - Export Options - Output Spaces G.5 - OCIO G.5.a - Introduction G.5.b - Installing OCIO plugin G.5.c - Disabling color management in After Effects G.5.d - Managing G.5.e - Input and workspace G.5.f - Output G.5.g - Display H - Color management : Adobe Premiere and Media Encoder H.1 - Output space H.2 - Applying a LUT H.3 - Workspace and display I - Color management : Adobe Photoshop J - Color management : Autodesk Maya K - Color management : Blender K.1 - Default configuration : Filmic K.1.a - Render (scene referred) and display K.1.b - Output K.1.c - Input (textures) K.2 - Cha\u00eene de fabrication OCIO K.2.a - Changer la configuration OCIO K.3 - Utiliser ACES K.3.a - Rendu (scene referred) et affichage K.3.b - Input (textures) L - Color management : Darktable L.1 - Input and Workspace L.2 - Output L.3 - Soft-Proofing M - Color management : DuME M.1 - Workspace M.2 - Input M.3 - Output M.4 - Presets M.5 - LUT M.6 - OCIO N - Color management : Krita N.1 - Workspace N.2 - Display N.2.a - Screen N.2.b - Soft-proofing N.3 - Color pickers N.4 - Output N.5 - OCIO with Krita","title":"II - Application"},{"location":"index.html#iii-appendixes","text":"In a Nutshell: a quick summary of how to get it right Common Errors and Misunderstandings, Problem Solving Glossary French-English Dictionary Downloads and Other Resources For the sake of simplicity, we will allow ourselves certain shortcuts and approximations, the aim being the concrete application and acquisition of know-how, and not the scientific study of the subject. We will also limit the explanation to the elements necessary for the concrete application and understanding of the parameters; we will avoid information of purely historical interest, with the intention of keeping this guide clear and as concrete as possible. We invite the reader to start by reading the [ Wikipedia ] articles (https://fr.wikipedia.org/wiki/Couleur) to learn more about the subject and to complete the theoretical information missing from this guide; there is no lack of resources on the subject. On each page, we regularly add and complete the sources and references to deepen what is explained. \u21a9","title":"III - Appendixes"},{"location":"A-definition.html","text":"I.A - What is color? \u00b6 The word color may seem simple and univocal, but before entering the subject, it is important to explain what the word color(s) will cover in the following pages. We will understand by color the whole range of light that the human eye can perceive, including in its nuances of intensity. The term thus covers as well the hue as the saturation and the luminosity , terms with which the people called to handle the colors are familiar. It is also implied that there are a certain number of defined colors, that this complete spectrum is divisible into quanta, into bricks . The size of the basic quantum, and incidentally the number of color quanta , the number of different colors, will be discussed later. I.A - What is color? A.1 - Physical color: visible light A.2 - Following light : from emission to reception A.2.a - Emission, transmission, reflection, reception A.2.b - Breakdown of the received light A.1 - Physical color: visible light \u00b6 What is called color is therefore the perception by the human being of visible light , visible light being in reality only a small part of the electro-magnetic spectrum . It is necessary to understand, considering this rainbow spectrum, that in nature, the sources of monochromatic light are rare, and one perceives especially a blend of these different frequencies; the colors of which one will thus discuss here include at the same time these monochromatic sources but also and especially all the possible blends between them, at different intensities. These blends thus represent an infinity of nuances, an infinity of different colors, which it would be impossible to describe all individually. So naturally we try to give ourselves references in all these possible colors, from black to white through gray and in all the shades of the rainbow (i.e. monochromatic lights), red, orange, yellow, green, blue, violet\u2026 The problem is then to agree on these names and define precisely what is red and what is purple for example. Historically, a solution came from the discovery of Fraunhofer lines dark discontinuities in the spectrum of light emitted by the sun, discovered in the early nineteenth century. These lines, caused by the absorption of certain frequencies by the media crossed by the light (in the atmosphere in particular), are visible on precise wavelengths and were one of the first references to define precise colors. To this problem of reference is added the fact that the definition of color is intrinsically linked to the perception that we have, and this perception obviously differs from one population to another, from one individual to another, and also from the environment in which we perceive the color. A.2 - Following light : from emission to reception \u00b6 A.2.a - Emission, transmission, reflection, reception \u00b6 The color that we perceive is the result of a group of rays of light emitted (\u201ccreated\u201d) by a given source (the sun, a screen, a light bulb \u2026), which has passed through one or more media with different properties that have absorbed, deviated or re-emitted part 1 , which has been reflected by one or more surfaces with different properties that have in turn absorbed, deviated or re-emitted part, before finally reaching a sensor: our eye, or a camera \u2026 A.2.b - Breakdown of the received light \u00b6 In the end, whatever the path of this group of rays of light, we only perceive the state in which it is at the arrival: it is the perceived color. Whatever the path and their origin, all colors are the same thing: groups of light rays. The color is therefore \u201cjust\u201d a mixture of a number of light rays that have reached a sensor or our eye. Each of these rays, or photons * has a precise wavelength called monochrome *, and if we see colors different from those of the electro-magnetic spectrum, they are the result of the mixture of all these rays. It is the quantity of received photons forming this color which makes the intensity, the luminance *, of the color, and the mixture of the various wavelengths which makes the hue *. Sources and references Color on Wikipedia Electro-magnetic spectrum on Wikipedia Fraunhofer rays on Wikipedia . We speak in particular of refraction for the deviation of the rays \u21a9","title":"A - What is color?"},{"location":"A-definition.html#ia-what-is-color","text":"The word color may seem simple and univocal, but before entering the subject, it is important to explain what the word color(s) will cover in the following pages. We will understand by color the whole range of light that the human eye can perceive, including in its nuances of intensity. The term thus covers as well the hue as the saturation and the luminosity , terms with which the people called to handle the colors are familiar. It is also implied that there are a certain number of defined colors, that this complete spectrum is divisible into quanta, into bricks . The size of the basic quantum, and incidentally the number of color quanta , the number of different colors, will be discussed later. I.A - What is color? A.1 - Physical color: visible light A.2 - Following light : from emission to reception A.2.a - Emission, transmission, reflection, reception A.2.b - Breakdown of the received light","title":"I.A - What is color?"},{"location":"A-definition.html#a1-physical-color-visible-light","text":"What is called color is therefore the perception by the human being of visible light , visible light being in reality only a small part of the electro-magnetic spectrum . It is necessary to understand, considering this rainbow spectrum, that in nature, the sources of monochromatic light are rare, and one perceives especially a blend of these different frequencies; the colors of which one will thus discuss here include at the same time these monochromatic sources but also and especially all the possible blends between them, at different intensities. These blends thus represent an infinity of nuances, an infinity of different colors, which it would be impossible to describe all individually. So naturally we try to give ourselves references in all these possible colors, from black to white through gray and in all the shades of the rainbow (i.e. monochromatic lights), red, orange, yellow, green, blue, violet\u2026 The problem is then to agree on these names and define precisely what is red and what is purple for example. Historically, a solution came from the discovery of Fraunhofer lines dark discontinuities in the spectrum of light emitted by the sun, discovered in the early nineteenth century. These lines, caused by the absorption of certain frequencies by the media crossed by the light (in the atmosphere in particular), are visible on precise wavelengths and were one of the first references to define precise colors. To this problem of reference is added the fact that the definition of color is intrinsically linked to the perception that we have, and this perception obviously differs from one population to another, from one individual to another, and also from the environment in which we perceive the color.","title":"A.1 - Physical color: visible light"},{"location":"A-definition.html#a2-following-light-from-emission-to-reception","text":"","title":"A.2 - Following light : from emission to reception"},{"location":"A-definition.html#a2a-emission-transmission-reflection-reception","text":"The color that we perceive is the result of a group of rays of light emitted (\u201ccreated\u201d) by a given source (the sun, a screen, a light bulb \u2026), which has passed through one or more media with different properties that have absorbed, deviated or re-emitted part 1 , which has been reflected by one or more surfaces with different properties that have in turn absorbed, deviated or re-emitted part, before finally reaching a sensor: our eye, or a camera \u2026","title":"A.2.a - Emission, transmission, reflection, reception"},{"location":"A-definition.html#a2b-breakdown-of-the-received-light","text":"In the end, whatever the path of this group of rays of light, we only perceive the state in which it is at the arrival: it is the perceived color. Whatever the path and their origin, all colors are the same thing: groups of light rays. The color is therefore \u201cjust\u201d a mixture of a number of light rays that have reached a sensor or our eye. Each of these rays, or photons * has a precise wavelength called monochrome *, and if we see colors different from those of the electro-magnetic spectrum, they are the result of the mixture of all these rays. It is the quantity of received photons forming this color which makes the intensity, the luminance *, of the color, and the mixture of the various wavelengths which makes the hue *. Sources and references Color on Wikipedia Electro-magnetic spectrum on Wikipedia Fraunhofer rays on Wikipedia . We speak in particular of refraction for the deviation of the rays \u21a9","title":"A.2.b - Breakdown of the received light"},{"location":"B-inventaire.html","text":"I.B - Small inventory and classification of colors \u00b6 The perceived colors are therefore mixtures of monochromatic * light rays. Let\u2019s see how to classify all these colors and how they are decomposed. I.B - Small inventory and classification of colors B.1 - Black B.2 - Monochromatic lights B.3 - Purple B.4 - White and grey B.4.a - Complementary colors B.4.b - Perception Note We are still talking here about light and light rays, and thus about an additive system; it is not a question of the composition of surfaces and the way they absorb and reflect colors, and which would be the object of a subtractive synthesis of light. See chapter D - Color Reproduction for more details on the subject. B.1 - Black \u00b6 The simplest color is therefore black * : the total absence of light is perceived as black. When the intensity of the light decreases, the colors approach gradually the black. But when the intensity rises, the colors do not go towards white! The higher the intensity, the more the color appears \u201cbright\u201d and \u201csaturated\u201d, but does not fade, does not whiten 1 . B.2 - Monochromatic lights \u00b6 The simplest colors, and the rarest (they are those emitted by Lasers for example), are those composed of a single monochromatic light. They are in fact the colors of the rainbow , those which are included in the monochromatic spectrum. They range from red to blue, orange, yellow, green, cyan\u2026 but do not include any shades of purple *. They can vary in intensity, in a scale from black (intensity 0) to the \u201cbrightest\u201d color. B.3 - Purple \u00b6 It should be noted that the purple/violet range 2 are not part of the \u201cnatural\u201d and monochromatic colors but are the result of the mixing of blue and red rays, which are the two extremes of the spectrum. The purples added to the monochromatic colors form the set of the most saturated * colors possible, which are thus all monochromatic colors and mixtures containing only blue and red. B.4 - White and grey \u00b6 In nature, all the lights, or more precisely all the luminous rays, are monochromatic * ; it is the perception of their blending that the brain interprets in an infinity of other colors. We saw that the purples are part of these mixtures; all the other colors not saturated * , the grays and the whites, are thus also mixed lights. All the \u201cdesaturated\u201d shades, whites, grays, are blends of these monochromatic colors, and there is for each color (and not only the grays, all the non-monochromatic colors) an infinity of different blends which can generate it. Two colors perceived in an identical way but composed of monochromatic rays in different proportions are said metamers * . B.4.a - Complementary colors \u00b6 A minimum of two \u201copposite\u201d rays is needed to form white: two monochromatic rays that form grey/white when mixed are said to be complementary * . We can also include purples as complementary to greens. Indeed, a purple in turn mixed with green gives white 3 , and can therefore be considered as the complementary of green 4 , although it is actually already the combination of red and blue. We find there the three common primaries of digital color reproduction systems. B.4.b - Perception \u00b6 The same white sheet of paper will be seen as white regardless of the time of day or the type of lighting, although in reality its color is quite different in each of these cases: the sheet of paper will in fact take on the color of the light that illuminates it, but the brain will operate a \u201cshift\u201d in perception that will make it appear white in all cases. Under the sun, in the shade or in the clouds, whatever the time of day, in reality we perceive snow as white even if in reality the light it reflects is each time different 5 . Therefore, rays of light with perfectly different physical attributes can be seen in the same way by the observer. ***The perception of colors is eminently subjective. In the real world. But artistically, the technical limits of color reproduction systems impose to \u201ccheat\u201d by whitening the highlights to compensate the limit of luminosity of the available colors. Without this trick, \u201cmonochromatic\u201d colors in an image could appear less luminous than gray or white of the same image; lights are therefore \u201cpulled\u201d towards white values to compensate. \u21a9 The term \u201cultraviolet\u201d to describe the part of the electromagnetic spectrum beyond blue is misleading; the end of the spectrum is perceived as blue (dark) rather than violet. The true violet is the color that we perceive as a result of the combination of blue and red rays. \u21a9 We are of course speaking here of light , in other words an \u201cadditive\u201d system where the intensities add up. This is obviously not true in painting and printing which are \u201csubtractive\u201d systems where the colors \u201casborbate\u201d light intensity, subtracting rays of light. cf. chapter D - Reproduction of colors . \u21a9 If we wanted to be physically correct, the greens would simply have no complements at all, and only the intervals of rays near the two extremes of the spectrum (oranges-reds and cyan-blues) would be complementary to each other. \u21a9 What a camera has the greatest difficulty to reproduce. see chapter E - About white values : temperature . \u21a9","title":"B - Small inventory and classification of colors"},{"location":"B-inventaire.html#ib-small-inventory-and-classification-of-colors","text":"The perceived colors are therefore mixtures of monochromatic * light rays. Let\u2019s see how to classify all these colors and how they are decomposed. I.B - Small inventory and classification of colors B.1 - Black B.2 - Monochromatic lights B.3 - Purple B.4 - White and grey B.4.a - Complementary colors B.4.b - Perception Note We are still talking here about light and light rays, and thus about an additive system; it is not a question of the composition of surfaces and the way they absorb and reflect colors, and which would be the object of a subtractive synthesis of light. See chapter D - Color Reproduction for more details on the subject.","title":"I.B - Small inventory and classification of colors"},{"location":"B-inventaire.html#b1-black","text":"The simplest color is therefore black * : the total absence of light is perceived as black. When the intensity of the light decreases, the colors approach gradually the black. But when the intensity rises, the colors do not go towards white! The higher the intensity, the more the color appears \u201cbright\u201d and \u201csaturated\u201d, but does not fade, does not whiten 1 .","title":"B.1 - Black"},{"location":"B-inventaire.html#b2-monochromatic-lights","text":"The simplest colors, and the rarest (they are those emitted by Lasers for example), are those composed of a single monochromatic light. They are in fact the colors of the rainbow , those which are included in the monochromatic spectrum. They range from red to blue, orange, yellow, green, cyan\u2026 but do not include any shades of purple *. They can vary in intensity, in a scale from black (intensity 0) to the \u201cbrightest\u201d color.","title":"B.2 - Monochromatic lights"},{"location":"B-inventaire.html#b3-purple","text":"It should be noted that the purple/violet range 2 are not part of the \u201cnatural\u201d and monochromatic colors but are the result of the mixing of blue and red rays, which are the two extremes of the spectrum. The purples added to the monochromatic colors form the set of the most saturated * colors possible, which are thus all monochromatic colors and mixtures containing only blue and red.","title":"B.3 - Purple"},{"location":"B-inventaire.html#b4-white-and-grey","text":"In nature, all the lights, or more precisely all the luminous rays, are monochromatic * ; it is the perception of their blending that the brain interprets in an infinity of other colors. We saw that the purples are part of these mixtures; all the other colors not saturated * , the grays and the whites, are thus also mixed lights. All the \u201cdesaturated\u201d shades, whites, grays, are blends of these monochromatic colors, and there is for each color (and not only the grays, all the non-monochromatic colors) an infinity of different blends which can generate it. Two colors perceived in an identical way but composed of monochromatic rays in different proportions are said metamers * .","title":"B.4 - White and grey"},{"location":"B-inventaire.html#b4a-complementary-colors","text":"A minimum of two \u201copposite\u201d rays is needed to form white: two monochromatic rays that form grey/white when mixed are said to be complementary * . We can also include purples as complementary to greens. Indeed, a purple in turn mixed with green gives white 3 , and can therefore be considered as the complementary of green 4 , although it is actually already the combination of red and blue. We find there the three common primaries of digital color reproduction systems.","title":"B.4.a - Complementary colors"},{"location":"B-inventaire.html#b4b-perception","text":"The same white sheet of paper will be seen as white regardless of the time of day or the type of lighting, although in reality its color is quite different in each of these cases: the sheet of paper will in fact take on the color of the light that illuminates it, but the brain will operate a \u201cshift\u201d in perception that will make it appear white in all cases. Under the sun, in the shade or in the clouds, whatever the time of day, in reality we perceive snow as white even if in reality the light it reflects is each time different 5 . Therefore, rays of light with perfectly different physical attributes can be seen in the same way by the observer. ***The perception of colors is eminently subjective. In the real world. But artistically, the technical limits of color reproduction systems impose to \u201ccheat\u201d by whitening the highlights to compensate the limit of luminosity of the available colors. Without this trick, \u201cmonochromatic\u201d colors in an image could appear less luminous than gray or white of the same image; lights are therefore \u201cpulled\u201d towards white values to compensate. \u21a9 The term \u201cultraviolet\u201d to describe the part of the electromagnetic spectrum beyond blue is misleading; the end of the spectrum is perceived as blue (dark) rather than violet. The true violet is the color that we perceive as a result of the combination of blue and red rays. \u21a9 We are of course speaking here of light , in other words an \u201cadditive\u201d system where the intensities add up. This is obviously not true in painting and printing which are \u201csubtractive\u201d systems where the colors \u201casborbate\u201d light intensity, subtracting rays of light. cf. chapter D - Reproduction of colors . \u21a9 If we wanted to be physically correct, the greens would simply have no complements at all, and only the intervals of rays near the two extremes of the spectrum (oranges-reds and cyan-blues) would be complementary to each other. \u21a9 What a camera has the greatest difficulty to reproduce. see chapter E - About white values : temperature . \u21a9","title":"B.4.b - Perception"},{"location":"C-perception.html","text":"I.C - Perception of light and colors by the human eye \u00b6 To the problem of having an objective reference to define the colors is added the fact that the definition of the color is intrinsically related to the perception that one has, and this perception obviously differs from one population to another, from one individual to another, and also of the environment in which one perceives the color. Hint In ancient times, Democritus even suggested that colors are only imagination. I.C - Perception of light and colors by the human eye C.1 - The Eye C.2 - The nervous system C.3 - Consequences C.3.a - On brightness and contrast C.3.b - On hues Comprehending this subjective interpretation of color is important for understanding how color management in digital image processing was designed and how it is evolving. C.1 - The Eye \u00b6 As soon as light rays are received by the eye and the retina, the light is broken down and interpreted. Two types of photoreceptors make up the retina, cones and rods. The rods are the useful and active cells in the half-light; they are saturated as soon as the light becomes too strong, around 500 photons per second. The cones are the effective cells in the reception of the more intense lights; they activate only starting from an intensity of 10 photons per second. Divided into three different types, they are able to capture rays over a wider range of frequencies; this division into three types also allows the hue of the light to be interpreted: the nervous system can compare the intensities on the three different types of cones and deduce a hue. C.2 - The nervous system \u00b6 The information received by the cells of the retina are converted into nerve impulses and interpreted by the brain. This process explains in particular the fact that objectively different mixtures of monochromatic sources can be perceived in exactly the same way and subjectively identical ; thus, two white colors perceived in the same way and indistinguishable by the individual can in reality be formed of different blends of monochromatic lights 1 . Conversely, colors that are objectively the same can be perceived in completely different ways depending on their environment. This is particularly the case for white ; what we perceive as the color white varies greatly depending on the types of light sources and the general hue of the environment. This is the equivalent of white balance in photography, but it is true for all colors. LThe nervous system is therefore responsible for an interpretation of the colors which is not directly related to the objective signal received by the eye, but which could be seen as a subjective \u201d adjustment \u201d in post-production: it adapts the image according to the environment, exactly as the photographer does by retouching the contrasts and the white balance (the tints), to \u201d equalize \u201d the image, and to increase the quantity of perceivable details in the reconstructed image. All these different steps change the perception of a physical and objective signal: the same light ray will not be \u201cseen\u201d in the same way in two different places, and according to the quantity and the nature of the other rays which accompany it. C.3 - Consequences \u00b6 The consequences of this subjective perception are important to understand the historical choices that were made on the techniques allowing the artificial reproduction and processing of images (analog and digital). C.3.a - On brightness and contrast \u00b6 The presence of the rods on the retina and their performance in the weaker lights, to the detriment of the perception of the colors in these lights, makes that the eye is globally more powerful to discern the details in the half-light than in the strong lights, and that it is in general more powerful to distinguish the contrasts of intensity luminous than the variations of color (the decomposition in precise wavelengths on the spectrum) C.3.b - On hues \u00b6 The division into three types of cones, each performing on specific wavelengths, means that three primary colors 2 are sufficient to reproduce all the colors as the human being can perceive them, as long as these primaries correspond approximately to the range of performance of these three types of cones 3 . The three \u201c primary \u201d colors closest to the ranges of perception of the cone cells are the combination Red, Green, and Blue , even if in reality the cones rather perceive yellow, green and blue. Note Trichromy is not the only model that can represent the vision of color. We can also imagine a model where the vision distinguishes the oppositions white-black, blue-yellow and red-green for example 4 . This way of perceiving the colors thus influences the choices of systems to reproduce them artificially, as we shall see later, in particular with the gamma , the RGB or YUV system. Sources & References Color on Wikipedia Psychophysics on Wikipedia . Rod cells on Wikipedia Cone cells on Wikipedia ) Colour representation, Kent State University ) There are several \u201d grades \u201d of white and apparently identical lights, depending on whether they are composed of a more or less varied range of rays of different wavelengths (thus of different tints). \u21a9 There is a vocabulary confusion, in particular in the audio-visual production, between primary color (which cannot be obtained by blending other colors), and complementary colors (whose blending gives black, gray or white). We use here the word primary in the sense of the spaces of numerical colors: whose mixing gives the white. \u21a9 Isaac Newton had discovered that it is quite possible to reproduce a white light with only two monochromatic colors but a third primary is necessary to depart from the simple gradation between the first two. \u21a9 Ewald Hering was vigorously opposed to trichromy. Based on the psychological study of perception, his model, which Erwin Schr\u00f6dinger showed the mathematical equivalence with trichromacy, has since been confirmed by neuroscience studies. \u21a9","title":"C - Perception of light and colors by the human eye"},{"location":"C-perception.html#ic-perception-of-light-and-colors-by-the-human-eye","text":"To the problem of having an objective reference to define the colors is added the fact that the definition of the color is intrinsically related to the perception that one has, and this perception obviously differs from one population to another, from one individual to another, and also of the environment in which one perceives the color. Hint In ancient times, Democritus even suggested that colors are only imagination. I.C - Perception of light and colors by the human eye C.1 - The Eye C.2 - The nervous system C.3 - Consequences C.3.a - On brightness and contrast C.3.b - On hues Comprehending this subjective interpretation of color is important for understanding how color management in digital image processing was designed and how it is evolving.","title":"I.C - Perception of light and colors by the human eye"},{"location":"C-perception.html#c1-the-eye","text":"As soon as light rays are received by the eye and the retina, the light is broken down and interpreted. Two types of photoreceptors make up the retina, cones and rods. The rods are the useful and active cells in the half-light; they are saturated as soon as the light becomes too strong, around 500 photons per second. The cones are the effective cells in the reception of the more intense lights; they activate only starting from an intensity of 10 photons per second. Divided into three different types, they are able to capture rays over a wider range of frequencies; this division into three types also allows the hue of the light to be interpreted: the nervous system can compare the intensities on the three different types of cones and deduce a hue.","title":"C.1 -  The Eye"},{"location":"C-perception.html#c2-the-nervous-system","text":"The information received by the cells of the retina are converted into nerve impulses and interpreted by the brain. This process explains in particular the fact that objectively different mixtures of monochromatic sources can be perceived in exactly the same way and subjectively identical ; thus, two white colors perceived in the same way and indistinguishable by the individual can in reality be formed of different blends of monochromatic lights 1 . Conversely, colors that are objectively the same can be perceived in completely different ways depending on their environment. This is particularly the case for white ; what we perceive as the color white varies greatly depending on the types of light sources and the general hue of the environment. This is the equivalent of white balance in photography, but it is true for all colors. LThe nervous system is therefore responsible for an interpretation of the colors which is not directly related to the objective signal received by the eye, but which could be seen as a subjective \u201d adjustment \u201d in post-production: it adapts the image according to the environment, exactly as the photographer does by retouching the contrasts and the white balance (the tints), to \u201d equalize \u201d the image, and to increase the quantity of perceivable details in the reconstructed image. All these different steps change the perception of a physical and objective signal: the same light ray will not be \u201cseen\u201d in the same way in two different places, and according to the quantity and the nature of the other rays which accompany it.","title":"C.2 - The nervous system"},{"location":"C-perception.html#c3-consequences","text":"The consequences of this subjective perception are important to understand the historical choices that were made on the techniques allowing the artificial reproduction and processing of images (analog and digital).","title":"C.3 - Consequences"},{"location":"C-perception.html#c3a-on-brightness-and-contrast","text":"The presence of the rods on the retina and their performance in the weaker lights, to the detriment of the perception of the colors in these lights, makes that the eye is globally more powerful to discern the details in the half-light than in the strong lights, and that it is in general more powerful to distinguish the contrasts of intensity luminous than the variations of color (the decomposition in precise wavelengths on the spectrum)","title":"C.3.a - On brightness and contrast"},{"location":"C-perception.html#c3b-on-hues","text":"The division into three types of cones, each performing on specific wavelengths, means that three primary colors 2 are sufficient to reproduce all the colors as the human being can perceive them, as long as these primaries correspond approximately to the range of performance of these three types of cones 3 . The three \u201c primary \u201d colors closest to the ranges of perception of the cone cells are the combination Red, Green, and Blue , even if in reality the cones rather perceive yellow, green and blue. Note Trichromy is not the only model that can represent the vision of color. We can also imagine a model where the vision distinguishes the oppositions white-black, blue-yellow and red-green for example 4 . This way of perceiving the colors thus influences the choices of systems to reproduce them artificially, as we shall see later, in particular with the gamma , the RGB or YUV system. Sources & References Color on Wikipedia Psychophysics on Wikipedia . Rod cells on Wikipedia Cone cells on Wikipedia ) Colour representation, Kent State University ) There are several \u201d grades \u201d of white and apparently identical lights, depending on whether they are composed of a more or less varied range of rays of different wavelengths (thus of different tints). \u21a9 There is a vocabulary confusion, in particular in the audio-visual production, between primary color (which cannot be obtained by blending other colors), and complementary colors (whose blending gives black, gray or white). We use here the word primary in the sense of the spaces of numerical colors: whose mixing gives the white. \u21a9 Isaac Newton had discovered that it is quite possible to reproduce a white light with only two monochromatic colors but a third primary is necessary to depart from the simple gradation between the first two. \u21a9 Ewald Hering was vigorously opposed to trichromy. Based on the psychological study of perception, his model, which Erwin Schr\u00f6dinger showed the mathematical equivalence with trichromacy, has since been confirmed by neuroscience studies. \u21a9","title":"C.3.b - On hues"},{"location":"D-reproduction.html","text":"I.D - Color reproduction \u00b6 When it is a question of reproducing ( synthesizing ) colors, two systems are possible, according to the fact that the device is \u201cpassive\u201d or reflecting (in painting, printing, etc.), i.e. that it needs to be illuminated, or \u201cactive\u201d, and emits its own light (screen, projector, etc) I.D - Color reproduction D.1 - Additive synthesis D.2 - Subtractive synthesis D.1 - Additive synthesis \u00b6 When a system emits its own light, such as a screen or projector, it produces colors ranging from black * (a system that is turned off, emitting nothing), to white * . In such an additive system, the most luminous color is the white: it results from the addition of the various primitives * of which the pixels of the device are made, in general a combination of green, red and blue. This system works by adding rays of light to mix the colors. A consequence to such a system is that a monochromatic color (the color of a primary * of the system, or the simulation of another monochromatic light obtained by mixing these primaries * ) is inevitably less intense than a white one, contrary to the real world where everything is possible. An additive system, being a light generator, can work in complete darkness, in which case the color reproduction can be perfectly controlled by not being influenced by any external light (as in a movie theater). D.2 - Subtractive synthesis \u00b6 When a color reproduction system is a reflective surface, it is the pigments that alter the light received; each pigment absorbs part of the rays received and reflects the rest. The resulting light is therefore necessarily of a lower intensity than that illuminating the surface: the system subtracts light rays from an available bouquet. Such a subtractive system thus produces the colors by going from white * (reflecting all received light) to black * (absorbing all received light). An important consequence is that a subtractive system depends on the light that illuminates it to reproduce colors: seen under a light poor in monochromatic rays * , an exposed painting will not render all its colors possible. For that, it is necessary to illuminate it not only with a white light, but with a white light \u201ccontaining\u201d enough different monochromatic * rays, of high quality 1 . A painting, a poster or any other object illuminated under a monochromatic light will be able to reproduce only this monochromatic * color, at different intensities, whatever the pigments which compose them. All light sources are not equal! With identical color, the light of different types of bulbs or fluorescent tubes will not be the same once reflected and \u201cdecomposed\u201d: the sources of better quality will restore better the colors of the surfaces which they meet, and will be often \u201cmore pleasant\u201d. \u21a9","title":"D - Color reproduction"},{"location":"D-reproduction.html#id-color-reproduction","text":"When it is a question of reproducing ( synthesizing ) colors, two systems are possible, according to the fact that the device is \u201cpassive\u201d or reflecting (in painting, printing, etc.), i.e. that it needs to be illuminated, or \u201cactive\u201d, and emits its own light (screen, projector, etc) I.D - Color reproduction D.1 - Additive synthesis D.2 - Subtractive synthesis","title":"I.D - Color reproduction"},{"location":"D-reproduction.html#d1-additive-synthesis","text":"When a system emits its own light, such as a screen or projector, it produces colors ranging from black * (a system that is turned off, emitting nothing), to white * . In such an additive system, the most luminous color is the white: it results from the addition of the various primitives * of which the pixels of the device are made, in general a combination of green, red and blue. This system works by adding rays of light to mix the colors. A consequence to such a system is that a monochromatic color (the color of a primary * of the system, or the simulation of another monochromatic light obtained by mixing these primaries * ) is inevitably less intense than a white one, contrary to the real world where everything is possible. An additive system, being a light generator, can work in complete darkness, in which case the color reproduction can be perfectly controlled by not being influenced by any external light (as in a movie theater).","title":"D.1 - Additive synthesis"},{"location":"D-reproduction.html#d2-subtractive-synthesis","text":"When a color reproduction system is a reflective surface, it is the pigments that alter the light received; each pigment absorbs part of the rays received and reflects the rest. The resulting light is therefore necessarily of a lower intensity than that illuminating the surface: the system subtracts light rays from an available bouquet. Such a subtractive system thus produces the colors by going from white * (reflecting all received light) to black * (absorbing all received light). An important consequence is that a subtractive system depends on the light that illuminates it to reproduce colors: seen under a light poor in monochromatic rays * , an exposed painting will not render all its colors possible. For that, it is necessary to illuminate it not only with a white light, but with a white light \u201ccontaining\u201d enough different monochromatic * rays, of high quality 1 . A painting, a poster or any other object illuminated under a monochromatic light will be able to reproduce only this monochromatic * color, at different intensities, whatever the pigments which compose them. All light sources are not equal! With identical color, the light of different types of bulbs or fluorescent tubes will not be the same once reflected and \u201cdecomposed\u201d: the sources of better quality will restore better the colors of the surfaces which they meet, and will be often \u201cmore pleasant\u201d. \u21a9","title":"D.2 - Subtractive synthesis"},{"location":"E-temperature.html","text":"I.E - About white values : temperature \u00b6 White being perceived differently depending on the environment and the observer, we must, before talking about it, define exactly what is white. Once again, we need an invariant, physical referential, on which everyone can agree, a bit like the use of Fraunhofer\u2019s rays can be used as a reference to define precise monochromatic lights * . Radiation of black bodies * is commonly used to define white. I.E - About white values : temperature E.1 - Black body E.2 - Planckian locus E.3 - White balance E.1 - Black body \u00b6 UA black body is an element which does not necessarily appear black , but which is black in the sense that it absorbs all the electromagnetic rays (and thus visible light) that it receives; in other words, if the black body were perfectly cold 1 , it would be pure black. But every body has some heat, and this heat causes it to emit radiation, some of it in the visible spectrum, which explains why what is called a black body is not seen as black. The consequence is that the light, and therefore the color, of a black body is not influenced at all by the light it receives, but is only the result of its heat. Several elements can be considered as black bodies: the embers of a barbecue, fire, a glowing metal, the sun \u2026 All these elements are not real black bodies in the physical sense (the radiation they emit is not perfectly independent of the radiation they receive) but are an approximation, very close in the case of the sun 2 . Black bodies are interesting because the light they emit, and therefore their perceived color, does not depend on the environment in which they are; they therefore form good objective reference frames. Moreover, we know that their radiation spectrum depends directly on their energy, their heat. One can thus associate to a given heat a given spectrum, i.e. a light of a precise composition of monochromatic rays * , resulting in a precise and measurable white * . Note Unlike the other incandescent elements in the examples above, the apparent color of the sun is not directly the result of its heat even though it is a black body; indeed, the absorption of rays by the atmosphere changes its color at our level. E.2 - Planckian locus \u00b6 Rather than defining the spectrum of light from black bodies by describing the rays that make it up, we simplify by simply linking this color, this white, to the temperature of the black body that emitted it; and this measurement is given in Kelvin (and could just as easily be converted into degrees Celsius or Fahreneit ). We can thus define a white light directly according to this theoretical temperature. Description Kelvin Degr\u00e9 Celsius Degr\u00e9 Farenheit Hot lava 1000 K 726,85\u00b0C 1340,33\u00b0F Sun at noon 5800 K 5526,85\u00b0C 9980,33\u00b0F Cloudy day 7000K 6726,85\u00b0C 12140,33\u00b0F Lightning 9000 K 8726,85\u00b0C 15 740,33\u00b0F We note that the temperature of white reflects its hue and not the actual temperature of the body that emits this light; this temperature is that of the black body that would have emitted radiation of the same color , but not the temperature nor the energy of the element seen. We use these temperature values in Kelvin by convention to standardize the description of white. All these temperatures can be represented in a degree going from yellow-orange to blue, in a whitish range (these are indeed colors resulting from a complex blend of monochromatic rays [*][ZZ-vocabulaire.md]* of the visible spectrum; only the proportions change 3 ). We call this set of colors distributed in a line the Planckian Locus . E.3 - White balance \u00b6 Under the sun, in the shade or in the clouds, no matter what time of day it is, in reality we perceive snow as white *even though the light it reflects is actually different each time. The brain is constantly making adjustments to compensate for the color of the light illuminating a scene, and always sees the whites\u2026 white. The problem arises when capturing colors via a camera or a camera to make the same adjustment. Indeed, a sensor that would receive white at noon in the sun would actually \u201csee\u201d the \u201ctrue\u201d color, tending more towards yellow, while the same object on a cloudy day would be \u201cseen\u201d by the sensor as much more blue. An operation of correction of these colors is then carried out to bring back the color of the object to a neutral \u201cwhite\u201d: that of the device of reproduction. We call this white balance : \u201cerasing\u201d the influence of the light illuminating the scene at the time of the capture. Thus, this \u201cneutralized\u201d image can be reproduced by the device under standard conditions. To display the image on a screen, the task is to bring the color of the white in the scene ( 5800 K under the sun or 7000 K under the clouds for example) to that of the white of the screen (in general the one named D65 , 6500 K ). When printing on paper, the white of the scene must be brought back to the white of the paper. Once the image is printed, it can be seen correctly under all possible lighting. Without this work of \u201cneutralization\u201d, the image is shifted compared to the reference white (the white of the screen or the white of the paper) and appears bluish or orange. Sources & References Black body on Wikipedia . Planckian locus on Wikipedia Color temperature on Wikipedia . Only black holes, which absorb all light by gravitation, can be considered as almost perfect black bodies and do not emit any radiation themselves; but it is now known that even black holes emit a very weak radiation, in a somewhat roundabout way, the Hawking radiation . On the other hand, we still don\u2019t really know if this radiation depends on the composition of the black hole (what is called the information paradox , but it is a completely different subject than color\u2026). \u21a9 Theoretically, even the sun reflects the rays it receives; but the proportion between the reflected rays and the emitted rays is so insignificant that we can consider the sun as a black body. And on our scale it is the same for fire, embers, sparks\u2026 \u21a9 The more energy the body has (the hotter it is), the higher the proportion of rays of high frequency * (and low wavelength * ) : indeed, the amount of energy carried by light rays depends directly on their frequency. The shorter wavelength rays are those of the blue side of the spectrum. Thus, the hotter the body, the more the proportion of blue rays increases in the emitted blend, the more the color moves away from the orange-red to the blue. But we must keep in mind that the light emitted remains a combination, and that we are talking about shades of white. The discovery of this link between wavelength and energy, and the work on the color of black bodies by the physicist Max Planck at the end of the XIX th century are at the origin of modern quantum physics , with the discovery by Planck that energy is composed of discrete values and not continuous (this is the foundation prior to the theorization of the photon by Albert Einstein in 1905). \u21a9","title":"E - About white values-temperature"},{"location":"E-temperature.html#ie-about-white-values-temperature","text":"White being perceived differently depending on the environment and the observer, we must, before talking about it, define exactly what is white. Once again, we need an invariant, physical referential, on which everyone can agree, a bit like the use of Fraunhofer\u2019s rays can be used as a reference to define precise monochromatic lights * . Radiation of black bodies * is commonly used to define white. I.E - About white values : temperature E.1 - Black body E.2 - Planckian locus E.3 - White balance","title":"I.E - About white values : temperature"},{"location":"E-temperature.html#e1-black-body","text":"UA black body is an element which does not necessarily appear black , but which is black in the sense that it absorbs all the electromagnetic rays (and thus visible light) that it receives; in other words, if the black body were perfectly cold 1 , it would be pure black. But every body has some heat, and this heat causes it to emit radiation, some of it in the visible spectrum, which explains why what is called a black body is not seen as black. The consequence is that the light, and therefore the color, of a black body is not influenced at all by the light it receives, but is only the result of its heat. Several elements can be considered as black bodies: the embers of a barbecue, fire, a glowing metal, the sun \u2026 All these elements are not real black bodies in the physical sense (the radiation they emit is not perfectly independent of the radiation they receive) but are an approximation, very close in the case of the sun 2 . Black bodies are interesting because the light they emit, and therefore their perceived color, does not depend on the environment in which they are; they therefore form good objective reference frames. Moreover, we know that their radiation spectrum depends directly on their energy, their heat. One can thus associate to a given heat a given spectrum, i.e. a light of a precise composition of monochromatic rays * , resulting in a precise and measurable white * . Note Unlike the other incandescent elements in the examples above, the apparent color of the sun is not directly the result of its heat even though it is a black body; indeed, the absorption of rays by the atmosphere changes its color at our level.","title":"E.1 - Black body"},{"location":"E-temperature.html#e2-planckian-locus","text":"Rather than defining the spectrum of light from black bodies by describing the rays that make it up, we simplify by simply linking this color, this white, to the temperature of the black body that emitted it; and this measurement is given in Kelvin (and could just as easily be converted into degrees Celsius or Fahreneit ). We can thus define a white light directly according to this theoretical temperature. Description Kelvin Degr\u00e9 Celsius Degr\u00e9 Farenheit Hot lava 1000 K 726,85\u00b0C 1340,33\u00b0F Sun at noon 5800 K 5526,85\u00b0C 9980,33\u00b0F Cloudy day 7000K 6726,85\u00b0C 12140,33\u00b0F Lightning 9000 K 8726,85\u00b0C 15 740,33\u00b0F We note that the temperature of white reflects its hue and not the actual temperature of the body that emits this light; this temperature is that of the black body that would have emitted radiation of the same color , but not the temperature nor the energy of the element seen. We use these temperature values in Kelvin by convention to standardize the description of white. All these temperatures can be represented in a degree going from yellow-orange to blue, in a whitish range (these are indeed colors resulting from a complex blend of monochromatic rays [*][ZZ-vocabulaire.md]* of the visible spectrum; only the proportions change 3 ). We call this set of colors distributed in a line the Planckian Locus .","title":"E.2 - Planckian locus"},{"location":"E-temperature.html#e3-white-balance","text":"Under the sun, in the shade or in the clouds, no matter what time of day it is, in reality we perceive snow as white *even though the light it reflects is actually different each time. The brain is constantly making adjustments to compensate for the color of the light illuminating a scene, and always sees the whites\u2026 white. The problem arises when capturing colors via a camera or a camera to make the same adjustment. Indeed, a sensor that would receive white at noon in the sun would actually \u201csee\u201d the \u201ctrue\u201d color, tending more towards yellow, while the same object on a cloudy day would be \u201cseen\u201d by the sensor as much more blue. An operation of correction of these colors is then carried out to bring back the color of the object to a neutral \u201cwhite\u201d: that of the device of reproduction. We call this white balance : \u201cerasing\u201d the influence of the light illuminating the scene at the time of the capture. Thus, this \u201cneutralized\u201d image can be reproduced by the device under standard conditions. To display the image on a screen, the task is to bring the color of the white in the scene ( 5800 K under the sun or 7000 K under the clouds for example) to that of the white of the screen (in general the one named D65 , 6500 K ). When printing on paper, the white of the scene must be brought back to the white of the paper. Once the image is printed, it can be seen correctly under all possible lighting. Without this work of \u201cneutralization\u201d, the image is shifted compared to the reference white (the white of the screen or the white of the paper) and appears bluish or orange. Sources & References Black body on Wikipedia . Planckian locus on Wikipedia Color temperature on Wikipedia . Only black holes, which absorb all light by gravitation, can be considered as almost perfect black bodies and do not emit any radiation themselves; but it is now known that even black holes emit a very weak radiation, in a somewhat roundabout way, the Hawking radiation . On the other hand, we still don\u2019t really know if this radiation depends on the composition of the black hole (what is called the information paradox , but it is a completely different subject than color\u2026). \u21a9 Theoretically, even the sun reflects the rays it receives; but the proportion between the reflected rays and the emitted rays is so insignificant that we can consider the sun as a black body. And on our scale it is the same for fire, embers, sparks\u2026 \u21a9 The more energy the body has (the hotter it is), the higher the proportion of rays of high frequency * (and low wavelength * ) : indeed, the amount of energy carried by light rays depends directly on their frequency. The shorter wavelength rays are those of the blue side of the spectrum. Thus, the hotter the body, the more the proportion of blue rays increases in the emitted blend, the more the color moves away from the orange-red to the blue. But we must keep in mind that the light emitted remains a combination, and that we are talking about shades of white. The discovery of this link between wavelength and energy, and the work on the color of black bodies by the physicist Max Planck at the end of the XIX th century are at the origin of modern quantum physics , with the discovery by Planck that energy is composed of discrete values and not continuous (this is the foundation prior to the theorization of the photon by Albert Einstein in 1905). \u21a9","title":"E.3 - White balance"},{"location":"F-representation.html","text":"I.F - Objective representation of colors \u00b6 Given that the perception of colors is eminently subjective, any joint work requires the establishment of an objective reference shared by all (and therefore a certain standardization of techniques, which is not always obvious \u2026). I.F - Objective representation of colors F.1 - Decomposing colors F.2 - Color diagrams, CIE XYZ of 1931 and CIE xyZ F.3 - Other CIE Spaces This work of rationalization was not born with the digital image; since 1913 the International Commission on Illumination ( CIE ) has worked on the problem. The modern color spaces 1 used in image processing are proposed solutions to this rationalization, standardization of color representation, and are based on the work and early attempts of the CIE. It is interesting to know that the CIE conducted its work empirically, defining an \u201caverage observer\u201d from numerous experiments of color comparison by human observers, in order to characterize the colors and lights as perceived by the average person. It is in 1931 that the CIE proposed a first representation and rationalization of the colors: the diagram CIE-1931 , still very used today as an objective reference, in particular to compare the various colorimetric spaces in use. F.1 - Decomposing colors \u00b6 It has been implied so far that a color as we perceive it is physically defined by three independent parameters: its intensity , its dominant wavelength and its Extraction purity . Intensity is quite intuitive to understand, and related to what is also called luminosity ; it is the number of photons received each second by the cells of the retina. Dominant wavelength will primarily influence the hue of the color; it is the most intense monochromatic component in the mixture of all wavelengths forming that particular color. Extraction purity represents the proportion between the dominant wavelength and the amount of white that must be added to obtain the color in question. It is close to what is called saturation . This breakdown is perfectly objective (related to the physical reality of light), it is a good foundation for making an objective representation of colors, which is what the CIE has done since 1931. F.2 - Color diagrams, CIE XYZ of 1931 and CIE xyZ \u00b6 With these three parameters, we can represent the colors in three dimensions. In order to establish this representation, the CIE chose three theoretical primary colors different from Red, Green and Blue more common, called X, Y and Z, able to include the entirety of the visible colors. This representation is thus the CIE XYZ colorimetric space of 1931 , and is still used today as reference to represent and compare all the other colorimetric spaces. The parameters defining this color space were carefully chosen in order to represent the entirety of the colors perceived by human vision. Note The three-dimensional representation above is not exact, but does illustrate the general feel of the color space. For better and easier use and visualization, it is mainly represented in a two-dimensional projection, not showing the intensity (brightness) of the color at all. This projection actually is actually translated to another color space derived from the CIE XYZ where the color is represented on a plane by coordinates named x and y (in lower case) which makes it the CIE xyZ space. On this diagram, we find the spectrum of visible light, on the upper turn, rounded, going from red to blue. This outline contains all the possible monochromatic lights, while the inside of the diagram represents the \u201cmixed\u201d colors as we perceive them. The lower straight line represents the blending of the two extremes of the visible spectrum, which we see as the purple colors, which are not part of the monochromatic lights and close the \u201ccolor wheel\u201d we know. This diagram containing all the visible colors is always used as reference in which one can register the other colorimetric spaces , necessarily \u201csmaller\u201d because representing only a subpart of all these colors, as we will see thereafter. F.3 - Other CIE Spaces \u00b6 Since the establishment of these first color spaces and until today, the CIE has continued this work, improving the xyZ space and creating other spaces more specific and for particular uses. In 1976, two other spaces were published: the CIE L*u*v* (for light) and the CIE L*a*b* (for surface colors, better known as CIE LAB ). These two spaces improve and compensate for a \u201cdefect\u201d of the xyZ 2 : the coordinates are no longer linear in order to better match the human vision. Indeed, in the xyZ space, two colors located \u201cat the same distance\u201d can in some areas appear more similar than in other areas. The L*a*b* and L*u*v* correct this \u201cdefect\u201d at the price of a greater complexity of calculations. But it is always the XYZ or xyZ of 1931 which is used as reference to work with and compare all the other colorimetric spaces. Sources et r\u00e9f\u00e9rences Color on Wikipedia International Commission on Illumination on Wikipedia . CIE XYZ on Wikipedia L*a*b* CIE on Wikipedia L*u*v* CIE on Wikipedia Colour representation, Kent State University The color spaces are the standardized ways of recording and representing the colors (analog as well as digital). There are for example: sRGB , BT.709 , ACES , BT.2020 , P3 , to name a few from a very large list. \u21a9 L*a*b* and L*u*v* are actually based on another space published in 1976, the CIE U\u2019V\u2019W\u2019 which is linear (and itself based on the CIE UVW of 1960). The chronology of publication of these successive spaces is as follows: - 1931: XYV , and its representation xyZ , linear. - 1960: UVW , and its representation uvW , linear. - 1976 : U\u2019V\u2019W\u2019 , linear. - 1976 : L*a*b* and L*u*v* , nonlinear. \u21a9","title":"F - Objective representation of colors"},{"location":"F-representation.html#if-objective-representation-of-colors","text":"Given that the perception of colors is eminently subjective, any joint work requires the establishment of an objective reference shared by all (and therefore a certain standardization of techniques, which is not always obvious \u2026). I.F - Objective representation of colors F.1 - Decomposing colors F.2 - Color diagrams, CIE XYZ of 1931 and CIE xyZ F.3 - Other CIE Spaces This work of rationalization was not born with the digital image; since 1913 the International Commission on Illumination ( CIE ) has worked on the problem. The modern color spaces 1 used in image processing are proposed solutions to this rationalization, standardization of color representation, and are based on the work and early attempts of the CIE. It is interesting to know that the CIE conducted its work empirically, defining an \u201caverage observer\u201d from numerous experiments of color comparison by human observers, in order to characterize the colors and lights as perceived by the average person. It is in 1931 that the CIE proposed a first representation and rationalization of the colors: the diagram CIE-1931 , still very used today as an objective reference, in particular to compare the various colorimetric spaces in use.","title":"I.F - Objective representation of colors"},{"location":"F-representation.html#f1-decomposing-colors","text":"It has been implied so far that a color as we perceive it is physically defined by three independent parameters: its intensity , its dominant wavelength and its Extraction purity . Intensity is quite intuitive to understand, and related to what is also called luminosity ; it is the number of photons received each second by the cells of the retina. Dominant wavelength will primarily influence the hue of the color; it is the most intense monochromatic component in the mixture of all wavelengths forming that particular color. Extraction purity represents the proportion between the dominant wavelength and the amount of white that must be added to obtain the color in question. It is close to what is called saturation . This breakdown is perfectly objective (related to the physical reality of light), it is a good foundation for making an objective representation of colors, which is what the CIE has done since 1931.","title":"F.1 - Decomposing colors"},{"location":"F-representation.html#f2-color-diagrams-cie-xyz-of-1931-and-cie-xyz","text":"With these three parameters, we can represent the colors in three dimensions. In order to establish this representation, the CIE chose three theoretical primary colors different from Red, Green and Blue more common, called X, Y and Z, able to include the entirety of the visible colors. This representation is thus the CIE XYZ colorimetric space of 1931 , and is still used today as reference to represent and compare all the other colorimetric spaces. The parameters defining this color space were carefully chosen in order to represent the entirety of the colors perceived by human vision. Note The three-dimensional representation above is not exact, but does illustrate the general feel of the color space. For better and easier use and visualization, it is mainly represented in a two-dimensional projection, not showing the intensity (brightness) of the color at all. This projection actually is actually translated to another color space derived from the CIE XYZ where the color is represented on a plane by coordinates named x and y (in lower case) which makes it the CIE xyZ space. On this diagram, we find the spectrum of visible light, on the upper turn, rounded, going from red to blue. This outline contains all the possible monochromatic lights, while the inside of the diagram represents the \u201cmixed\u201d colors as we perceive them. The lower straight line represents the blending of the two extremes of the visible spectrum, which we see as the purple colors, which are not part of the monochromatic lights and close the \u201ccolor wheel\u201d we know. This diagram containing all the visible colors is always used as reference in which one can register the other colorimetric spaces , necessarily \u201csmaller\u201d because representing only a subpart of all these colors, as we will see thereafter.","title":"F.2 - Color diagrams, CIE XYZ of 1931 and CIE xyZ"},{"location":"F-representation.html#f3-other-cie-spaces","text":"Since the establishment of these first color spaces and until today, the CIE has continued this work, improving the xyZ space and creating other spaces more specific and for particular uses. In 1976, two other spaces were published: the CIE L*u*v* (for light) and the CIE L*a*b* (for surface colors, better known as CIE LAB ). These two spaces improve and compensate for a \u201cdefect\u201d of the xyZ 2 : the coordinates are no longer linear in order to better match the human vision. Indeed, in the xyZ space, two colors located \u201cat the same distance\u201d can in some areas appear more similar than in other areas. The L*a*b* and L*u*v* correct this \u201cdefect\u201d at the price of a greater complexity of calculations. But it is always the XYZ or xyZ of 1931 which is used as reference to work with and compare all the other colorimetric spaces. Sources et r\u00e9f\u00e9rences Color on Wikipedia International Commission on Illumination on Wikipedia . CIE XYZ on Wikipedia L*a*b* CIE on Wikipedia L*u*v* CIE on Wikipedia Colour representation, Kent State University The color spaces are the standardized ways of recording and representing the colors (analog as well as digital). There are for example: sRGB , BT.709 , ACES , BT.2020 , P3 , to name a few from a very large list. \u21a9 L*a*b* and L*u*v* are actually based on another space published in 1976, the CIE U\u2019V\u2019W\u2019 which is linear (and itself based on the CIE UVW of 1960). The chronology of publication of these successive spaces is as follows: - 1931: XYV , and its representation xyZ , linear. - 1960: UVW , and its representation uvW , linear. - 1976 : U\u2019V\u2019W\u2019 , linear. - 1976 : L*a*b* and L*u*v* , nonlinear. \u21a9","title":"F.3 - Other CIE Spaces"},{"location":"G-numerisation.html","text":"I.G - From real life to digital \u00b6 Having now a way to objectively represent the colors we see, to locate them on a reference diagram, we can ask ourselves the question of how to digitize, store, then restore these colors via a binary system. I.G - From real life to digital G.1 - Digitization and storage: converting to binary G.2 - The color spaces G.1 - Digitization and storage: converting to binary \u00b6 The difficulty is to represent an analog and continuous world 1 in a fundamentally discontinuous and digital system, with discrete values . In binary numerics, any value, whether it is a number or a color, must be stored in a sequence of zeros and ones, which are called bits * 2 . It is therefore impossible to represent the whole set of real numbers in numerical form, and we have to quantify , to cut the infinite number of colors we can see into a finite number of numerical values. This is sampling . The more we cut these values into small quanta, small samples, small bricks, the more we gain in precision and the more we can approach a faithful representation of reality; but the question of storage and the necessary space arises. Let\u2019s do a simple calculation: let\u2019s take a digital image divided into a \u201cgrid\u201d of 1920 pixels in width and 1080 pixels in height, the resolution of a standard digital video (in 2020). This image therefore contains 1920 x 1080 = 2,073,600 pixels . In its most common form, a pixel is represented by three intensity values for three primary colors. Each of these three values is usually stored in at least 8 bits. Each pixel therefore needs 3 x 8 = 24 bits * or 3 bytes * to be stored (a byte being formed of 8 bits). Note that this choice of using one byte (8 bits) per primary color allows \u201conly\u201d 256 values per primary ( 2^8 ), that is to say a total of about 65 million ( 256^3 ) different color shades. This may seem like a lot, but it is still far from the shades perceived by the human eye, and the number of shades used in movies. So our image needs 2,073,600 pixels x 3 bytes = 6,220,800 bytes to be stored. 6 MB for a single image, which means at least 6 x 24 = 144 MB for one second of video, or 144 x 60 = 8,640 MB for one minute, more than 8 GB 3 ! 1920 px x 1080 px x 3 primaries x 8 bits x 24 fps x 60 s \u2248 64 Gb/min \u2248 8 Gb/min This size represents a data rate of about 1.2 Gbps. As an example, a movie on a Blu-Ray disc is encoded with a data rate of about 24 Mbps, which means that this size must be divided by about 50\u2026 G.2 - The color spaces \u00b6 It is thus essential to find methods making it possible to reduce the place occupied by all this information; it is there that the various choices of colorimetric spaces and methods of compression and sampling of the associated data come into play. This problem appeared long before the advent of digital technology: analog data flows, via electrical signals, also have a limited transfer capacity (bandwidth), and even much more limited than digital information transmission using the same copper cables, which explains the format limitations imposed in the early days of video, not to mention the storage problems. The way of storing the sampled (digitized) luminous information therefore depends on the material used for the capture (the camera) or the production (in synthesis), as well as on the use that will be made of it (only what can be reproduced is stored), and influences the quantity of data to be stored. The choice of how to store the data will also define which data will be lost , since it is impossible to remain faithful to the real thing, or which types of lights and data will be prioritized (intensity or hue, darkness or highlights\u2026). The system of storage of the data of light is what we call the color space . Sources & References Discrete mathematics on Wikipedia . With the advent of quantum physics , we now know that the world is in fact also discontinuous and divided into quanta of energy and matter; simply these discrete values are so small that they will always be imperceptible. This argument is sometimes used to try to demonstrate that we are actually living in a digital simulation (of extreme precision), but that is another subject\u2026 \u21a9 And 8 bits represent 1 byte, the unit most commonly used in storage. \u21a9 That\u2019s about 25% of the capacity of a standard Blu-Ray \u2026 \u21a9","title":"G - From real life to digital"},{"location":"G-numerisation.html#ig-from-real-life-to-digital","text":"Having now a way to objectively represent the colors we see, to locate them on a reference diagram, we can ask ourselves the question of how to digitize, store, then restore these colors via a binary system. I.G - From real life to digital G.1 - Digitization and storage: converting to binary G.2 - The color spaces","title":"I.G - From real life to digital"},{"location":"G-numerisation.html#g1-digitization-and-storage-converting-to-binary","text":"The difficulty is to represent an analog and continuous world 1 in a fundamentally discontinuous and digital system, with discrete values . In binary numerics, any value, whether it is a number or a color, must be stored in a sequence of zeros and ones, which are called bits * 2 . It is therefore impossible to represent the whole set of real numbers in numerical form, and we have to quantify , to cut the infinite number of colors we can see into a finite number of numerical values. This is sampling . The more we cut these values into small quanta, small samples, small bricks, the more we gain in precision and the more we can approach a faithful representation of reality; but the question of storage and the necessary space arises. Let\u2019s do a simple calculation: let\u2019s take a digital image divided into a \u201cgrid\u201d of 1920 pixels in width and 1080 pixels in height, the resolution of a standard digital video (in 2020). This image therefore contains 1920 x 1080 = 2,073,600 pixels . In its most common form, a pixel is represented by three intensity values for three primary colors. Each of these three values is usually stored in at least 8 bits. Each pixel therefore needs 3 x 8 = 24 bits * or 3 bytes * to be stored (a byte being formed of 8 bits). Note that this choice of using one byte (8 bits) per primary color allows \u201conly\u201d 256 values per primary ( 2^8 ), that is to say a total of about 65 million ( 256^3 ) different color shades. This may seem like a lot, but it is still far from the shades perceived by the human eye, and the number of shades used in movies. So our image needs 2,073,600 pixels x 3 bytes = 6,220,800 bytes to be stored. 6 MB for a single image, which means at least 6 x 24 = 144 MB for one second of video, or 144 x 60 = 8,640 MB for one minute, more than 8 GB 3 ! 1920 px x 1080 px x 3 primaries x 8 bits x 24 fps x 60 s \u2248 64 Gb/min \u2248 8 Gb/min This size represents a data rate of about 1.2 Gbps. As an example, a movie on a Blu-Ray disc is encoded with a data rate of about 24 Mbps, which means that this size must be divided by about 50\u2026","title":"G.1 - Digitization and storage: converting to binary"},{"location":"G-numerisation.html#g2-the-color-spaces","text":"It is thus essential to find methods making it possible to reduce the place occupied by all this information; it is there that the various choices of colorimetric spaces and methods of compression and sampling of the associated data come into play. This problem appeared long before the advent of digital technology: analog data flows, via electrical signals, also have a limited transfer capacity (bandwidth), and even much more limited than digital information transmission using the same copper cables, which explains the format limitations imposed in the early days of video, not to mention the storage problems. The way of storing the sampled (digitized) luminous information therefore depends on the material used for the capture (the camera) or the production (in synthesis), as well as on the use that will be made of it (only what can be reproduced is stored), and influences the quantity of data to be stored. The choice of how to store the data will also define which data will be lost , since it is impossible to remain faithful to the real thing, or which types of lights and data will be prioritized (intensity or hue, darkness or highlights\u2026). The system of storage of the data of light is what we call the color space . Sources & References Discrete mathematics on Wikipedia . With the advent of quantum physics , we now know that the world is in fact also discontinuous and divided into quanta of energy and matter; simply these discrete values are so small that they will always be imperceptible. This argument is sometimes used to try to demonstrate that we are actually living in a digital simulation (of extreme precision), but that is another subject\u2026 \u21a9 And 8 bits represent 1 byte, the unit most commonly used in storage. \u21a9 That\u2019s about 25% of the capacity of a standard Blu-Ray \u2026 \u21a9","title":"G.2 - The color spaces"},{"location":"H-espace-colo.html","text":"I.H - What is a color space? \u00b6 A color space defines a subset of colors (a volume in a 3D color diagram, or a surface on a 2D projection like the CIE xy ) drawn from the visible spectrum (or even outside the visible spectrum) 1 . Note The color space describes the colors as they should be reproduced, but does not define the technical means of recording them in the files. See chapter K - Pixel Format on this subject. I.H - What is a color space? H.1 - What defines a color space H.1.a - Primaries and gamut H.1.b - White point H.1.c - The transfer curve H.2 - Other parameters H.2.a - Pixel format H.2.b - Depth H.3 - Why different color spaces? H.1 - What defines a color space \u00b6 The whole of the colors being able to be represented by a colorimetric space depends on a principal parameter: The primary colors of the color space. The way of representing these colors depends on two other parameters which supplement the definition of a colorimetric space: The white point and the transfer curve . H.1.a - Primaries and gamut \u00b6 Most often with the number of 3, the primary colors are the paramount values of the colorimetric space; they can be represented by precise co-ordinates in the space CIE XYZ (which is a representation of all the visible colors on three axes). They are in the majority of the cases a shade of Red , Green and Blue , and are ideally close as possible to a real monochromatic color (the external edge of the diagram CIE xy ). Each space defines its primary colors by at least three coordinates in the space CIE XYZ . For example, here are the coordinates of the Red, Green and Blue sRGB (standard Red, Green, Blue) space: R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79 This means that for two given spaces, the most intense \u201cRed\u201d value is not concretely reproduced by the same shade: some spaces will have a more or less intense or saturated primary red (or less close to monochromatic waves), more or less orange, for example. The colorimetric space thus defines the way in which the most intense value of a primary color must be reproduced by the physical device which will display it (the screen, the projector, or the printer, etc). In other words, each device of reproduction must be able to control precisely which light rays, at which wavelengths, which physical colors it is able to produce; according to the range of these wavelengths, the device is compatible with various colorimetric spaces if it is able to produce the wavelengths necessary to the reproduction of the primaries of space. The coordinates of the primary define the limits of a subspace including all the colors which can be represented in the colorimetric space. This subspace is more or less large (it allows to represent more or less different colors); the size of this space is what is called the gamut . H.1.b - White point \u00b6 We have seen that in the human perception, what is regarded as white varies enormously according to the conditions, of the environment in which we perceive the colors. It is t therefore necessary to define in the colorimetric space the color which will be regarded as white , corresponding concretely to that which must be obtained by the mixing in equal proportion of the various primary colors (white with the maximum intensity, gray with lower intensities). This white point is also given by its coordinates in the space CIE XYZ . For example, here are the coordinates of the white point in the sRGB space: White x 0,3127 y 0,3290 The CIE has defined a number of remarkable white dots (also called illuminant ), useful for particular purposes. They are represented by letters (from A to F ), sometimes accompanied by a number in the case of series of white points. A few examples : The illuminant A represents the color emitted by a tungsten filament of the old domestic bulbs; in other words the color of a black body of 2856 Kelvin (cf Color temperature ). The illuminant B represents direct sunlight. The D series of illuminants represent daylight in different conditions (e.g. D65 represents light under a cloudy sky). The illuminant D65 is the one most commonly used as a white point in the various colorimetric spaces. Note The white point is sometimes itself, by simplification, considered as a primary (in the sense that it is also simply a precise color, a coordinate in the CIE XYZ ). In this case the primaries of the space are at least 4 and the space is only defined by two parameters. The color space normally also associates with the white point its maximum intensity, its luminance, most often measured in cd/m\u00b2 . H.1.c - The transfer curve \u00b6 Colors in a color space are given by the intensity of each primary, represented by a value ranging from 0.0 to 1.0 . The value 0.0 represents a zero intensity (the absence of light, a perfect black) and the value 1.0 represents the color given by the coordinate of the primary in the CIE XYZ . The question arises as to how the intermediate values are transcribed into real colors (or coordinates in the CIE XYZ ). This transcription is done by the transfer curve . It is a \u201csimple\u201d mathematical function which, for a given value in the colorimetric space, gives the corresponding coordinate in the CIE XYZ . The space is said linear when this curve is a simple affine function, ie represented by a straight line, and whose correspondence between the values is just proportional. Other spaces use a more complex transfer curve to better reflect human vision and/or the way early CRTs reproduced intensities 2 (with a gamma 2.4 ); the \u201csimplest\u201d of these curves are the so-called gamma curves. This being said, most spaces do not use simple gamma curves but more complex ones. See the section entitled L - Transfer curves, linear space and gamma for more details. H.2 - Other parameters \u00b6 The color spaces can also impose or recommend other parameters, especially on the way of encoding/storing the colors, according to their intended uses. H.2.a - Pixel format \u00b6 For storing pixels, spaces can recommend different formats. The two most common are: RGB, where the pixel contains three values: red, green and blue. YUV, where the pixel contains one value of luminance and two of chrominance . See section K.1 - RGB and YUV for more details. H.2.b - Depth \u00b6 Spaces may specify a specific number of bits to be used for each pixel to encode each color. For example sRGB specifies a minimum of 8 bits, and Rec. 2020 specifies 10 or 12 bits. See section K.3 - Depth (bpc) for more details. H.3 - Why different color spaces? \u00b6 The color spaces are (very) numerous. There is in fact as many as different uses of the colors, without counting that the manufacturers of materials, and designers of software, sometimes add their own spaces to the already long list. The uses are relatively unstandardized. Thus, certain spaces are related to precise materials of capture: a camera can record the data which it captures in a space which is specific to it, and corresponds to the capacities of its sensor, or to the use which will be made of the images (by proposing for example a broad range of colors which facilitates the post-production) Other spaces are linked to reproduction equipment (screens, projectors, printers\u2026) and represent the colors that the equipment is capable of reproducing. Finally, more recently, spaces were created specifically for imagery work; they are then used temporarily and are different from the spaces used to capture, record or reproduce the images. It is thus necessary to keep in mind that several colorimetric spaces come into play at the time of work on an image: The color space of the imported and used media. The color space of the display used for the display. The working color space, used to make the calculations on the image, by the computer. The color space used to record and store the output data (and later on the destination display of the media). These spaces are classified into two categories: Display-referred or Output-referred : Spaces used for display and reproduction. Scene-referred : Spaces used for calculation. Not all of these spaces are necessarily the same (and are rarely the same in reality), each having a specific use and corresponding to specific needs; a certain number of conversions then come into play to pass from one space to another. This is where the ability to convert information to and from a reference space (the CIE XYZ ) allows one to work safely with multiple spaces, with each space defining its primary , white point and transfer curve relative to the same reference space. Warning It is a common mistake to think that you have to work in the space used for output ( output-referred ) when the output-referred spaces were intended for display, not calculation. The scene-referred spaces are intended for work. Sources & References Illuminant on Wikipedia It doesn\u2019t define a number of colors, of different shades (which depends on the sampling, on the way the values are stored digitally), but a limited range of colors in the set of real colors; in other words, it defines the size of the set of colors, but not the number of subdvisions of the set, which can be more or less fine in its concrete implementation. See the section Pixel format for more details. \u21a9 The transfer curves during the analog era represented in fact the correspondence between the intensity of the electric signal, and the luminosity captured or reproduced by the camera or the screen; the modern gamma * follows the same rules to link the luminous intensity to the digital value \u21a9","title":"H - What is a color space?"},{"location":"H-espace-colo.html#ih-what-is-a-color-space","text":"A color space defines a subset of colors (a volume in a 3D color diagram, or a surface on a 2D projection like the CIE xy ) drawn from the visible spectrum (or even outside the visible spectrum) 1 . Note The color space describes the colors as they should be reproduced, but does not define the technical means of recording them in the files. See chapter K - Pixel Format on this subject. I.H - What is a color space? H.1 - What defines a color space H.1.a - Primaries and gamut H.1.b - White point H.1.c - The transfer curve H.2 - Other parameters H.2.a - Pixel format H.2.b - Depth H.3 - Why different color spaces?","title":"I.H - What is a color space?"},{"location":"H-espace-colo.html#h1-what-defines-a-color-space","text":"The whole of the colors being able to be represented by a colorimetric space depends on a principal parameter: The primary colors of the color space. The way of representing these colors depends on two other parameters which supplement the definition of a colorimetric space: The white point and the transfer curve .","title":"H.1 - What defines a color space"},{"location":"H-espace-colo.html#h1a-primaries-and-gamut","text":"Most often with the number of 3, the primary colors are the paramount values of the colorimetric space; they can be represented by precise co-ordinates in the space CIE XYZ (which is a representation of all the visible colors on three axes). They are in the majority of the cases a shade of Red , Green and Blue , and are ideally close as possible to a real monochromatic color (the external edge of the diagram CIE xy ). Each space defines its primary colors by at least three coordinates in the space CIE XYZ . For example, here are the coordinates of the Red, Green and Blue sRGB (standard Red, Green, Blue) space: R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79 This means that for two given spaces, the most intense \u201cRed\u201d value is not concretely reproduced by the same shade: some spaces will have a more or less intense or saturated primary red (or less close to monochromatic waves), more or less orange, for example. The colorimetric space thus defines the way in which the most intense value of a primary color must be reproduced by the physical device which will display it (the screen, the projector, or the printer, etc). In other words, each device of reproduction must be able to control precisely which light rays, at which wavelengths, which physical colors it is able to produce; according to the range of these wavelengths, the device is compatible with various colorimetric spaces if it is able to produce the wavelengths necessary to the reproduction of the primaries of space. The coordinates of the primary define the limits of a subspace including all the colors which can be represented in the colorimetric space. This subspace is more or less large (it allows to represent more or less different colors); the size of this space is what is called the gamut .","title":"H.1.a - Primaries and gamut"},{"location":"H-espace-colo.html#h1b-white-point","text":"We have seen that in the human perception, what is regarded as white varies enormously according to the conditions, of the environment in which we perceive the colors. It is t therefore necessary to define in the colorimetric space the color which will be regarded as white , corresponding concretely to that which must be obtained by the mixing in equal proportion of the various primary colors (white with the maximum intensity, gray with lower intensities). This white point is also given by its coordinates in the space CIE XYZ . For example, here are the coordinates of the white point in the sRGB space: White x 0,3127 y 0,3290 The CIE has defined a number of remarkable white dots (also called illuminant ), useful for particular purposes. They are represented by letters (from A to F ), sometimes accompanied by a number in the case of series of white points. A few examples : The illuminant A represents the color emitted by a tungsten filament of the old domestic bulbs; in other words the color of a black body of 2856 Kelvin (cf Color temperature ). The illuminant B represents direct sunlight. The D series of illuminants represent daylight in different conditions (e.g. D65 represents light under a cloudy sky). The illuminant D65 is the one most commonly used as a white point in the various colorimetric spaces. Note The white point is sometimes itself, by simplification, considered as a primary (in the sense that it is also simply a precise color, a coordinate in the CIE XYZ ). In this case the primaries of the space are at least 4 and the space is only defined by two parameters. The color space normally also associates with the white point its maximum intensity, its luminance, most often measured in cd/m\u00b2 .","title":"H.1.b - White point"},{"location":"H-espace-colo.html#h1c-the-transfer-curve","text":"Colors in a color space are given by the intensity of each primary, represented by a value ranging from 0.0 to 1.0 . The value 0.0 represents a zero intensity (the absence of light, a perfect black) and the value 1.0 represents the color given by the coordinate of the primary in the CIE XYZ . The question arises as to how the intermediate values are transcribed into real colors (or coordinates in the CIE XYZ ). This transcription is done by the transfer curve . It is a \u201csimple\u201d mathematical function which, for a given value in the colorimetric space, gives the corresponding coordinate in the CIE XYZ . The space is said linear when this curve is a simple affine function, ie represented by a straight line, and whose correspondence between the values is just proportional. Other spaces use a more complex transfer curve to better reflect human vision and/or the way early CRTs reproduced intensities 2 (with a gamma 2.4 ); the \u201csimplest\u201d of these curves are the so-called gamma curves. This being said, most spaces do not use simple gamma curves but more complex ones. See the section entitled L - Transfer curves, linear space and gamma for more details.","title":"H.1.c - The transfer curve"},{"location":"H-espace-colo.html#h2-other-parameters","text":"The color spaces can also impose or recommend other parameters, especially on the way of encoding/storing the colors, according to their intended uses.","title":"H.2 - Other parameters"},{"location":"H-espace-colo.html#h2a-pixel-format","text":"For storing pixels, spaces can recommend different formats. The two most common are: RGB, where the pixel contains three values: red, green and blue. YUV, where the pixel contains one value of luminance and two of chrominance . See section K.1 - RGB and YUV for more details.","title":"H.2.a - Pixel format"},{"location":"H-espace-colo.html#h2b-depth","text":"Spaces may specify a specific number of bits to be used for each pixel to encode each color. For example sRGB specifies a minimum of 8 bits, and Rec. 2020 specifies 10 or 12 bits. See section K.3 - Depth (bpc) for more details.","title":"H.2.b - Depth"},{"location":"H-espace-colo.html#h3-why-different-color-spaces","text":"The color spaces are (very) numerous. There is in fact as many as different uses of the colors, without counting that the manufacturers of materials, and designers of software, sometimes add their own spaces to the already long list. The uses are relatively unstandardized. Thus, certain spaces are related to precise materials of capture: a camera can record the data which it captures in a space which is specific to it, and corresponds to the capacities of its sensor, or to the use which will be made of the images (by proposing for example a broad range of colors which facilitates the post-production) Other spaces are linked to reproduction equipment (screens, projectors, printers\u2026) and represent the colors that the equipment is capable of reproducing. Finally, more recently, spaces were created specifically for imagery work; they are then used temporarily and are different from the spaces used to capture, record or reproduce the images. It is thus necessary to keep in mind that several colorimetric spaces come into play at the time of work on an image: The color space of the imported and used media. The color space of the display used for the display. The working color space, used to make the calculations on the image, by the computer. The color space used to record and store the output data (and later on the destination display of the media). These spaces are classified into two categories: Display-referred or Output-referred : Spaces used for display and reproduction. Scene-referred : Spaces used for calculation. Not all of these spaces are necessarily the same (and are rarely the same in reality), each having a specific use and corresponding to specific needs; a certain number of conversions then come into play to pass from one space to another. This is where the ability to convert information to and from a reference space (the CIE XYZ ) allows one to work safely with multiple spaces, with each space defining its primary , white point and transfer curve relative to the same reference space. Warning It is a common mistake to think that you have to work in the space used for output ( output-referred ) when the output-referred spaces were intended for display, not calculation. The scene-referred spaces are intended for work. Sources & References Illuminant on Wikipedia It doesn\u2019t define a number of colors, of different shades (which depends on the sampling, on the way the values are stored digitally), but a limited range of colors in the set of real colors; in other words, it defines the size of the set of colors, but not the number of subdvisions of the set, which can be more or less fine in its concrete implementation. See the section Pixel format for more details. \u21a9 The transfer curves during the analog era represented in fact the correspondence between the intensity of the electric signal, and the luminosity captured or reproduced by the camera or the screen; the modern gamma * follows the same rules to link the luminous intensity to the digital value \u21a9","title":"H.3 - Why different color spaces?"},{"location":"I-liste-espaces.html","text":"I.I - Selective list of color spaces \u00b6 It would not be possible to list here all the color spaces available, but here is a selection useful in audiovisual production in 2021. I.I - Selective list of color spaces sRGB / CIE 61966-2-1 / IEC 61966-2-1 Primaries White Transfer curve Linear RGB Rec. 601 / ITU-R BT.601 / CCIR 601 Primaries White Transfer curve Rec.709 / UIT-R BT 709 Primaries White Transfer curve Rec.2020 / UIT-R BT.2020 Primaries White Transfer curve DCI-P3 Primaries White Transfer curve Display P3 Primaries White Transfer curve ACES Primaries (AP0) Primaries (AP1) White ACES2065-1 ACEScg ACEScc Adobe RGB Primaries White Transfer curve sRGB / CIE 61966-2-1 / IEC 61966-2-1 \u00b6 Display space, display-referred . The sRGB (for standard RGB) is one of the most common spaces in computing; indeed, it is that of most computer screens, and is also used by extension for most image formats in a standard way ( JPEG , PNG , TGA \u2026). It dates from 1996. Primaries \u00b6 These primaries are the same as those in Rec. 709 . R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79 White \u00b6 White x 0,3127 y 0,3290 CIE D65 lum. 80 cd/m\u00b2 Transfer curve \u00b6 sRGB uses a transfer curve very close on average to a Gamma 2.2 , with actually a linear transfer for linear luminances below 0.0031308 then a shifted Gamma 2.4 . Linear RGB \u00b6 Color space, scene-referred . Linear RGB is a variation of sRGB , identical in all points except for the transfer curve which is * linear . It is mainly used as a workspace ( scene referred ) when we need a better simulation of light (rendering engines, blending modes in compositing \u2026) and to simplify calculations. It is found in particular in EXR* files. Note Often software mistakenly talks about linear SRGB , or worse, IEC 61966-2-1 linear , but it is not sRGB since the transfer curve is different. Rec. 601 / ITU-R BT.601 / CCIR 601 \u00b6 Display space, display-referred . Rec. 601 is the complete standard for interlaced video for Standard Definition Television SDTV . It is no longer used but was used in the PAL and NTSC standards. Its PAL version is very close to sRGB . Primaries \u00b6 R G B X (NTSC) 0,63 0,31 0,155 Y (NTSC) 0,34 0,595 0,007 Z (NTSC) 0,03 0,095 0,775 X (PAL) 0,64 0,29 0,15 Y (PAL) 0,33 0,60 0,06 Z (PAL) 0,03 0,11 0,79 Note In PAL , the primaries of Rec. 601 are very close to those of sRGB ; only the Gx changes (0.29 instead of 0.3) White \u00b6 White x 0,3127 y 0,3290 CIE D65 lum. 100 cd/m\u00b2 Transfer curve \u00b6 Gamma 2.4 . Rec.709 / UIT-R BT 709 \u00b6 Display space, display-referred . Rec. 709 is the standard of the High Definition Television, HDTV and dates from 1990. It is generally found in the current video formats ( mp4 \u2026) It is very close to sRGB , only the transfer curve changes (and its white point must be a little more intense). Primaries \u00b6 These primaries are the same as the sRGB ones. R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79 White \u00b6 White x 0,3127 y 0,3290 CIE D65 lum. 100 cd/m\u00b2 Transfer curve \u00b6 Close to a Gamma 2.4 or Gamma 2.35 according to the recommendations. Rec.2020 / UIT-R BT.2020 \u00b6 Display space, display-referred . Rec. 2020 is the standard for Ultra High Definition Television, UHDTV and dates from 2012. Primaries \u00b6 R G B X 0,708 0,17 0,131 Y 0,292 0,797 0,046 Z 0,0 0,033 0,823 White \u00b6 White x 0,3127 y 0,3290 CIE D65 lum. variable Transfer curve \u00b6 The same as Rec.709 close to a Gamma 2,4 , but more precise. DCI-P3 \u00b6 Display space, display-referred . DCI-P3 was created as a transition to Rec. 2020 and for movie projection. It is used by some TVs and computer monitors, and even more recently some phones/tablets. Primaries \u00b6 R G B X 0,68 0,265 0,15 Y 0,32 0,69 0,06 Z 0,00 0,045 0,79 White \u00b6 White x 0,3140 y 0,3510 CIE N/A lum. 48 cd/m\u00b2 Transfer curve \u00b6 Gamma 2,6 Display P3 \u00b6 Display space, display-referred . The Display P3 is a variation of the DCI-P3 adapted to be closer to (and better compatible with) the sRGB (and thus to make screens able to display both more easily). It keeps the primaries (and thus the wider gamut) of the DCI-P3 but uses the White point and the Transfer curve of the sRGB . Primaries \u00b6 R G B X 0,68 0,265 0,15 Y 0,32 0,69 0,06 Z 0,00 0,045 0,79 White \u00b6 White x 0,3127 y 0,3290 CIE D65 lum. 80 cd/m\u00b2 Transfer curve \u00b6 That of the sRGB close on average to a Gamma 2.2 . ACES \u00b6 ACES stands for Academy Color Encoding System , and represents a system comprising 5 color spaces designed by the Academy of the Oscars specifically for audiovisual production, released in 2014 for its first version. These 5 spaces share the same White point, close to D60 ; they use two sets of Primaries named AP0 or AP1 . The AP0 Primaries are outside the visible colors (and thus the CIE XYZ ): they are the closest theoretical Primaries encompassing all visible colors. The AP1 Primaries are closer to the usual RGB Primaries and to the screens and projectors, making them more practical in the production of images. Primaries (AP0) \u00b6 R G B X 0,7347 0,0 0,001 Y 0,2653 1,0 -0,77 Primaries (AP1) \u00b6 R G B X 0,713 0,165 0,128 Y 0,293 1,830 0,044 White \u00b6 White x 0,32168 y 0,33767 CIE proche de D60 lum. infinie ACES2065-1 \u00b6 Storage space . ACES2065-1 is the main space of the ACES system. It is designed to store any color information in a non-destructive way and is usable in the long term with future new spaces. It thus encompasses all visible colors and is larger than the CIE XYZ . Its use is mainly theoretical. It is intended to be used in openEXR images or MXF videos. It is linear. ACEScg \u00b6 Workspace, scene-referred . ACEScg is similar to ACES2065-1 but uses the AP1 Primaries. It is intended primarily for 3D renderers and compositing. Its standard image storage format is openEXR . ACEScc \u00b6 Workspace, scene-referred . ACEScc is similar to ACEScg (with AP1 Primaries) but uses a non-linear Transfer curve, more convenient for color correction and calibration. Adobe RGB \u00b6 Display space, display-referred and working space. Adobe RGB was designed in 1998 for graphic designers working on screen but whose work is intended for printing. It is close to sRGB , but the green primary has been shifted to include more colors that can be reproduced in CMYK printing. Its Transfer curve is also slightly different. Primaries \u00b6 R G B X 0,64 0,21 0,15 Y 0,33 0,71 0,06 White \u00b6 White x 0,3127 y 0,3290 CIE D65 lum. 160 cd/m\u00b2 Transfer curve \u00b6 Gamma 563/256 soit 2,199 218 75 . Sources et r\u00e9f\u00e9rences sRGB on Wikipedia Specifications sRGB on color.org Specifications BT.601 on color.org Rec. 709 on Wikipedia Specifications BT.709 on color.org Rec. 2020 on Wikipedia Specifications BT.2020 on color.org DCI-P3 on Wikipedia Specifications DCI-P3 on color.org ACES on Wikipedia acescolorspace.com/","title":"I - Selective list of color spaces"},{"location":"I-liste-espaces.html#ii-selective-list-of-color-spaces","text":"It would not be possible to list here all the color spaces available, but here is a selection useful in audiovisual production in 2021. I.I - Selective list of color spaces sRGB / CIE 61966-2-1 / IEC 61966-2-1 Primaries White Transfer curve Linear RGB Rec. 601 / ITU-R BT.601 / CCIR 601 Primaries White Transfer curve Rec.709 / UIT-R BT 709 Primaries White Transfer curve Rec.2020 / UIT-R BT.2020 Primaries White Transfer curve DCI-P3 Primaries White Transfer curve Display P3 Primaries White Transfer curve ACES Primaries (AP0) Primaries (AP1) White ACES2065-1 ACEScg ACEScc Adobe RGB Primaries White Transfer curve","title":"I.I - Selective list of color spaces"},{"location":"I-liste-espaces.html#srgb-cie-61966-2-1-iec-61966-2-1","text":"Display space, display-referred . The sRGB (for standard RGB) is one of the most common spaces in computing; indeed, it is that of most computer screens, and is also used by extension for most image formats in a standard way ( JPEG , PNG , TGA \u2026). It dates from 1996.","title":"sRGB / CIE 61966-2-1 / IEC 61966-2-1"},{"location":"I-liste-espaces.html#primaries","text":"These primaries are the same as those in Rec. 709 . R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79","title":"Primaries"},{"location":"I-liste-espaces.html#white","text":"White x 0,3127 y 0,3290 CIE D65 lum. 80 cd/m\u00b2","title":"White"},{"location":"I-liste-espaces.html#transfer-curve","text":"sRGB uses a transfer curve very close on average to a Gamma 2.2 , with actually a linear transfer for linear luminances below 0.0031308 then a shifted Gamma 2.4 .","title":"Transfer curve"},{"location":"I-liste-espaces.html#linear-rgb","text":"Color space, scene-referred . Linear RGB is a variation of sRGB , identical in all points except for the transfer curve which is * linear . It is mainly used as a workspace ( scene referred ) when we need a better simulation of light (rendering engines, blending modes in compositing \u2026) and to simplify calculations. It is found in particular in EXR* files. Note Often software mistakenly talks about linear SRGB , or worse, IEC 61966-2-1 linear , but it is not sRGB since the transfer curve is different.","title":"Linear RGB"},{"location":"I-liste-espaces.html#rec-601-itu-r-bt601-ccir-601","text":"Display space, display-referred . Rec. 601 is the complete standard for interlaced video for Standard Definition Television SDTV . It is no longer used but was used in the PAL and NTSC standards. Its PAL version is very close to sRGB .","title":"Rec. 601 / ITU-R BT.601 / CCIR 601"},{"location":"I-liste-espaces.html#primaries_1","text":"R G B X (NTSC) 0,63 0,31 0,155 Y (NTSC) 0,34 0,595 0,007 Z (NTSC) 0,03 0,095 0,775 X (PAL) 0,64 0,29 0,15 Y (PAL) 0,33 0,60 0,06 Z (PAL) 0,03 0,11 0,79 Note In PAL , the primaries of Rec. 601 are very close to those of sRGB ; only the Gx changes (0.29 instead of 0.3)","title":"Primaries"},{"location":"I-liste-espaces.html#white_1","text":"White x 0,3127 y 0,3290 CIE D65 lum. 100 cd/m\u00b2","title":"White"},{"location":"I-liste-espaces.html#transfer-curve_1","text":"Gamma 2.4 .","title":"Transfer curve"},{"location":"I-liste-espaces.html#rec709-uit-r-bt-709","text":"Display space, display-referred . Rec. 709 is the standard of the High Definition Television, HDTV and dates from 1990. It is generally found in the current video formats ( mp4 \u2026) It is very close to sRGB , only the transfer curve changes (and its white point must be a little more intense).","title":"Rec.709 / UIT-R BT 709"},{"location":"I-liste-espaces.html#primaries_2","text":"These primaries are the same as the sRGB ones. R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79","title":"Primaries"},{"location":"I-liste-espaces.html#white_2","text":"White x 0,3127 y 0,3290 CIE D65 lum. 100 cd/m\u00b2","title":"White"},{"location":"I-liste-espaces.html#transfer-curve_2","text":"Close to a Gamma 2.4 or Gamma 2.35 according to the recommendations.","title":"Transfer curve"},{"location":"I-liste-espaces.html#rec2020-uit-r-bt2020","text":"Display space, display-referred . Rec. 2020 is the standard for Ultra High Definition Television, UHDTV and dates from 2012.","title":"Rec.2020 / UIT-R BT.2020"},{"location":"I-liste-espaces.html#primaries_3","text":"R G B X 0,708 0,17 0,131 Y 0,292 0,797 0,046 Z 0,0 0,033 0,823","title":"Primaries"},{"location":"I-liste-espaces.html#white_3","text":"White x 0,3127 y 0,3290 CIE D65 lum. variable","title":"White"},{"location":"I-liste-espaces.html#transfer-curve_3","text":"The same as Rec.709 close to a Gamma 2,4 , but more precise.","title":"Transfer curve"},{"location":"I-liste-espaces.html#dci-p3","text":"Display space, display-referred . DCI-P3 was created as a transition to Rec. 2020 and for movie projection. It is used by some TVs and computer monitors, and even more recently some phones/tablets.","title":"DCI-P3"},{"location":"I-liste-espaces.html#primaries_4","text":"R G B X 0,68 0,265 0,15 Y 0,32 0,69 0,06 Z 0,00 0,045 0,79","title":"Primaries"},{"location":"I-liste-espaces.html#white_4","text":"White x 0,3140 y 0,3510 CIE N/A lum. 48 cd/m\u00b2","title":"White"},{"location":"I-liste-espaces.html#transfer-curve_4","text":"Gamma 2,6","title":"Transfer curve"},{"location":"I-liste-espaces.html#display-p3","text":"Display space, display-referred . The Display P3 is a variation of the DCI-P3 adapted to be closer to (and better compatible with) the sRGB (and thus to make screens able to display both more easily). It keeps the primaries (and thus the wider gamut) of the DCI-P3 but uses the White point and the Transfer curve of the sRGB .","title":"Display P3"},{"location":"I-liste-espaces.html#primaries_5","text":"R G B X 0,68 0,265 0,15 Y 0,32 0,69 0,06 Z 0,00 0,045 0,79","title":"Primaries"},{"location":"I-liste-espaces.html#white_5","text":"White x 0,3127 y 0,3290 CIE D65 lum. 80 cd/m\u00b2","title":"White"},{"location":"I-liste-espaces.html#transfer-curve_5","text":"That of the sRGB close on average to a Gamma 2.2 .","title":"Transfer curve"},{"location":"I-liste-espaces.html#aces","text":"ACES stands for Academy Color Encoding System , and represents a system comprising 5 color spaces designed by the Academy of the Oscars specifically for audiovisual production, released in 2014 for its first version. These 5 spaces share the same White point, close to D60 ; they use two sets of Primaries named AP0 or AP1 . The AP0 Primaries are outside the visible colors (and thus the CIE XYZ ): they are the closest theoretical Primaries encompassing all visible colors. The AP1 Primaries are closer to the usual RGB Primaries and to the screens and projectors, making them more practical in the production of images.","title":"ACES"},{"location":"I-liste-espaces.html#primaries-ap0","text":"R G B X 0,7347 0,0 0,001 Y 0,2653 1,0 -0,77","title":"Primaries (AP0)"},{"location":"I-liste-espaces.html#primaries-ap1","text":"R G B X 0,713 0,165 0,128 Y 0,293 1,830 0,044","title":"Primaries (AP1)"},{"location":"I-liste-espaces.html#white_6","text":"White x 0,32168 y 0,33767 CIE proche de D60 lum. infinie","title":"White"},{"location":"I-liste-espaces.html#aces2065-1","text":"Storage space . ACES2065-1 is the main space of the ACES system. It is designed to store any color information in a non-destructive way and is usable in the long term with future new spaces. It thus encompasses all visible colors and is larger than the CIE XYZ . Its use is mainly theoretical. It is intended to be used in openEXR images or MXF videos. It is linear.","title":"ACES2065-1"},{"location":"I-liste-espaces.html#acescg","text":"Workspace, scene-referred . ACEScg is similar to ACES2065-1 but uses the AP1 Primaries. It is intended primarily for 3D renderers and compositing. Its standard image storage format is openEXR .","title":"ACEScg"},{"location":"I-liste-espaces.html#acescc","text":"Workspace, scene-referred . ACEScc is similar to ACEScg (with AP1 Primaries) but uses a non-linear Transfer curve, more convenient for color correction and calibration.","title":"ACEScc"},{"location":"I-liste-espaces.html#adobe-rgb","text":"Display space, display-referred and working space. Adobe RGB was designed in 1998 for graphic designers working on screen but whose work is intended for printing. It is close to sRGB , but the green primary has been shifted to include more colors that can be reproduced in CMYK printing. Its Transfer curve is also slightly different.","title":"Adobe RGB"},{"location":"I-liste-espaces.html#primaries_6","text":"R G B X 0,64 0,21 0,15 Y 0,33 0,71 0,06","title":"Primaries"},{"location":"I-liste-espaces.html#white_7","text":"White x 0,3127 y 0,3290 CIE D65 lum. 160 cd/m\u00b2","title":"White"},{"location":"I-liste-espaces.html#transfer-curve_6","text":"Gamma 563/256 soit 2,199 218 75 . Sources et r\u00e9f\u00e9rences sRGB on Wikipedia Specifications sRGB on color.org Specifications BT.601 on color.org Rec. 709 on Wikipedia Specifications BT.709 on color.org Rec. 2020 on Wikipedia Specifications BT.2020 on color.org DCI-P3 on Wikipedia Specifications DCI-P3 on color.org ACES on Wikipedia acescolorspace.com/","title":"Transfer curve"},{"location":"J-liste-params.html","text":"I.J - List of color space parameters \u00b6 Here is the list of the various primaries , white points and transfer curves shared by the different color spaces, as well as their comparisons. I.J - List of color space parameters J.1 - Primaries RGB / sRGB / Rec. 709 Rec. 601 Rec. 2020 P3 AP0 / ACES2065-1 AP1 / ACEScg / ACEScc Adobe RGB J.2 - Whites D65 / RGB / sRGB / Rec. 601 / Rec. 709 / Display P3 / Adobe RGB DCI-P3 ACES / ACES2065-1 / ACEScg / ACEScc J.3 - Transfer curves Lin\u00e9aire / RGB / ACES2065-1 / ACEScg sRGB / Display P3 2.4 / Rec. 601 / Rec. 709 / Rec. 2020 2.2 / Adobe RGB ACEScc J.1 - Primaries \u00b6 RGB / sRGB / Rec. 709 \u00b6 R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79 Rec. 601 \u00b6 R G B X (NTSC) 0,63 0,31 0,155 Y (NTSC) 0,34 0,595 0,007 Z (NTSC) 0,03 0,095 0,775 X (PAL) 0,64 0,29 0,15 Y (PAL) 0,33 0,60 0,06 Z (PAL) 0,03 0,11 0,79 Rec. 2020 \u00b6 R G B X 0,708 0,17 0,131 Y 0,292 0,797 0,046 Z 0,0 0,033 0,823 P3 \u00b6 R G B X 0,68 0,265 0,15 Y 0,32 0,69 0,06 Z 0,00 0,045 0,79 AP0 / ACES2065-1 \u00b6 R G B X 0,7347 0,0 0,001 Y 0,2653 1,0 -0,77 AP1 / ACEScg / ACEScc \u00b6 R G B X 0,713 0,165 0,128 Y 0,293 1,830 0,044 Adobe RGB \u00b6 R G B X 0,64 0,21 0,15 Y 0,33 0,71 0,06 J.2 - Whites \u00b6 D65 / RGB / sRGB / Rec. 601 / Rec. 709 / Display P3 / Adobe RGB \u00b6 White x 0,3127 y 0,3290 CIE D65 DCI-P3 \u00b6 White x 0,3140 y 0,3510 ACES / ACES2065-1 / ACEScg / ACEScc \u00b6 White x 0,32168 y 0,33767 CIE proche de D60 J.3 - Transfer curves \u00b6 Lin\u00e9aire / RGB / ACES2065-1 / ACEScg \u00b6 Linear transfer sRGB / Display P3 \u00b6 Close on average to a Gamma 2.2 , with in reality a linear transfer for linear luminances below 0.0031308 and then a shifted Gamma 2.4 . 2.4 / Rec. 601 / Rec. 709 / Rec. 2020 \u00b6 Gamma 2,4 2.2 / Adobe RGB \u00b6 Gamma 563/256 i.e 2,199 218 75 . ACEScc \u00b6 .","title":"J - List of color space parameters"},{"location":"J-liste-params.html#ij-list-of-color-space-parameters","text":"Here is the list of the various primaries , white points and transfer curves shared by the different color spaces, as well as their comparisons. I.J - List of color space parameters J.1 - Primaries RGB / sRGB / Rec. 709 Rec. 601 Rec. 2020 P3 AP0 / ACES2065-1 AP1 / ACEScg / ACEScc Adobe RGB J.2 - Whites D65 / RGB / sRGB / Rec. 601 / Rec. 709 / Display P3 / Adobe RGB DCI-P3 ACES / ACES2065-1 / ACEScg / ACEScc J.3 - Transfer curves Lin\u00e9aire / RGB / ACES2065-1 / ACEScg sRGB / Display P3 2.4 / Rec. 601 / Rec. 709 / Rec. 2020 2.2 / Adobe RGB ACEScc","title":"I.J - List of color space parameters"},{"location":"J-liste-params.html#j1-primaries","text":"","title":"J.1 - Primaries"},{"location":"J-liste-params.html#rgb-srgb-rec-709","text":"R G B X 0,64 0,30 0,15 Y 0,33 0,60 0,06 Z 0,03 0,1 0,79","title":"RGB / sRGB / Rec. 709"},{"location":"J-liste-params.html#rec-601","text":"R G B X (NTSC) 0,63 0,31 0,155 Y (NTSC) 0,34 0,595 0,007 Z (NTSC) 0,03 0,095 0,775 X (PAL) 0,64 0,29 0,15 Y (PAL) 0,33 0,60 0,06 Z (PAL) 0,03 0,11 0,79","title":"Rec. 601"},{"location":"J-liste-params.html#rec-2020","text":"R G B X 0,708 0,17 0,131 Y 0,292 0,797 0,046 Z 0,0 0,033 0,823","title":"Rec. 2020"},{"location":"J-liste-params.html#p3","text":"R G B X 0,68 0,265 0,15 Y 0,32 0,69 0,06 Z 0,00 0,045 0,79","title":"P3"},{"location":"J-liste-params.html#ap0-aces2065-1","text":"R G B X 0,7347 0,0 0,001 Y 0,2653 1,0 -0,77","title":"AP0 / ACES2065-1"},{"location":"J-liste-params.html#ap1-acescg-acescc","text":"R G B X 0,713 0,165 0,128 Y 0,293 1,830 0,044","title":"AP1 / ACEScg / ACEScc"},{"location":"J-liste-params.html#adobe-rgb","text":"R G B X 0,64 0,21 0,15 Y 0,33 0,71 0,06","title":"Adobe RGB"},{"location":"J-liste-params.html#j2-whites","text":"","title":"J.2 - Whites"},{"location":"J-liste-params.html#d65-rgb-srgb-rec-601-rec-709-display-p3-adobe-rgb","text":"White x 0,3127 y 0,3290 CIE D65","title":"D65 / RGB / sRGB / Rec. 601 / Rec. 709 / Display P3 / Adobe RGB"},{"location":"J-liste-params.html#dci-p3","text":"White x 0,3140 y 0,3510","title":"DCI-P3"},{"location":"J-liste-params.html#aces-aces2065-1-acescg-acescc","text":"White x 0,32168 y 0,33767 CIE proche de D60","title":"ACES / ACES2065-1 / ACEScg / ACEScc"},{"location":"J-liste-params.html#j3-transfer-curves","text":"","title":"J.3 - Transfer curves"},{"location":"J-liste-params.html#lineaire-rgb-aces2065-1-acescg","text":"Linear transfer","title":"Lin\u00e9aire / RGB / ACES2065-1 / ACEScg"},{"location":"J-liste-params.html#srgb-display-p3","text":"Close on average to a Gamma 2.2 , with in reality a linear transfer for linear luminances below 0.0031308 and then a shifted Gamma 2.4 .","title":"sRGB / Display P3"},{"location":"J-liste-params.html#24-rec-601-rec-709-rec-2020","text":"Gamma 2,4","title":"2.4 / Rec. 601 / Rec. 709 / Rec. 2020"},{"location":"J-liste-params.html#22-adobe-rgb","text":"Gamma 563/256 i.e 2,199 218 75 .","title":"2.2 / Adobe RGB"},{"location":"J-liste-params.html#acescc","text":".","title":"ACEScc"},{"location":"K-pix-format.html","text":"I.K - Pixel format \u00b6 Color spaces define how colors should be reproduced, and what specific colors should be generated, but they do not define how those colors are recorded by the computer. It is the pixel format used that will define what types of values are recorded, and how. This format is completely independent of the color spaces, but will influence the quality of the image and the amount of shading that can be stored. I.K - Pixel format K.1 - RGB or YUV K.1.a - RGB K.1.b - YUV K.1.c - Comparison K.1.d - Others K.2 - YUV 4:4:4, 4:2:2, 4:2:0\u2026 Chrominance subsampling K.2.a - 4:4:4 K.2.b - 4:2:2 K.2.c - 4:2:0 K.3 - Color depth (bpc) K.3.a - In RGB K.3.a.a 8 bpc / 24 bits / 24 bpp / 32 bits with alpha K.3.a.b 16 bpc / 48 bits / 48 bpp / 64 bits with alpha K.3.a.c 32 bpc / 96 bits / 96 bpp / 128 bits with alpha K.3.b - In YUV K.3.b.a 8 bits K.3.b.b 10 bits K.3.b.c 12 bits K.3.c - Others K.4 - Full range / Limited / TV / PC ? K.4.a - Full range / PC K.4.b - Limited range / TV K.4.c - Practical conclusion K.4.c.a - Encoding K.4.c.b - Playback and display Each pixel is composed of different values of colors, they are the channels * , which are in general (but not necessarily) either red, green and blue (it is the RGB ), or a luminance * and two chrominances * (it is the YUV ) In the case of YUV , there is sometimes chrominance subsampling , which reduces the amount of data to be stored with a minimal (and almost indiscernible) loss of quality. Finally, for each channel , you can choose the range and precision of the recorded values. f K.1 - RGB or YUV \u00b6 Any visible color can be represented by only two complementary * colors and primary * colors. As soon as three primaries are defined and fixed, by varying their proportions, one can obtain a wider range (a gamut * ) of visible colors, a useful (and sufficient) surface of visible colors. Thus, the majority of the devices of reproduction of the colors (screens, projectors\u2026) use three primary colors. These colors are mostly reds, greens, and blues; they may vary depending on the device and the color space * they use but are always within this range. In any case, all the color coding systems use a group of different values corresponding to precise primaries or properties; we speak of channels * , generally three in number. K.1.a - RGB \u00b6 A numerical division of the information of colors is generally used based on the same primary * Red, Green and Blue as the devices of reproduction. Red, Green and Blue are the three channels R , G , B of this system. There are several reasons for the use of these primaries and this system of color representation: The choice of red, green and blue is close to what the receptor cells perceive best naturally Red and blue are the extremes of the visible spectrum, and green is halfway between the two: it is the choice that easily allows to have the widest gamut in the set of visible colors. -These colors are naturally the most used by the display devices although in theory it would be possible to choose others. But historically, and for reasons of performance and storage, another system is very widespread: YUV . K.1.b - YUV \u00b6 In the first uses of an electrical signal (analog, not digital at first) to represent videos, the signal was a \u201csimple\u201d one-dimensional signal: videos were represented only in a range from black to white through grays, in black and white . In other words, one stored and reproduced only the information of luminous intensity, of luminance * . Then came the color televisions, but it was necessary to keep the compatibility of the signal with the oldest black and white televisions; the information of colors, of chrominance was therefore added to the luminance signal 1 , without touching the original signal; the old black and white televisions simply ignored this additional information. The system used is not a RGB system but of three channels YUV 2 , where Y represents the luminance , and UV represent two information of chrominance (containing respectively the ratio blue/green and red/green). What is interesting about this system is not only historical: as we have seen previously, the human eye distinguishes better the contrasts of luminance * than of chrominance . Separating this information makes it possible to treat them differently, and in particular to reserve a quantity of information, a resolution, higher in the luminance than the chrominance, and thus to reduce the quantity of information to be stored and transmitted without notable loss of visible quality. K.1.c - Comparison \u00b6 We have two main systems of color coding, independent of color spaces used 3 . Here are the main differences: History: the use of RGB spread with the digital age, YUV dates from the analog Consequently YUV is more widespread in video formats, RGB in image formats. YUV allows a more efficient data compression, with a certain loss in chrominance, while RGB* must store as much information in its three channels. The conversion from YUV to RGB , and vice versa, is done easily and almost without loss of information. K.1.d - Others \u00b6 There are other less common combinations of channels and pixel formats, for specific uses or containers * . For example, some image formats use a palette of colors instead of several mixed primaries, and thus have only one channel per pixel, whose value corresponds to a predefined color 4 . Other formats store only grayscale, and thus just one channel of brightness. There are also more exotic formats with two layers\u2026 K.2 - YUV 4:4:4, 4:2:2, 4:2:0\u2026 Chrominance subsampling \u00b6 YUV has the advantage, compared to RGB , to be able to function in practice in a way closer to the human perception which is more powerful in luminosity * . Indeed, by separating the luminance * from the chrominance * , it is possible to decrease the quantity of data recorded in chrominance to support the luminance , without losing the perceived visual quality and thus to compress the video data effectively. To operate this reduction of quality in chrominance , we simply decrease the resolution, the number of pixels; it is what we call chroma sub-sampling. Chroma subsampling is therefore a lossy compression method that is completely independent of the standard * ( codec * ) encoding of the video. The acronyms 4:4:4 , 4:2:2 , 4:2:0 5 \u2026 describe how the subsampling is done and indicate the amount of data lost. This description is made from a 4 by 2 pixel grid. The first value of the trio represents the resolution (sampling) of the luminance * . The second value represents the subsampling of the chrominance * on the first line (all odd lines), while the third represents this subsampling on the second line (all even lines). A fourth value is sometimes added to the acronym and represents in this case a subsampling in the alpha* channel of the video 6 . We can easily calculate the amount of data saved by adding the three values and dividing by 12 (or 16 if there is a separate value for alpha). For example: in 4:4:4 , there is no loss (factor of 1.0 ) in 4:2:2 , one third is gained (factor of 0.66 ) in 4:2:0 , we gain half (factor of 0.5 ) Tip In the case of black and white video, chrominance is completely useless, so you can choose the mode that has the least chrominance. K.2.a - 4:4:4 \u00b6 The 4:4:4 subsampling in YUV is the only equivalent to RGB in terms of quality (and quantity of data). There is in fact no subsampling in this mode and all pixels contain chrominance * and luminance * information. It is not used in broadcasting but only in production (or for archiving). Indeed, the bitrate would be too high, but this data is essential for post-production, especially when using green or blue background ( chroma-key ): since masking is done on the chrominance information, it is absolutely essential to have the full resolution. Warning Unfortunately, only high-end professional cameras and recorders can record in 4:4:4 , many cameras record in 4:2:2 , only in 4:2:0 for the entry level. K.2.b - 4:2:2 \u00b6 In 4:2:2 , the resolution of the chrominance * is half that of the luminance (so the amount of data is reduced by one third). The loss is imperceptible, which makes it a very efficient way of compressing video. This mode is used in production (as long as there is no overlay, green/blue background), in high-end formats and in high quality broadcasting (especially in television). The horizontal resolution of the chrominance is reduced by half while the vertical resolution is kept. K.2.c - 4:2:0 \u00b6 In 4:2:0 , the chrominance * resolution is reduced by half on every other line, and completely removed on the other line. The amount of data is thus reduced by half overall, but the difference is still very difficult to perceive, which makes it a very good broadcast format 7 . This mode is the main one in consumer computer files and on the internet. Many software players, and most hardware players (blu-rays players, smart TVs, etc) only support 4:2:0 . It is to be proscribed in production in case of colorimetric correction or overlay; the chrominance information is very insufficient (a \u201cstaircase\u201d effect can easily appear, due to the lack of resolution in chrominance). Both the horizontal and vertical resolution of the chrominance are reduced by half. K.3 - Color depth (bpc) \u00b6 Regardless of the chosen color space, and whether for exported files or the working space, the color depth parameter describes the accuracy of the values recorded for each channel * of pixels. Contrary to a widespread idea, the depth of color does not really influence the quantity of visible colors, but rather the precision of the calculations, and thus the number of \u201csub-shades\u201d usable within the chosen colorimetric space. In other words, the depth does not change the gamut * . We will speak here rather about quantity of shades rather than quantity of colors to avoid this confusion. By defining the precision of the values, and the amount of data recorded, the color depth directly influences the size of the files. This is usually measured in bits * (meaning per pixel ) or in bpc ( bits per channel). The more bits ( 0 or 1 ) you use to store the pixel (or each of the channels), the more space the file will take up but the higher the accuracy (and therefore the quality). Depending on the system, the standards vary, especially because of the YUV chrominance subsampling. K.3.a - In RGB \u00b6 In RGB each channel * contains as much information (there is no subsampling), and if in theory one could imagine an arbitrary number of bits * to store the channels (and it is the case in certain file formats), one generally uses multiples of 8 (and thus integer bytes 8 ) K.3.a.a 8 bpc / 24 bits / 24 bpp / 32 bits with alpha \u00b6 Most images use 8 bits per channel . With 8 bits, we can code 2 8 , that is 256, different values (from 0 to 255). With three layers, we therefore have a total of 8 3 , that is, a little over 16 million, different values for a pixel. This quantity of nuances is necessary and sufficient so that the human eye does not distinguish \u201clevels\u201d in the images with a gamut * like that of the sRGB , and is thus the most widespread in the digital images RGB intended for the computer screens. But this quantity is not sufficient when working on the image, as a workspace. Indeed, when modifying the images, the computer performs calculations on the values of the different layers, and these calculations on only 256 integer values lead to a strong loss of precision, visible very quickly 9 . This depth is also not sufficient for TV or film work that uses a higher depth (associated with a wider gamut in spaces other than sRGB ). To be able to work without degrading the image, we therefore increase the color depth of the workspace. K.3.a.b 16 bpc / 48 bits / 48 bpp / 64 bits with alpha \u00b6 By adding a byte for each layer, we greatly increase the number of available shades. In effect, we increase the number of values available for each layer to 2 16 , or 65536. This makes a total of 65536 3 , or several trillion , shades per pixel. As a general rule, these 16 bits per layer provide the necessary precision for fine work on the image, but may still be insufficient in specific cases: When using a linear workspace for an export intended for high-end TV and HDR or cinema ( cf. chapter L - Transfer curves ): the transition from linear * to non-linear output space (the application of a gamma * ) \u201ccompresses\u201d the dark values and \u201cstretches\u201d the lights. As a result of this calculation, 16 bpc linear corresponds in quality to just 12 bpc of the highest standards for television 10 and cinema 11 . In case of complex and heavy work, even on a non-linear space, for television or cinema, it\u2019s possible to reach the limit of precision necessary for very fine color gradations. We can therefore further increase the depth of color in these cases. K.3.a.c 32 bpc / 96 bits / 96 bpp / 128 bits with alpha \u00b6 Warning Do not confuse 32 bpc (per channel) with 32 bits or 32 bpp (per pixel) which is actually only 8 bpc (with an alpha layer)! We add a third byte per channel, which further increases exponentially the number of available shades, with 2 32 , 4 billion, possible values per channel * , that is to say a number that we will consider infinite shades per pixel. This mode is the one which allows a work virtually without any loss on the image, whatever the colorimetric space used, whether it is linear or not, but becomes very heavy in term of memory. While it can be useful as a working mode, it is in fact rarely used for storing files (even intermediate ones) where 16 bits per layer are often enough 12 . K.3.b - In YUV \u00b6 In YUV , which is never used as a working system but only for storage and diffusion, the depth used is different from RGB systems. It\u2019s usually noted in the number of bits used for the luminance layer per pixel; indeed the number of bits * per channel * doesn\u2019t really make sense with the different sub-samples of chrominance * possible, as well as a number of bits per pixel By taking into account the subsampling of chrominance , we can calculate the average number of bits used per byte (and therefore the approximate size of an image without compression by multiplying by the number of pixels), even if in reality, the number of bits used by chrominance is the one indicated for pixels containing chrominance, and zero for the rest. The various modes thus differ only in term of quality, and as a general rule, the more we increase the resolution of the image and the gamut * of the color space, the more we increase the depth to ensure that the gradations remain fine and without effect of staircase (\u201c banding \u201d ) K.3.b.a 8 bits \u00b6 This is the most common depth in computing; most computer monitors can\u2019t display more than this. In 4:4:4 , this means that each pixel is represented by 24 bits . In 4:2:2 , this means that a pixel is represented on average by 16 bits , with actually half of the pixels (every other column) containing 24 bits ( 8 bits per channel), and the other half containing 8 bits ( 8 bits of luminance and none of chrominance). In 4:2:0 , this means that a pixel is represented on average by 12 bits , with actually every fourth pixel (every second row and every second column) containing 24 bits ( 8 bits per channel), and the last quarter containing 8 bits ( 8 bits of luminance and none of chrominance). This is the standard depth for HD video in Rec.709 . K.3.b.b 10 bits \u00b6 This is the basic depth of high-end and UHD videos in Rec.2020 . In 4:4:4 , this means that each pixel is represented by 30 bits . In 4:2:2 , this means that a pixel is represented on average by 20 bits , with actually half of the pixels (every other column) containing 30 bits ( 10 bits per layer), and the other half containing 10 bits ( 10 bits of luminance and none of chrominance). In 4:2:0 , this means that a pixel is represented on average by 15 bits , with every fourth pixel (every second row and every second column) actually containing 30 bits ( 10 bits per layer), and the last quarter, 10 bits ( 10 bits of luminance and none of chrominance). K.3.b.c 12 bits \u00b6 This is the \u201c HDR \u201d depth of high-end video and UHD in Rec.2020 . In 4:4:4 , this means that each pixel is represented by 36 bits . In 4:2:2 , this means that a pixel is represented on average by 24 bits , with actually half of the pixels (every other column) containing 36 bits ( 12 bits per layer), and the other half containing 12 bits ( 12 bits of luminance and none of chrominance). In 4:2:0 , this means that a pixel is represented on average by 18 bits , with every fourth pixel (every second row and every second column) actually containing 18 bits ( 12 bits per layer), and the last quarter, 12 bits ( 12 bits of luminance and none of chrominance). K.3.c - Others \u00b6 There are other depths, from 1 bit per pixel (monochrome images), depending on specific uses. For example, an image using a palette of 256 colors as found in the GIF format or some PNG for example use 8 bits per pixel (and therefore per channel too, since there is only one channel in this case) K.4 - Full range / Limited / TV / PC ? \u00b6 When encoding video (and decoding it), an important parameter is the color range . This parameter has its historical origin at the time of the passage from analog TV to digital RGB screens. It gives a range of possible levels on each of the channels * of color (red, green, blue). K.4.a - Full range / PC \u00b6 Digital computer monitors use the full range of red, green and blue levels for color reproduction. With the 8 bits * per channel * most common, this means that each channel stores values between 0 and 255. 0 represents black, and 255 represents white. K.4.b - Limited range / TV \u00b6 Televisions are expected to use the range known as limited of the levels; this range is originally adapted specifically to represent more correctly the contrasts of the films and corresponds with 8 bits * by channel * to values between 16 and 235. This means that in television, the value 16 represents black, and the value 235 represents white. All values below 16 are ignored (more black than black) as well as all values above 235 (more white than white) 13 . K.4.c - Practical conclusion \u00b6 It is therefore necessary to know both your material when reproducing a video, and what to do when encoding it. K.4.c.a - Encoding \u00b6 In the vast majority of cases, video standards recommend encoding in limited range / TV : videos are intended to be seen in television conditions (including on the Internet). This is the case for example for mp4 in h.264 or h.265 , for mkv , and for all broadcast formats. On the other hand, image formats ( PNG , Jpeg , openEXR , etc.), as well as intermediate video formats (those used during production and not broadcasting, such as Prores ), being intended for a computer environment, use rather the full range / PC . It is important to respect these standards to be sure that the files are correctly interpreted by the viewers\u2019 equipment, and always inquire about the formats recommended by the broadcasters. K.4.c.b - Playback and display \u00b6 When playing videos, it is also necessary that the whole system is correctly configured; a common (not to say recurrent) problem on computers is that videos are left in limited range while the screen is full range . During playback, the video source must be converted to match the screen or projector. Without conversion, a limited range video on a full range screen will be \u201cdull\u201d: there will be no black or white, the range of the image going only from light to dark gray. Conversely, a full range video on a limited range TV will have a loss of information in both highlights and shadows, with large parts completely black or white (the image will be too contrasted). So the hardware must be set up correctly. On a disc player or console, there must be a setting to specify whether the connected display is full range or limited range (as a general rule, a computer monitor or video projector is full range , a TV limited range ). On a computer, things can be a bit more complex: you have to start by checking the settings of the graphics card driver, usually in a section called \u201cvideo\u201d, and specify full range / PC (unless it is a TV that is connected to the computer) 14 . If after having set this parameter variations are still visible, you have to check that the software used to play the video does not make a bad conversion (for example Quicktime on Windows was known for this 15 ); most of these softwares should however let the graphics card do this conversion and not cause any problem (this is the case of VLC , of the video player of Windows , of Totem under Linux \u2026). Example of parameters via the settings of a Nvidia graphics card (under Linux ). Note especially here the color range parameter, to be set to Full if the screen is a computer screen, and Limited if it is a TV. Warning On some hardware, an \u201cAuto\u201d option is available in addition to full / limited range . In this case the hardware try to detect the type of screen connected. Since this is a parameter that should only be changed when the screen is changed, it is strongly recommended to set it manually. Sources & References Sous-\u00e9chantillonage de la chrominance sur Wikipedia Color Depth sur Wikipedia (en anglais) RGB Full vs Limited sur Reference Home Theater (en anglais) In reality, we speak of either luminance (ZZ-vocabulaire.md) or luminosity (ZZ-vocabulaire.md) or luma * : - The luminance has a linear transfer curve. - The luma/luminosity/brightness has a gamma. When we speak of luminance we note YUV while when we speak of brightness we should note Y\u2019UV , but most of the time we omit the premium. See chapter L - Transfer curves, linear space and gamma *. \u21a9 The general term YUV actually covers two families, each declined in luminance or luminosity ( Y or Y\u2019 ): - In analog we speak of YUV and Y\u2019UV , or sometimes YPbPr and Y\u2019PbPr . - In digital the exact term is YCbCr or Y\u2019CbCr , and sometimes YCC . But it is generally the term YUV that is used in all these different cases\u2026 \u21a9 In fact, YUV and RGB can be used interchangeably, but some standards and color spaces specify one or the other, or both. For example sRGB is specifically intended to be used on RGB encoding, while Rec.709 specifies that it can be used in both RGB and YUV . \u21a9 This is the case for GIF which contains a maximum of 256 different colors in its palette. This principle can also be used for PNG and other formats. The aim here is to reduce the overall size of the file by storing less information (only the palette with the colors described in RGB , and only one channel per pixel; this way we can reduce the overall size by a factor of three). \u21a9 This list are the most common subsamples, but there are rarer ones ( 4:2:1 , 4:1:1 ), or more complex or downright exotic ones ( 3:1.5:1.5 , 3:1:1 )\u2026 \u21a9 By default, the subsampling of the alpha channel is the same as that of the luminance * . \u21a9 This mode exists in the openEXR format: it is the option noted \u201cLuminance/Chroma\u201d. \u21a9 In current computing where the byte * is also the Byte * , the smallest unit of memory, one cannot use \u201chalf bytes\u201d (or any other fraction). Using an integer number of bytes per pixel means that the number of pixels in the image can be completely arbitrary; if the number of bytes per pixel is not integer, several pixels will have to share bytes, and the number of pixels is constrained: the total number of bytes in the image must be integer. This is why the number of rows and columns in a mp4 video must be even for example. \u21a9 It is easy to see why: let\u2019s imagine that we have to divide a value of 127 by two. The result will be rounded to 63 or 64 . If other calculations follow, the precision drops very quickly and so does the quality. \u21a9 In 2021, the standard for Ultra-High Definition ( 4K ) Rec.2020 , still little used in broadcasting but already standardized, which has a very wide gamut, recommends 12 bits in luminance for its \u201cHDR\u201d mode. \u21a9 The Digital Cinema Package ( DCP ) (where the image is in fact encoded under the JPEG 2000 lossless standard) in use in 2021 encodes the colors in a XYZ space with 12 bpc . \u21a9 We won\u2019t go into the technical details of how these values are represented, either by a float number (between 0.0 and 1.0 and potentially negative too) or by an integer, but it is important to know that this distinction exists as much for the 16 bpc as for the 32 bpc . When in doubt, prefer floating point numbers. \u21a9 Some blu-rays and game consoles take advantage of this limit to add speculars (brightness) beyond the white and make them brighter. If the TV is compatible, it will display \u201csuper whites\u201d, otherwise it will simply ignore this information, without affecting the image. \u21a9 For a long time, NVidia graphics card drivers under Windows were configured to display videos in limited range by default\u2026 \u21a9 Although loved by animators for its ability to easily play videos frame by frame, Quicktime under Windows is to be avoided for its poor color management; its development has been abandoned by Apple anyway. DJV , available for Windows , Mac OS , Linux , as well as BSD , replaces it efficiently and integrates a professional color management. \u21a9","title":"K - Pixel formats"},{"location":"K-pix-format.html#ik-pixel-format","text":"Color spaces define how colors should be reproduced, and what specific colors should be generated, but they do not define how those colors are recorded by the computer. It is the pixel format used that will define what types of values are recorded, and how. This format is completely independent of the color spaces, but will influence the quality of the image and the amount of shading that can be stored. I.K - Pixel format K.1 - RGB or YUV K.1.a - RGB K.1.b - YUV K.1.c - Comparison K.1.d - Others K.2 - YUV 4:4:4, 4:2:2, 4:2:0\u2026 Chrominance subsampling K.2.a - 4:4:4 K.2.b - 4:2:2 K.2.c - 4:2:0 K.3 - Color depth (bpc) K.3.a - In RGB K.3.a.a 8 bpc / 24 bits / 24 bpp / 32 bits with alpha K.3.a.b 16 bpc / 48 bits / 48 bpp / 64 bits with alpha K.3.a.c 32 bpc / 96 bits / 96 bpp / 128 bits with alpha K.3.b - In YUV K.3.b.a 8 bits K.3.b.b 10 bits K.3.b.c 12 bits K.3.c - Others K.4 - Full range / Limited / TV / PC ? K.4.a - Full range / PC K.4.b - Limited range / TV K.4.c - Practical conclusion K.4.c.a - Encoding K.4.c.b - Playback and display Each pixel is composed of different values of colors, they are the channels * , which are in general (but not necessarily) either red, green and blue (it is the RGB ), or a luminance * and two chrominances * (it is the YUV ) In the case of YUV , there is sometimes chrominance subsampling , which reduces the amount of data to be stored with a minimal (and almost indiscernible) loss of quality. Finally, for each channel , you can choose the range and precision of the recorded values. f","title":"I.K - Pixel format"},{"location":"K-pix-format.html#k1-rgb-or-yuv","text":"Any visible color can be represented by only two complementary * colors and primary * colors. As soon as three primaries are defined and fixed, by varying their proportions, one can obtain a wider range (a gamut * ) of visible colors, a useful (and sufficient) surface of visible colors. Thus, the majority of the devices of reproduction of the colors (screens, projectors\u2026) use three primary colors. These colors are mostly reds, greens, and blues; they may vary depending on the device and the color space * they use but are always within this range. In any case, all the color coding systems use a group of different values corresponding to precise primaries or properties; we speak of channels * , generally three in number.","title":"K.1 - RGB or YUV"},{"location":"K-pix-format.html#k1a-rgb","text":"A numerical division of the information of colors is generally used based on the same primary * Red, Green and Blue as the devices of reproduction. Red, Green and Blue are the three channels R , G , B of this system. There are several reasons for the use of these primaries and this system of color representation: The choice of red, green and blue is close to what the receptor cells perceive best naturally Red and blue are the extremes of the visible spectrum, and green is halfway between the two: it is the choice that easily allows to have the widest gamut in the set of visible colors. -These colors are naturally the most used by the display devices although in theory it would be possible to choose others. But historically, and for reasons of performance and storage, another system is very widespread: YUV .","title":"K.1.a - RGB"},{"location":"K-pix-format.html#k1b-yuv","text":"In the first uses of an electrical signal (analog, not digital at first) to represent videos, the signal was a \u201csimple\u201d one-dimensional signal: videos were represented only in a range from black to white through grays, in black and white . In other words, one stored and reproduced only the information of luminous intensity, of luminance * . Then came the color televisions, but it was necessary to keep the compatibility of the signal with the oldest black and white televisions; the information of colors, of chrominance was therefore added to the luminance signal 1 , without touching the original signal; the old black and white televisions simply ignored this additional information. The system used is not a RGB system but of three channels YUV 2 , where Y represents the luminance , and UV represent two information of chrominance (containing respectively the ratio blue/green and red/green). What is interesting about this system is not only historical: as we have seen previously, the human eye distinguishes better the contrasts of luminance * than of chrominance . Separating this information makes it possible to treat them differently, and in particular to reserve a quantity of information, a resolution, higher in the luminance than the chrominance, and thus to reduce the quantity of information to be stored and transmitted without notable loss of visible quality.","title":"K.1.b - YUV"},{"location":"K-pix-format.html#k1c-comparison","text":"We have two main systems of color coding, independent of color spaces used 3 . Here are the main differences: History: the use of RGB spread with the digital age, YUV dates from the analog Consequently YUV is more widespread in video formats, RGB in image formats. YUV allows a more efficient data compression, with a certain loss in chrominance, while RGB* must store as much information in its three channels. The conversion from YUV to RGB , and vice versa, is done easily and almost without loss of information.","title":"K.1.c - Comparison"},{"location":"K-pix-format.html#k1d-others","text":"There are other less common combinations of channels and pixel formats, for specific uses or containers * . For example, some image formats use a palette of colors instead of several mixed primaries, and thus have only one channel per pixel, whose value corresponds to a predefined color 4 . Other formats store only grayscale, and thus just one channel of brightness. There are also more exotic formats with two layers\u2026","title":"K.1.d - Others"},{"location":"K-pix-format.html#k2-yuv-444-422-420-chrominance-subsampling","text":"YUV has the advantage, compared to RGB , to be able to function in practice in a way closer to the human perception which is more powerful in luminosity * . Indeed, by separating the luminance * from the chrominance * , it is possible to decrease the quantity of data recorded in chrominance to support the luminance , without losing the perceived visual quality and thus to compress the video data effectively. To operate this reduction of quality in chrominance , we simply decrease the resolution, the number of pixels; it is what we call chroma sub-sampling. Chroma subsampling is therefore a lossy compression method that is completely independent of the standard * ( codec * ) encoding of the video. The acronyms 4:4:4 , 4:2:2 , 4:2:0 5 \u2026 describe how the subsampling is done and indicate the amount of data lost. This description is made from a 4 by 2 pixel grid. The first value of the trio represents the resolution (sampling) of the luminance * . The second value represents the subsampling of the chrominance * on the first line (all odd lines), while the third represents this subsampling on the second line (all even lines). A fourth value is sometimes added to the acronym and represents in this case a subsampling in the alpha* channel of the video 6 . We can easily calculate the amount of data saved by adding the three values and dividing by 12 (or 16 if there is a separate value for alpha). For example: in 4:4:4 , there is no loss (factor of 1.0 ) in 4:2:2 , one third is gained (factor of 0.66 ) in 4:2:0 , we gain half (factor of 0.5 ) Tip In the case of black and white video, chrominance is completely useless, so you can choose the mode that has the least chrominance.","title":"K.2 - YUV 4:4:4, 4:2:2, 4:2:0... Chrominance subsampling"},{"location":"K-pix-format.html#k2a-444","text":"The 4:4:4 subsampling in YUV is the only equivalent to RGB in terms of quality (and quantity of data). There is in fact no subsampling in this mode and all pixels contain chrominance * and luminance * information. It is not used in broadcasting but only in production (or for archiving). Indeed, the bitrate would be too high, but this data is essential for post-production, especially when using green or blue background ( chroma-key ): since masking is done on the chrominance information, it is absolutely essential to have the full resolution. Warning Unfortunately, only high-end professional cameras and recorders can record in 4:4:4 , many cameras record in 4:2:2 , only in 4:2:0 for the entry level.","title":"K.2.a - 4:4:4"},{"location":"K-pix-format.html#k2b-422","text":"In 4:2:2 , the resolution of the chrominance * is half that of the luminance (so the amount of data is reduced by one third). The loss is imperceptible, which makes it a very efficient way of compressing video. This mode is used in production (as long as there is no overlay, green/blue background), in high-end formats and in high quality broadcasting (especially in television). The horizontal resolution of the chrominance is reduced by half while the vertical resolution is kept.","title":"K.2.b - 4:2:2"},{"location":"K-pix-format.html#k2c-420","text":"In 4:2:0 , the chrominance * resolution is reduced by half on every other line, and completely removed on the other line. The amount of data is thus reduced by half overall, but the difference is still very difficult to perceive, which makes it a very good broadcast format 7 . This mode is the main one in consumer computer files and on the internet. Many software players, and most hardware players (blu-rays players, smart TVs, etc) only support 4:2:0 . It is to be proscribed in production in case of colorimetric correction or overlay; the chrominance information is very insufficient (a \u201cstaircase\u201d effect can easily appear, due to the lack of resolution in chrominance). Both the horizontal and vertical resolution of the chrominance are reduced by half.","title":"K.2.c - 4:2:0"},{"location":"K-pix-format.html#k3-color-depth-bpc","text":"Regardless of the chosen color space, and whether for exported files or the working space, the color depth parameter describes the accuracy of the values recorded for each channel * of pixels. Contrary to a widespread idea, the depth of color does not really influence the quantity of visible colors, but rather the precision of the calculations, and thus the number of \u201csub-shades\u201d usable within the chosen colorimetric space. In other words, the depth does not change the gamut * . We will speak here rather about quantity of shades rather than quantity of colors to avoid this confusion. By defining the precision of the values, and the amount of data recorded, the color depth directly influences the size of the files. This is usually measured in bits * (meaning per pixel ) or in bpc ( bits per channel). The more bits ( 0 or 1 ) you use to store the pixel (or each of the channels), the more space the file will take up but the higher the accuracy (and therefore the quality). Depending on the system, the standards vary, especially because of the YUV chrominance subsampling.","title":"K.3 - Color depth (bpc)"},{"location":"K-pix-format.html#k3a-in-rgb","text":"In RGB each channel * contains as much information (there is no subsampling), and if in theory one could imagine an arbitrary number of bits * to store the channels (and it is the case in certain file formats), one generally uses multiples of 8 (and thus integer bytes 8 )","title":"K.3.a - In RGB"},{"location":"K-pix-format.html#k3aa-8-bpc-24-bits-24-bpp-32-bits-with-alpha","text":"Most images use 8 bits per channel . With 8 bits, we can code 2 8 , that is 256, different values (from 0 to 255). With three layers, we therefore have a total of 8 3 , that is, a little over 16 million, different values for a pixel. This quantity of nuances is necessary and sufficient so that the human eye does not distinguish \u201clevels\u201d in the images with a gamut * like that of the sRGB , and is thus the most widespread in the digital images RGB intended for the computer screens. But this quantity is not sufficient when working on the image, as a workspace. Indeed, when modifying the images, the computer performs calculations on the values of the different layers, and these calculations on only 256 integer values lead to a strong loss of precision, visible very quickly 9 . This depth is also not sufficient for TV or film work that uses a higher depth (associated with a wider gamut in spaces other than sRGB ). To be able to work without degrading the image, we therefore increase the color depth of the workspace.","title":"K.3.a.a 8 bpc / 24 bits / 24 bpp / 32 bits with alpha"},{"location":"K-pix-format.html#k3ab-16-bpc-48-bits-48-bpp-64-bits-with-alpha","text":"By adding a byte for each layer, we greatly increase the number of available shades. In effect, we increase the number of values available for each layer to 2 16 , or 65536. This makes a total of 65536 3 , or several trillion , shades per pixel. As a general rule, these 16 bits per layer provide the necessary precision for fine work on the image, but may still be insufficient in specific cases: When using a linear workspace for an export intended for high-end TV and HDR or cinema ( cf. chapter L - Transfer curves ): the transition from linear * to non-linear output space (the application of a gamma * ) \u201ccompresses\u201d the dark values and \u201cstretches\u201d the lights. As a result of this calculation, 16 bpc linear corresponds in quality to just 12 bpc of the highest standards for television 10 and cinema 11 . In case of complex and heavy work, even on a non-linear space, for television or cinema, it\u2019s possible to reach the limit of precision necessary for very fine color gradations. We can therefore further increase the depth of color in these cases.","title":"K.3.a.b 16 bpc / 48 bits / 48 bpp / 64 bits with alpha"},{"location":"K-pix-format.html#k3ac-32-bpc-96-bits-96-bpp-128-bits-with-alpha","text":"Warning Do not confuse 32 bpc (per channel) with 32 bits or 32 bpp (per pixel) which is actually only 8 bpc (with an alpha layer)! We add a third byte per channel, which further increases exponentially the number of available shades, with 2 32 , 4 billion, possible values per channel * , that is to say a number that we will consider infinite shades per pixel. This mode is the one which allows a work virtually without any loss on the image, whatever the colorimetric space used, whether it is linear or not, but becomes very heavy in term of memory. While it can be useful as a working mode, it is in fact rarely used for storing files (even intermediate ones) where 16 bits per layer are often enough 12 .","title":"K.3.a.c 32 bpc / 96 bits / 96 bpp / 128 bits with alpha"},{"location":"K-pix-format.html#k3b-in-yuv","text":"In YUV , which is never used as a working system but only for storage and diffusion, the depth used is different from RGB systems. It\u2019s usually noted in the number of bits used for the luminance layer per pixel; indeed the number of bits * per channel * doesn\u2019t really make sense with the different sub-samples of chrominance * possible, as well as a number of bits per pixel By taking into account the subsampling of chrominance , we can calculate the average number of bits used per byte (and therefore the approximate size of an image without compression by multiplying by the number of pixels), even if in reality, the number of bits used by chrominance is the one indicated for pixels containing chrominance, and zero for the rest. The various modes thus differ only in term of quality, and as a general rule, the more we increase the resolution of the image and the gamut * of the color space, the more we increase the depth to ensure that the gradations remain fine and without effect of staircase (\u201c banding \u201d )","title":"K.3.b - In YUV"},{"location":"K-pix-format.html#k3ba-8-bits","text":"This is the most common depth in computing; most computer monitors can\u2019t display more than this. In 4:4:4 , this means that each pixel is represented by 24 bits . In 4:2:2 , this means that a pixel is represented on average by 16 bits , with actually half of the pixels (every other column) containing 24 bits ( 8 bits per channel), and the other half containing 8 bits ( 8 bits of luminance and none of chrominance). In 4:2:0 , this means that a pixel is represented on average by 12 bits , with actually every fourth pixel (every second row and every second column) containing 24 bits ( 8 bits per channel), and the last quarter containing 8 bits ( 8 bits of luminance and none of chrominance). This is the standard depth for HD video in Rec.709 .","title":"K.3.b.a 8 bits"},{"location":"K-pix-format.html#k3bb-10-bits","text":"This is the basic depth of high-end and UHD videos in Rec.2020 . In 4:4:4 , this means that each pixel is represented by 30 bits . In 4:2:2 , this means that a pixel is represented on average by 20 bits , with actually half of the pixels (every other column) containing 30 bits ( 10 bits per layer), and the other half containing 10 bits ( 10 bits of luminance and none of chrominance). In 4:2:0 , this means that a pixel is represented on average by 15 bits , with every fourth pixel (every second row and every second column) actually containing 30 bits ( 10 bits per layer), and the last quarter, 10 bits ( 10 bits of luminance and none of chrominance).","title":"K.3.b.b 10 bits"},{"location":"K-pix-format.html#k3bc-12-bits","text":"This is the \u201c HDR \u201d depth of high-end video and UHD in Rec.2020 . In 4:4:4 , this means that each pixel is represented by 36 bits . In 4:2:2 , this means that a pixel is represented on average by 24 bits , with actually half of the pixels (every other column) containing 36 bits ( 12 bits per layer), and the other half containing 12 bits ( 12 bits of luminance and none of chrominance). In 4:2:0 , this means that a pixel is represented on average by 18 bits , with every fourth pixel (every second row and every second column) actually containing 18 bits ( 12 bits per layer), and the last quarter, 12 bits ( 12 bits of luminance and none of chrominance).","title":"K.3.b.c 12 bits"},{"location":"K-pix-format.html#k3c-others","text":"There are other depths, from 1 bit per pixel (monochrome images), depending on specific uses. For example, an image using a palette of 256 colors as found in the GIF format or some PNG for example use 8 bits per pixel (and therefore per channel too, since there is only one channel in this case)","title":"K.3.c - Others"},{"location":"K-pix-format.html#k4-full-range-limited-tv-pc","text":"When encoding video (and decoding it), an important parameter is the color range . This parameter has its historical origin at the time of the passage from analog TV to digital RGB screens. It gives a range of possible levels on each of the channels * of color (red, green, blue).","title":"K.4 - Full range / Limited / TV / PC ?"},{"location":"K-pix-format.html#k4a-full-range-pc","text":"Digital computer monitors use the full range of red, green and blue levels for color reproduction. With the 8 bits * per channel * most common, this means that each channel stores values between 0 and 255. 0 represents black, and 255 represents white.","title":"K.4.a - Full range / PC"},{"location":"K-pix-format.html#k4b-limited-range-tv","text":"Televisions are expected to use the range known as limited of the levels; this range is originally adapted specifically to represent more correctly the contrasts of the films and corresponds with 8 bits * by channel * to values between 16 and 235. This means that in television, the value 16 represents black, and the value 235 represents white. All values below 16 are ignored (more black than black) as well as all values above 235 (more white than white) 13 .","title":"K.4.b - Limited range / TV"},{"location":"K-pix-format.html#k4c-practical-conclusion","text":"It is therefore necessary to know both your material when reproducing a video, and what to do when encoding it.","title":"K.4.c - Practical conclusion"},{"location":"K-pix-format.html#k4ca-encoding","text":"In the vast majority of cases, video standards recommend encoding in limited range / TV : videos are intended to be seen in television conditions (including on the Internet). This is the case for example for mp4 in h.264 or h.265 , for mkv , and for all broadcast formats. On the other hand, image formats ( PNG , Jpeg , openEXR , etc.), as well as intermediate video formats (those used during production and not broadcasting, such as Prores ), being intended for a computer environment, use rather the full range / PC . It is important to respect these standards to be sure that the files are correctly interpreted by the viewers\u2019 equipment, and always inquire about the formats recommended by the broadcasters.","title":"K.4.c.a - Encoding"},{"location":"K-pix-format.html#k4cb-playback-and-display","text":"When playing videos, it is also necessary that the whole system is correctly configured; a common (not to say recurrent) problem on computers is that videos are left in limited range while the screen is full range . During playback, the video source must be converted to match the screen or projector. Without conversion, a limited range video on a full range screen will be \u201cdull\u201d: there will be no black or white, the range of the image going only from light to dark gray. Conversely, a full range video on a limited range TV will have a loss of information in both highlights and shadows, with large parts completely black or white (the image will be too contrasted). So the hardware must be set up correctly. On a disc player or console, there must be a setting to specify whether the connected display is full range or limited range (as a general rule, a computer monitor or video projector is full range , a TV limited range ). On a computer, things can be a bit more complex: you have to start by checking the settings of the graphics card driver, usually in a section called \u201cvideo\u201d, and specify full range / PC (unless it is a TV that is connected to the computer) 14 . If after having set this parameter variations are still visible, you have to check that the software used to play the video does not make a bad conversion (for example Quicktime on Windows was known for this 15 ); most of these softwares should however let the graphics card do this conversion and not cause any problem (this is the case of VLC , of the video player of Windows , of Totem under Linux \u2026). Example of parameters via the settings of a Nvidia graphics card (under Linux ). Note especially here the color range parameter, to be set to Full if the screen is a computer screen, and Limited if it is a TV. Warning On some hardware, an \u201cAuto\u201d option is available in addition to full / limited range . In this case the hardware try to detect the type of screen connected. Since this is a parameter that should only be changed when the screen is changed, it is strongly recommended to set it manually. Sources & References Sous-\u00e9chantillonage de la chrominance sur Wikipedia Color Depth sur Wikipedia (en anglais) RGB Full vs Limited sur Reference Home Theater (en anglais) In reality, we speak of either luminance (ZZ-vocabulaire.md) or luminosity (ZZ-vocabulaire.md) or luma * : - The luminance has a linear transfer curve. - The luma/luminosity/brightness has a gamma. When we speak of luminance we note YUV while when we speak of brightness we should note Y\u2019UV , but most of the time we omit the premium. See chapter L - Transfer curves, linear space and gamma *. \u21a9 The general term YUV actually covers two families, each declined in luminance or luminosity ( Y or Y\u2019 ): - In analog we speak of YUV and Y\u2019UV , or sometimes YPbPr and Y\u2019PbPr . - In digital the exact term is YCbCr or Y\u2019CbCr , and sometimes YCC . But it is generally the term YUV that is used in all these different cases\u2026 \u21a9 In fact, YUV and RGB can be used interchangeably, but some standards and color spaces specify one or the other, or both. For example sRGB is specifically intended to be used on RGB encoding, while Rec.709 specifies that it can be used in both RGB and YUV . \u21a9 This is the case for GIF which contains a maximum of 256 different colors in its palette. This principle can also be used for PNG and other formats. The aim here is to reduce the overall size of the file by storing less information (only the palette with the colors described in RGB , and only one channel per pixel; this way we can reduce the overall size by a factor of three). \u21a9 This list are the most common subsamples, but there are rarer ones ( 4:2:1 , 4:1:1 ), or more complex or downright exotic ones ( 3:1.5:1.5 , 3:1:1 )\u2026 \u21a9 By default, the subsampling of the alpha channel is the same as that of the luminance * . \u21a9 This mode exists in the openEXR format: it is the option noted \u201cLuminance/Chroma\u201d. \u21a9 In current computing where the byte * is also the Byte * , the smallest unit of memory, one cannot use \u201chalf bytes\u201d (or any other fraction). Using an integer number of bytes per pixel means that the number of pixels in the image can be completely arbitrary; if the number of bytes per pixel is not integer, several pixels will have to share bytes, and the number of pixels is constrained: the total number of bytes in the image must be integer. This is why the number of rows and columns in a mp4 video must be even for example. \u21a9 It is easy to see why: let\u2019s imagine that we have to divide a value of 127 by two. The result will be rounded to 63 or 64 . If other calculations follow, the precision drops very quickly and so does the quality. \u21a9 In 2021, the standard for Ultra-High Definition ( 4K ) Rec.2020 , still little used in broadcasting but already standardized, which has a very wide gamut, recommends 12 bits in luminance for its \u201cHDR\u201d mode. \u21a9 The Digital Cinema Package ( DCP ) (where the image is in fact encoded under the JPEG 2000 lossless standard) in use in 2021 encodes the colors in a XYZ space with 12 bpc . \u21a9 We won\u2019t go into the technical details of how these values are represented, either by a float number (between 0.0 and 1.0 and potentially negative too) or by an integer, but it is important to know that this distinction exists as much for the 16 bpc as for the 32 bpc . When in doubt, prefer floating point numbers. \u21a9 Some blu-rays and game consoles take advantage of this limit to add speculars (brightness) beyond the white and make them brighter. If the TV is compatible, it will display \u201csuper whites\u201d, otherwise it will simply ignore this information, without affecting the image. \u21a9 For a long time, NVidia graphics card drivers under Windows were configured to display videos in limited range by default\u2026 \u21a9 Although loved by animators for its ability to easily play videos frame by frame, Quicktime under Windows is to be avoided for its poor color management; its development has been abandoned by Apple anyway. DJV , available for Windows , Mac OS , Linux , as well as BSD , replaces it efficiently and integrates a professional color management. \u21a9","title":"K.4.c.b - Playback and display"},{"location":"L-transfer.html","text":"I.L - Transfer curves, linear space and gamma \u00b6 When the human eye perceives colors, it distinguishes better the contrasts in the weak intensities, the shades, than in the strong lights; in other words, the response to the intensity is not linear (proportional) 1 : a light that we perceive at \u201chalf\u201d the intensity of another light is not really and physically half as intense, but rather four times less intense. cf. chapter C - Perception of light and colors by the human eye . To optimize the amount of data and the quality of images in video, and later in digital images, instead of representing light in a physical way, we therefore used non-linear transfer curves , which we call a gamma correction 2 in its simple form 3 , which simulates this non-linear perception of human vision and allows us to store the values as they are perceived . The color spaces therefore each define their own transfer curve, and it is often possible to linearize the spaces used as workspaces in applications, to get closer to the physical light during the work (and simplify the calculations). The interest of non-linear spaces is therefore twofold: as spaces for storage and for diffusion, they make it possible to limit the quantity of data without any visible loss of quality; as workspaces, they make it possible to work with intuitive values and color selectors, which work in the same way as our perception of colors. *The selection of dark colors is much more difficult in linear, while with a gamma the scale of brightness seems more regular and logical. However, linear spaces also have their advantages: they simplify calculations (for developers), and by simulating real and physical light, allow 3D rendering engines to efficiently generate realistic images. In two dimensions too, linear spaces allow better calculations in the fusion of colors (the different types of transparency) and solve the problems of banding that appear in some color combinations, allowing more realistic, more logical blends. *Notice how the colors blend more naturally in linear, especially the blue in the red that pulls on the magenta, and especially how the blends do not darken or desaturate the colors. Note It is important to note that choosing a linear space to store (and work with) images imposes a higher color depth , in order to keep the quality when converting to non-linear display and broadcast spaces. Sources & References We speak of \u201clinear\u201d because the graphical representation of the mathematical function corresponding to a given physical intensity its human perception would be a affine function, proportional, represented by a straight line. \u21a9 In reality, gamma correction was created to compensate for the fact that the light intensity of the first CRT screens was not linear either. But in digital, gamma correction is used to optimize data storage and bandwidth. The fact that this modern gamma correction is close to the one used with the old CRT screens is both a coincidence and the result of engineering that aimed to simplify the process. \u21a9 The gamma correction \u03b3 is not the Gamma function \u0393 of mathematics, but a simple function using a power (often close to the square, the most common gamma varying around the value 2 ). \u21a9","title":"L - Transfer curves, linear space and gamma"},{"location":"L-transfer.html#il-transfer-curves-linear-space-and-gamma","text":"When the human eye perceives colors, it distinguishes better the contrasts in the weak intensities, the shades, than in the strong lights; in other words, the response to the intensity is not linear (proportional) 1 : a light that we perceive at \u201chalf\u201d the intensity of another light is not really and physically half as intense, but rather four times less intense. cf. chapter C - Perception of light and colors by the human eye . To optimize the amount of data and the quality of images in video, and later in digital images, instead of representing light in a physical way, we therefore used non-linear transfer curves , which we call a gamma correction 2 in its simple form 3 , which simulates this non-linear perception of human vision and allows us to store the values as they are perceived . The color spaces therefore each define their own transfer curve, and it is often possible to linearize the spaces used as workspaces in applications, to get closer to the physical light during the work (and simplify the calculations). The interest of non-linear spaces is therefore twofold: as spaces for storage and for diffusion, they make it possible to limit the quantity of data without any visible loss of quality; as workspaces, they make it possible to work with intuitive values and color selectors, which work in the same way as our perception of colors. *The selection of dark colors is much more difficult in linear, while with a gamma the scale of brightness seems more regular and logical. However, linear spaces also have their advantages: they simplify calculations (for developers), and by simulating real and physical light, allow 3D rendering engines to efficiently generate realistic images. In two dimensions too, linear spaces allow better calculations in the fusion of colors (the different types of transparency) and solve the problems of banding that appear in some color combinations, allowing more realistic, more logical blends. *Notice how the colors blend more naturally in linear, especially the blue in the red that pulls on the magenta, and especially how the blends do not darken or desaturate the colors. Note It is important to note that choosing a linear space to store (and work with) images imposes a higher color depth , in order to keep the quality when converting to non-linear display and broadcast spaces. Sources & References We speak of \u201clinear\u201d because the graphical representation of the mathematical function corresponding to a given physical intensity its human perception would be a affine function, proportional, represented by a straight line. \u21a9 In reality, gamma correction was created to compensate for the fact that the light intensity of the first CRT screens was not linear either. But in digital, gamma correction is used to optimize data storage and bandwidth. The fact that this modern gamma correction is close to the one used with the old CRT screens is both a coincidence and the result of engineering that aimed to simplify the process. \u21a9 The gamma correction \u03b3 is not the Gamma function \u0393 of mathematics, but a simple function using a power (often close to the square, the most common gamma varying around the value 2 ). \u21a9","title":"I.L - Transfer curves, linear space and gamma"},{"location":"M-LUT.html","text":"I.M - LUTs \u00b6 The acronym LUT comes from Lookup Table . I.M - LUTs M.1 - Description M.2 - Use M.1 - Description \u00b6 As its name suggests, it is a simple array of values, allowing to match some input values with other output values. In computing, it is a convenient way to replace complex mathematical functions, the result of time-consuming calculations, with a simple search in the array, which can greatly improve performance: instead of having to recalculate, you already have the results in memory. We keep the use in colors; it is a simple way to match a color in input to a different color in output: it is therefore a table that allows to retouch the colors or convert color spaces, describing them, color by color. The main interest of a LUT is that any program can easily use them and thus convert from and to color spaces which are not provided for at the base by the program; instead of having to \u201cknow\u201d the mathematical functions which define a color space (in particular the transfer curve), the program has only to read the results in the mapping table without having to carry out the calculation. It is a utility tool, for conversion. They are also a way to \u201csave\u201d any colorimetric adjustment; even if an adjustment is made using multiple effects in an application (levels, curves, saturation\u2026), it can be described and saved as a single LUT and then reapplied easily, and in any application capable of reading a LUT . In this case, it is an artistic and practical use. Here is an example of a few lines of values from a LUT , which is really just a text file containing such an array of values: R G B 0 0 0 0 0 298 0 0 596 0 0 894 0 351 0 0 326 260 0 302 558 0 278 856 381 3784 0 282 3760 0 184 3736 117 85 3712 415 A LUT does not describe all possible color matches, but a selection of colors; to obtain a color that is not in the table, it is necessary to perform interpolation . Normally, the application using the LUT proposes different interpolations which will change the way these colors are obtained (by rounding, linear interpolation, etc.). The quantity of values contained in a LUT is thus very important for the quality of the output image: being only a table of limited values, there can be a loss during the conversion and the result can more or less differ from a true mathematical operation. However, the difference is not discernible in the case of LUT with many values, such as those used by OCIO + ACES for its conversions for example. See Chapter N - OCIO, ACES . M.2 - Use \u00b6 There are many different file formats for LUTs , depending on applications, developers, etc. Here are some examples (formats supported by FFmpeg ): Extension Name, publisher, application\u2026 3dl Discreet / After Effects / Autodesk cube Iridas / Resolve dat DaVinci m3d Pandora csp cineSpace spi1d , spi3d Sony Pictures Imageworks It can be useful to convert between different LUT formats; OCIO includes a command line conversion tool, for which the media encoder DuME provides a graphical interface. There are two main types of LUT : LUT 1D and LUT 3D . 1D LUT 1D LUT work only on the luminance and do not modify the colors. Having only one dimension, they are lighter and simpler than the 3D LUT and are useful for the conversions of gamma * , for example between two colorimetric spaces which would have the same primaries * . See chapter L - Transfer curves, linear space and gamma . 3D LUT The 3D LUT work on the three channels red, green and blue. They therefore allow detailed retouching of both luminance and hues as well as saturation. They allow artistic effects as well as conversions from and to any color space. Whatever the type of LUT , these conversion tables offer only a simple correspondence of value, each LUT is designed for a precise space and colors in entry and cannot be used indifferently on any image ! For example, a LUT that converts to the Rec.2020 color space is certainly intended to be applied to an image in another specific color space, e.g. Rec.709 ; in this example, it cannot be used on an sRGB image, unless it has first been converted to Rec.709 . This is also true for LUTs that are for artistic use, not utilitarian. It is necessary to know on which space and which type of image each LUT is supposed to be applied. Depending on the application, the choice of workspace is therefore important when using LUT , or at least you must be aware of it in order to make any conversions prior to using LUT (which means that they are not as simple as they seem). Sources & References Lookup table on Wikipedia 1D vs 3D LUTs by James Ritson on Affinity Spotlight 1 Beware, his conclusion contains an error. In the otherwise very good article, James Ritson explains that a 1D LUT can be used to convert an image from Rec.2020 to Rec.709 , which is wrong since the primaries * of the two spaces are different, not just the transfer curve * . You must then use a 3D LUT . On the other hand the conversion from Rec.709 to sRGB is well possible with a LUT 1D since only the transfer curve is different. \u21a9","title":"M - LUTs"},{"location":"M-LUT.html#im-luts","text":"The acronym LUT comes from Lookup Table . I.M - LUTs M.1 - Description M.2 - Use","title":"I.M - LUTs"},{"location":"M-LUT.html#m1-description","text":"As its name suggests, it is a simple array of values, allowing to match some input values with other output values. In computing, it is a convenient way to replace complex mathematical functions, the result of time-consuming calculations, with a simple search in the array, which can greatly improve performance: instead of having to recalculate, you already have the results in memory. We keep the use in colors; it is a simple way to match a color in input to a different color in output: it is therefore a table that allows to retouch the colors or convert color spaces, describing them, color by color. The main interest of a LUT is that any program can easily use them and thus convert from and to color spaces which are not provided for at the base by the program; instead of having to \u201cknow\u201d the mathematical functions which define a color space (in particular the transfer curve), the program has only to read the results in the mapping table without having to carry out the calculation. It is a utility tool, for conversion. They are also a way to \u201csave\u201d any colorimetric adjustment; even if an adjustment is made using multiple effects in an application (levels, curves, saturation\u2026), it can be described and saved as a single LUT and then reapplied easily, and in any application capable of reading a LUT . In this case, it is an artistic and practical use. Here is an example of a few lines of values from a LUT , which is really just a text file containing such an array of values: R G B 0 0 0 0 0 298 0 0 596 0 0 894 0 351 0 0 326 260 0 302 558 0 278 856 381 3784 0 282 3760 0 184 3736 117 85 3712 415 A LUT does not describe all possible color matches, but a selection of colors; to obtain a color that is not in the table, it is necessary to perform interpolation . Normally, the application using the LUT proposes different interpolations which will change the way these colors are obtained (by rounding, linear interpolation, etc.). The quantity of values contained in a LUT is thus very important for the quality of the output image: being only a table of limited values, there can be a loss during the conversion and the result can more or less differ from a true mathematical operation. However, the difference is not discernible in the case of LUT with many values, such as those used by OCIO + ACES for its conversions for example. See Chapter N - OCIO, ACES .","title":"M.1 - Description"},{"location":"M-LUT.html#m2-use","text":"There are many different file formats for LUTs , depending on applications, developers, etc. Here are some examples (formats supported by FFmpeg ): Extension Name, publisher, application\u2026 3dl Discreet / After Effects / Autodesk cube Iridas / Resolve dat DaVinci m3d Pandora csp cineSpace spi1d , spi3d Sony Pictures Imageworks It can be useful to convert between different LUT formats; OCIO includes a command line conversion tool, for which the media encoder DuME provides a graphical interface. There are two main types of LUT : LUT 1D and LUT 3D . 1D LUT 1D LUT work only on the luminance and do not modify the colors. Having only one dimension, they are lighter and simpler than the 3D LUT and are useful for the conversions of gamma * , for example between two colorimetric spaces which would have the same primaries * . See chapter L - Transfer curves, linear space and gamma . 3D LUT The 3D LUT work on the three channels red, green and blue. They therefore allow detailed retouching of both luminance and hues as well as saturation. They allow artistic effects as well as conversions from and to any color space. Whatever the type of LUT , these conversion tables offer only a simple correspondence of value, each LUT is designed for a precise space and colors in entry and cannot be used indifferently on any image ! For example, a LUT that converts to the Rec.2020 color space is certainly intended to be applied to an image in another specific color space, e.g. Rec.709 ; in this example, it cannot be used on an sRGB image, unless it has first been converted to Rec.709 . This is also true for LUTs that are for artistic use, not utilitarian. It is necessary to know on which space and which type of image each LUT is supposed to be applied. Depending on the application, the choice of workspace is therefore important when using LUT , or at least you must be aware of it in order to make any conversions prior to using LUT (which means that they are not as simple as they seem). Sources & References Lookup table on Wikipedia 1D vs 3D LUTs by James Ritson on Affinity Spotlight 1 Beware, his conclusion contains an error. In the otherwise very good article, James Ritson explains that a 1D LUT can be used to convert an image from Rec.2020 to Rec.709 , which is wrong since the primaries * of the two spaces are different, not just the transfer curve * . You must then use a 3D LUT . On the other hand the conversion from Rec.709 to sRGB is well possible with a LUT 1D since only the transfer curve is different. \u21a9","title":"M.2 - Use"},{"location":"N-ocio.html","text":"I.N - OpenColorIO and ACES \u00b6 OpenColorIO 1 , often abbreviated as OCIO , is a free and open source library for developers (not users), facilitating color management. It aims to be used by all applications in digital image production, thus facilitating color management throughout the production pipeline: by becoming standard, it makes it possible to share the color configuration throughout the production pipeline and to have control over it at all stages, as easily as possible. I.N - OpenColorIO and ACES N.1 - Compatible applications N.2 - ACES N.2.a - ACES color space N.2.a.a - ACES2065-1 N.2.a.b - ACEScg N.2.a.c - ACEScc Applications using OCIO therefore all share the same color management configuration format, a file named congig.ocio . It is thus possible to easily share the same color management throughout the production pipeline, defining workspaces, file spaces, color picker spaces, display spaces, final output spaces\u2026 See Chapter II.A - Practical Implementation *. Note OpenColorIO also brings support for a large number of LUTs * formats, solving the problem of LUTs compatibility with applications. Several OCIO configurations are provided as examples that can already be used in production as is, including the ACES configuration (see below) or the Sony Pictures Imageworks configuration for animation ( spi-anim ) and special effects ( spi-vfx ). N.1 - Compatible applications \u00b6 Here is an alphabetical list (not exhaustive) of the main applications using OpenColorIO , established in early 2021 2 . Application Use Notes Adobe After Effects Compositing Partial support whith a plugin . Autodesk Arnold Render engine Maxon Redshift Render engine Blender, Cycles, Eevee 3D, Render engine, compositing DuME Encoder Partial support via LUTs SideFX Houdini 3D Krita Drawing The Foundry Mari Textures Autodesk Maya 3D Mocha Pro Tracking Modo 3D The Foundry Nuke Compositing Adobe Photoshop Dessin Partial support via ICC profiles Substance Designer Textures Unreal Engine Render engine V-Ray Render engine Hint Any application that can use LUTs * can be integrated into a production line using OCIO , by exporting the LUTs needed for production from OpenColorIO , either via the command line tool provided, or by using the DuME media encoder which can create these LUTs . N.2 - ACES \u00b6 ACES stands for Academy Color Encoding System , and is intended to be a color management and exchange standard for digital imaging; it aims to simplify color management and maintain the highest fidelity in all production pipelines. It is free and open source, and uses OpenColorIO in its operation. ACES actually consists of: 5 color spaces designed for different uses in the production chain. An OpenColorIO configuration, with the LUTs necessary for its use everywhere. A set of recommendations on color calculation and storage. See chapter I - List of color spaces for the technical details concerning the various color spaces of ACES . The main recommendations on storage are: Use the openEXR file format. Save in wide color spaces, encompassing all others. The ideal space encompassing all visible colors is ACES2065-1 . N.2.a - ACES color space \u00b6 The different spaces offered by ACES correspond to different needs and specific uses. Here are the most important ones. N.2.a.a - ACES2065-1 \u00b6 The space ACES2065-1 is the space with the broadest gamut * , including all the visible colors, and its curve of transfer is linear. It is in fact the most complete of the spaces, its goal being to be able to store the colors without loss, and compatible with all the past and future spaces. Like all linear spaces, it requires a depth of at least 16 bpc , with a preference for 32 bpc compatible with its goal of efficiently representing any visible color. It is in fact little used, representing too much data, its interest being mainly theoretical. N.2.a.b - ACEScg \u00b6 The ACEScg (for computer generated (images) ) space is derived from ACES2065-1 , but uses different primaries * , reducing its gamut * , which is still very large, while being more convenient than ACES2065-1 . It is a linear space, designed and ideal for 3D rendering and compositing. With its very wide gamut , and the associated OCIO configuration allowing to efficiently generate images in smaller standard spaces, it allows a color synthesis more faithful to reality and human vision, especially in the highlights. Comparison between rendering using RGB primaries and ACEScg 3 The ACES recommendation is to use this space in animation and CGI production, both as a workspace in applications, and for storage in the openEXR file format. Thus, from rendering and image generation to final delivery, no data conversion is required (except for display of course). N.2.a.c - ACEScc \u00b6 The ACEScc (for color correction ) space is similar to ACEScg except that its transfer curve is not linear. This transfer curve makes it a poor space for rendering and image generation, but makes it much more practical for color grading. It is therefore recommended for color correction and retouching work on filmed images, or at the end of animation production for final color correction. Sources & References OpenColorIO official website Being free and open source, many studios contribute to the development of OpenColorIO , managed by the ASWF, Academy Software Foundation in the USA. \u21a9 A complete list is available on the OpenColorIO website. \u21a9 This comparison is made with Blender ; ACES is not included in Blender , but it is easy to use with OpenColorIO . Although ACES does not come standard with Blender , the Blender renderings are still much better than this example: Blender uses a LUT named Filmic which greatly improves the conversion from linear RGB space to sRGB of the final image (disabled for this example), but still remaining less colorful than the result using the ACES configuration of OCIO . \u21a9","title":"N - OCIO, ACES"},{"location":"N-ocio.html#in-opencolorio-and-aces","text":"OpenColorIO 1 , often abbreviated as OCIO , is a free and open source library for developers (not users), facilitating color management. It aims to be used by all applications in digital image production, thus facilitating color management throughout the production pipeline: by becoming standard, it makes it possible to share the color configuration throughout the production pipeline and to have control over it at all stages, as easily as possible. I.N - OpenColorIO and ACES N.1 - Compatible applications N.2 - ACES N.2.a - ACES color space N.2.a.a - ACES2065-1 N.2.a.b - ACEScg N.2.a.c - ACEScc Applications using OCIO therefore all share the same color management configuration format, a file named congig.ocio . It is thus possible to easily share the same color management throughout the production pipeline, defining workspaces, file spaces, color picker spaces, display spaces, final output spaces\u2026 See Chapter II.A - Practical Implementation *. Note OpenColorIO also brings support for a large number of LUTs * formats, solving the problem of LUTs compatibility with applications. Several OCIO configurations are provided as examples that can already be used in production as is, including the ACES configuration (see below) or the Sony Pictures Imageworks configuration for animation ( spi-anim ) and special effects ( spi-vfx ).","title":"I.N - OpenColorIO and ACES"},{"location":"N-ocio.html#n1-compatible-applications","text":"Here is an alphabetical list (not exhaustive) of the main applications using OpenColorIO , established in early 2021 2 . Application Use Notes Adobe After Effects Compositing Partial support whith a plugin . Autodesk Arnold Render engine Maxon Redshift Render engine Blender, Cycles, Eevee 3D, Render engine, compositing DuME Encoder Partial support via LUTs SideFX Houdini 3D Krita Drawing The Foundry Mari Textures Autodesk Maya 3D Mocha Pro Tracking Modo 3D The Foundry Nuke Compositing Adobe Photoshop Dessin Partial support via ICC profiles Substance Designer Textures Unreal Engine Render engine V-Ray Render engine Hint Any application that can use LUTs * can be integrated into a production line using OCIO , by exporting the LUTs needed for production from OpenColorIO , either via the command line tool provided, or by using the DuME media encoder which can create these LUTs .","title":"N.1 - Compatible applications"},{"location":"N-ocio.html#n2-aces","text":"ACES stands for Academy Color Encoding System , and is intended to be a color management and exchange standard for digital imaging; it aims to simplify color management and maintain the highest fidelity in all production pipelines. It is free and open source, and uses OpenColorIO in its operation. ACES actually consists of: 5 color spaces designed for different uses in the production chain. An OpenColorIO configuration, with the LUTs necessary for its use everywhere. A set of recommendations on color calculation and storage. See chapter I - List of color spaces for the technical details concerning the various color spaces of ACES . The main recommendations on storage are: Use the openEXR file format. Save in wide color spaces, encompassing all others. The ideal space encompassing all visible colors is ACES2065-1 .","title":"N.2 - ACES"},{"location":"N-ocio.html#n2a-aces-color-space","text":"The different spaces offered by ACES correspond to different needs and specific uses. Here are the most important ones.","title":"N.2.a - ACES color space"},{"location":"N-ocio.html#n2aa-aces2065-1","text":"The space ACES2065-1 is the space with the broadest gamut * , including all the visible colors, and its curve of transfer is linear. It is in fact the most complete of the spaces, its goal being to be able to store the colors without loss, and compatible with all the past and future spaces. Like all linear spaces, it requires a depth of at least 16 bpc , with a preference for 32 bpc compatible with its goal of efficiently representing any visible color. It is in fact little used, representing too much data, its interest being mainly theoretical.","title":"N.2.a.a - ACES2065-1"},{"location":"N-ocio.html#n2ab-acescg","text":"The ACEScg (for computer generated (images) ) space is derived from ACES2065-1 , but uses different primaries * , reducing its gamut * , which is still very large, while being more convenient than ACES2065-1 . It is a linear space, designed and ideal for 3D rendering and compositing. With its very wide gamut , and the associated OCIO configuration allowing to efficiently generate images in smaller standard spaces, it allows a color synthesis more faithful to reality and human vision, especially in the highlights. Comparison between rendering using RGB primaries and ACEScg 3 The ACES recommendation is to use this space in animation and CGI production, both as a workspace in applications, and for storage in the openEXR file format. Thus, from rendering and image generation to final delivery, no data conversion is required (except for display of course).","title":"N.2.a.b - ACEScg"},{"location":"N-ocio.html#n2ac-acescc","text":"The ACEScc (for color correction ) space is similar to ACEScg except that its transfer curve is not linear. This transfer curve makes it a poor space for rendering and image generation, but makes it much more practical for color grading. It is therefore recommended for color correction and retouching work on filmed images, or at the end of animation production for final color correction. Sources & References OpenColorIO official website Being free and open source, many studios contribute to the development of OpenColorIO , managed by the ASWF, Academy Software Foundation in the USA. \u21a9 A complete list is available on the OpenColorIO website. \u21a9 This comparison is made with Blender ; ACES is not included in Blender , but it is easy to use with OpenColorIO . Although ACES does not come standard with Blender , the Blender renderings are still much better than this example: Blender uses a LUT named Filmic which greatly improves the conversion from linear RGB space to sRGB of the final image (disabled for this example), but still remaining less colorful than the result using the ACES configuration of OCIO . \u21a9","title":"N.2.a.c - ACEScc"},{"location":"ZZ-bref.html","text":"In a Nutshell: a quick summary of how to get it right \u00b6 Here\u2019s the to-do list for perfect color management without trials and errors! File formats \u00b6 Intermediate, render, master and backup exports \u00b6 Use openEXR for all non-final exports! It is a lossless format It can store any color information from any color space It offers very efficient compression formats (including some lossy ones to further reduce size) Color depth in EXR : Choose 16 bpc (float) if the following pipeline steps use the same gamma * (linear or non-linear). Choose 32 bpc in other cases. The other depth options are only useful in case of resource constraints\u2026 Compression of the EXR : Read our guide on EXR compressions here! Final export and delivery \u00b6 Refer to the standard spaces according to the delivered format (if the customer does not specify a color space). Importing into applications \u00b6 It is always important to know in which space the imported file was made/exported. If you follow these recommendations, the file is always an openEXR file, and most often interpreted by applications as RGB Linear by default. It may be useful to note the color space used directly in the file name. Refer to the standard spaces for other formats if you do not know their color spaces. Display \u00b6 In the vast majority of cases, we display in sRGB without any other simulation or conversion. Only in the case where a screen uses another display space can another parameter be chosen. Workspaces \u00b6 The spaces provided by ACES are extremely practical, and easy to set up using OCIO . If OCIO is not available on the application, the ACES workspaces may still be available natively; as a last resort another wide workspace specific to the application should be chosen. Drawing, textures, \u2026 \u00b6 Either the space of the 3D renderer or the compositing and animation software if possible. Otherwise, RGB Linear or another space with a gamut * very wide and linear. Exports in openEXR 16 bpc . When importing into other applications, be sure to specify the space used when making the image. 3D rendering \u00b6 On Blender : Filmic or ACEScg via an OCIO configuration In general: ACEScg Or any other space with gamut * very wide and linear. Exports to openEXR 16 bpc if compositing is done in a linear space. Compositing \u00b6 In 3D, we try as much as possible to use the color space of the 3D render. In 2D, we prefer a gamut * wide and linear space, ACEScg works very well. If not possible, you can stay in RGB Linear . Exports in openEXR 16 bpc if compositing is not followed by a color correction step in a non-linear space, 32 bpc in other cases. Colorimetric correction \u00b6 We can follow the same reasoning as for compositing, although it may be more convenient to work in a non-linear space. In this case ACEScc is perfect. Exports to openEXR 16 bpc for archiving, and to standard space file for delivery. )","title":"In a Nutshell-a quick summary of how to get it right"},{"location":"ZZ-bref.html#in-a-nutshell-a-quick-summary-of-how-to-get-it-right","text":"Here\u2019s the to-do list for perfect color management without trials and errors!","title":"In a Nutshell: a quick summary of how to get it right"},{"location":"ZZ-bref.html#file-formats","text":"","title":"File formats"},{"location":"ZZ-bref.html#intermediate-render-master-and-backup-exports","text":"Use openEXR for all non-final exports! It is a lossless format It can store any color information from any color space It offers very efficient compression formats (including some lossy ones to further reduce size) Color depth in EXR : Choose 16 bpc (float) if the following pipeline steps use the same gamma * (linear or non-linear). Choose 32 bpc in other cases. The other depth options are only useful in case of resource constraints\u2026 Compression of the EXR : Read our guide on EXR compressions here!","title":"Intermediate, render, master and backup exports"},{"location":"ZZ-bref.html#final-export-and-delivery","text":"Refer to the standard spaces according to the delivered format (if the customer does not specify a color space).","title":"Final export and delivery"},{"location":"ZZ-bref.html#importing-into-applications","text":"It is always important to know in which space the imported file was made/exported. If you follow these recommendations, the file is always an openEXR file, and most often interpreted by applications as RGB Linear by default. It may be useful to note the color space used directly in the file name. Refer to the standard spaces for other formats if you do not know their color spaces.","title":"Importing into applications"},{"location":"ZZ-bref.html#display","text":"In the vast majority of cases, we display in sRGB without any other simulation or conversion. Only in the case where a screen uses another display space can another parameter be chosen.","title":"Display"},{"location":"ZZ-bref.html#workspaces","text":"The spaces provided by ACES are extremely practical, and easy to set up using OCIO . If OCIO is not available on the application, the ACES workspaces may still be available natively; as a last resort another wide workspace specific to the application should be chosen.","title":"Workspaces"},{"location":"ZZ-bref.html#drawing-textures","text":"Either the space of the 3D renderer or the compositing and animation software if possible. Otherwise, RGB Linear or another space with a gamut * very wide and linear. Exports in openEXR 16 bpc . When importing into other applications, be sure to specify the space used when making the image.","title":"Drawing, textures, ..."},{"location":"ZZ-bref.html#3d-rendering","text":"On Blender : Filmic or ACEScg via an OCIO configuration In general: ACEScg Or any other space with gamut * very wide and linear. Exports to openEXR 16 bpc if compositing is done in a linear space.","title":"3D rendering"},{"location":"ZZ-bref.html#compositing","text":"In 3D, we try as much as possible to use the color space of the 3D render. In 2D, we prefer a gamut * wide and linear space, ACEScg works very well. If not possible, you can stay in RGB Linear . Exports in openEXR 16 bpc if compositing is not followed by a color correction step in a non-linear space, 32 bpc in other cases.","title":"Compositing"},{"location":"ZZ-bref.html#colorimetric-correction","text":"We can follow the same reasoning as for compositing, although it may be more convenient to work in a non-linear space. In this case ACEScc is perfect. Exports to openEXR 16 bpc for archiving, and to standard space file for delivery. )","title":"Colorimetric correction"},{"location":"ZZ-download.html","text":"Downloads & Other Resources \u00b6 Downloads \u00b6 With this document, we offer as a free download several tools that you can use to manage your colors. LUT Filmic from Blender: different LUT * formats (including ICC profiles) to use Blender \u2018s Filmic space easily in other applications, especially those that do not support OCIO * Test images and test patterns : images to test the calibration of your screens (and your production pipeline). ACES Anim OpenColorIO Config v1.0.3 : a version of the OCIO * ACES * config better suited to animation production. Videos \u00b6 At the origin of this document, this video recorded live with Motion Caf\u00e9 and Nicolas Dufresne (in French): Later, Nicolas Dufresne also participated in this program on color and in particular in 3D and in Blender , on PFX\u2019s Twitch channel (in French) :","title":"Downloads and Other Resources"},{"location":"ZZ-download.html#downloads-other-resources","text":"","title":"Downloads &amp; Other Resources"},{"location":"ZZ-download.html#downloads","text":"With this document, we offer as a free download several tools that you can use to manage your colors. LUT Filmic from Blender: different LUT * formats (including ICC profiles) to use Blender \u2018s Filmic space easily in other applications, especially those that do not support OCIO * Test images and test patterns : images to test the calibration of your screens (and your production pipeline). ACES Anim OpenColorIO Config v1.0.3 : a version of the OCIO * ACES * config better suited to animation production.","title":"Downloads"},{"location":"ZZ-download.html#videos","text":"At the origin of this document, this video recorded live with Motion Caf\u00e9 and Nicolas Dufresne (in French): Later, Nicolas Dufresne also participated in this program on color and in particular in 3D and in Blender , on PFX\u2019s Twitch channel (in French) :","title":"Videos"},{"location":"ZZ-english.html","text":"Dictionnaire fran\u00e7ais - anglais \u00b6 La plupart des applications et des ressources disponibles \u00e9tant plut\u00f4t en anglais, nous proposons ici un petit dictionnaire fran\u00e7ais-anglais contenant le vocabulaire important dans la gestion des couleurs. ACES : ACES Alpha : Alpha Bit : Bit Byte : Byte Canal (de couleur): (Color) Channel CIE : CIE, International Commission on Illumination Codec : Codec, coder-decoder (couleurs) Compl\u00e9mentaires : Complementary (colors) Conteneur : Container Couche (de couleur): (Color) Channel Courbe de transfert : Transfer curve, trc Format : (File) Format Fr\u00e9quence : Frequency Gamma : Gamma Gamut : Gamut (Couleurs) Homochromes : Metamers Illuminant : Standard Illuminant Intensit\u00e9 : Intensity Lin\u00e9aire : Linear Longueur d\u2019onde : Wavelength Luma : Brightness, Luma Luminance : Luminance Luminosit\u00e9 : Brightness, Luma LUT : LUT, Lookup Table (Couleurs) M\u00e9tam\u00e8res : Metamers (Lumi\u00e8re) Monochromatique : Monochromatic (light) Noir : Black Norme : Norm OCIO : OCIO, OpenColorIO Octet : Octet OIIO : OIIO, OpenImageIO Photon : Photon Point Blanc : White Point Pourpre : Violet (Couleurs) Primaires : Primary Colors, Primaries Puret\u00e9 colorim\u00e9trique : Color Purity, Colorfulness Puret\u00e9 d\u2019excitation : Excitation Purity Saturation : Saturation Teinte : Hue Temp\u00e9rature : Temperature Violet : Violet, Purple","title":"Dictionnaire fran\u00e7ais - anglais"},{"location":"ZZ-english.html#dictionnaire-francais-anglais","text":"La plupart des applications et des ressources disponibles \u00e9tant plut\u00f4t en anglais, nous proposons ici un petit dictionnaire fran\u00e7ais-anglais contenant le vocabulaire important dans la gestion des couleurs. ACES : ACES Alpha : Alpha Bit : Bit Byte : Byte Canal (de couleur): (Color) Channel CIE : CIE, International Commission on Illumination Codec : Codec, coder-decoder (couleurs) Compl\u00e9mentaires : Complementary (colors) Conteneur : Container Couche (de couleur): (Color) Channel Courbe de transfert : Transfer curve, trc Format : (File) Format Fr\u00e9quence : Frequency Gamma : Gamma Gamut : Gamut (Couleurs) Homochromes : Metamers Illuminant : Standard Illuminant Intensit\u00e9 : Intensity Lin\u00e9aire : Linear Longueur d\u2019onde : Wavelength Luma : Brightness, Luma Luminance : Luminance Luminosit\u00e9 : Brightness, Luma LUT : LUT, Lookup Table (Couleurs) M\u00e9tam\u00e8res : Metamers (Lumi\u00e8re) Monochromatique : Monochromatic (light) Noir : Black Norme : Norm OCIO : OCIO, OpenColorIO Octet : Octet OIIO : OIIO, OpenImageIO Photon : Photon Point Blanc : White Point Pourpre : Violet (Couleurs) Primaires : Primary Colors, Primaries Puret\u00e9 colorim\u00e9trique : Color Purity, Colorfulness Puret\u00e9 d\u2019excitation : Excitation Purity Saturation : Saturation Teinte : Hue Temp\u00e9rature : Temperature Violet : Violet, Purple","title":"Dictionnaire fran\u00e7ais - anglais"},{"location":"ZZ-erreurs.html","text":"Common mistakes and misunderstandings, problem solving \u00b6 Common mistakes \u00b6 The workspace must be the same as the file to be delivered. \u00b6 This is not true: when working and calculating colors, it is necessary to work in a space \u201clarger\u201d than the delivery; indeed, the precision during calculations must be greater than that of the data to obtain. To be able to work in a larger space, as well in gamut * as in depth * guarantees a better respect of the colors at the time of export. If the workspace is not larger than the output workspace, flaws may appear, such as banding , the appearance of visible bands in fine gradients. If the workspace is linear, its depth must also be greater than that of the output workspace, and this is in any case recommended even for a non-linear workspace. Thus, for an 8 bpc output, it is preferable to work at least in 16 bpc , or in 32 bpc for an 8 bpc output. It is equally false to believe that working in a larger space will make us select and use colors \u201coutside\u201d the output format. Cf . next question. Working in a color space larger than the output color space is dangerous because we might choose colors that cannot be reproduced. \u00b6 This is not true: even if the workspace is very large (such as ACEScg for example), the colors displayed by the screen are the result of a \u201clive\u201d conversion to the screen space, which is necessarily smaller ( sRGB in most cases). It\u2019s impossible to have variation at the time of the export, which will undergo exactly the same transformation. It should be noted that the majority of the screens (except HDR and other screens in P3 ) cannot display the colors used in HDR and cinema ; the problem is thus rather opposite: how to work on colors apart from the display space of the screen\u2026 However, what is true is that one must be careful in the selection of colors in the rendering engines (3D): the colors chosen on the screen are colors displayed in sRGB , and it\u2019s easy to select colors that are too intense or too saturated without realizing it, because they are actually outside the sRGB space in values. One way to overcome this problem is to make sure that the color pickers of the application are limited to sRGB for example (the application taking care of the conversion from sRGB to the workspace); in this case, the selection of an intense green for example will still be far from the extremes of the larger workspace, and the color will not risk to brighten and saturate the scene too much. You must choose a Rec.709 display space because the video output will be Rec.709 . \u00b6 The display color space must be that of the connected screen ( sRGB in most cases in computing). This display area is used for conversion from the workspace to the screen display. When the video file is output, a conversion is made from the workspace to the video space, Rec.709 in this example. And it is when the video is played that a new conversion takes place again from the Rec.709 to the probable sRGB of the screen. Note However, some applications, especially compositing applications, allow simulation ( soft-proofing or proofing * ) of the conversions that the images undergo once exported; in this case, several conversions take place between the workspace and the display: \u2022 conversion from the workspace to the (simulated) export space ( Rec.709 in the example) \u2022 conversion from export to screen display ( sRGB in the example) But in no case is there a conversion to a Rec.709 display to be made. The activation/deactivation of this simulation should not, in theory, change the display of the image; the changes are only due to the loss of precision of the successive conversions that are simulated. Problem solving \u00b6 When playing a video, the colors are not the same as in the software that was used to create it. \u00b6 Here are the parameters to check: Display problem in the software: the display color space in the software must match that of the screen ( sRGB most often). Problem of output : the color space of output of the video is not standard ; check the space used in output of the application. Playback problem : the video playback is badly configured on the graphics card (see below When a video is played on the computer (\u2026) the colors appear \u201cdull\u201d. ) If a (very slight) variation persists, it may be due to the conversion from RGB to YUV , or the compression of the video format in particular. In this case, there is nothing to do\u2026 \u201cBanding\u201d : colored \u201cbands\u201d appear in the fine gradations \u00b6 Most often, this problem comes from the fact that the application workspace is too small and the depth * is too small compared to the output format. Here is what to check and what can correct the problem: The depth in the application should be higher than the output ( 16 bpc minimum to output in 8 bpc , etc.), especially if the workspace is linear. If it\u2019s not possible to increase the depth of the workspace, when possible a space with the same gamma * as the output one can help reduce the problem. Choose a workspace at gamut * that is larger than the output one. If the gradient is part of an imported file, check the above points in the application that created the file. Banding problems may occur due to compression; check if they are still visible in lossless formats ( openEXR , PNG , Quicktime Animation , etc.) Adding a very slight, imperceptible noise or grain to the image often \u201cfixes\u201d this problem. A fringe, a line that appears blurred, appears in colored or high contrast areas, especially on vertical or horizontal lines \u00b6 This problem is usually a consequence of chrominance subsampling , when switching from RGB to YUV . If possible, increase the subsampling ( 4:4:4 or 4:2:2 instead of 4:2:0 for example). If this is not possible, and the image is a still image, it is sometimes possible to move the image a pixel or two in the video to correct the problem. When you play a video on your computer, in most players and on videos on websites, the colors look \u201cdull\u201d. Whites are light gray, blacks are dark gray. \u00b6 There is probably a conversion error from Full to Limited or vice versa. In the parameters of the graphics card, a \u201cvideo\u201d option often allows to change this setting; you have to choose Full if it is a computer screen that is connected, or Limited if it is a TV. Sources and references )","title":"Common Errors and Misunderstandings, Problem Solving"},{"location":"ZZ-erreurs.html#common-mistakes-and-misunderstandings-problem-solving","text":"","title":"Common mistakes and misunderstandings, problem solving"},{"location":"ZZ-erreurs.html#common-mistakes","text":"","title":"Common mistakes"},{"location":"ZZ-erreurs.html#the-workspace-must-be-the-same-as-the-file-to-be-delivered","text":"This is not true: when working and calculating colors, it is necessary to work in a space \u201clarger\u201d than the delivery; indeed, the precision during calculations must be greater than that of the data to obtain. To be able to work in a larger space, as well in gamut * as in depth * guarantees a better respect of the colors at the time of export. If the workspace is not larger than the output workspace, flaws may appear, such as banding , the appearance of visible bands in fine gradients. If the workspace is linear, its depth must also be greater than that of the output workspace, and this is in any case recommended even for a non-linear workspace. Thus, for an 8 bpc output, it is preferable to work at least in 16 bpc , or in 32 bpc for an 8 bpc output. It is equally false to believe that working in a larger space will make us select and use colors \u201coutside\u201d the output format. Cf . next question.","title":"The workspace must be the same as the file to be delivered."},{"location":"ZZ-erreurs.html#working-in-a-color-space-larger-than-the-output-color-space-is-dangerous-because-we-might-choose-colors-that-cannot-be-reproduced","text":"This is not true: even if the workspace is very large (such as ACEScg for example), the colors displayed by the screen are the result of a \u201clive\u201d conversion to the screen space, which is necessarily smaller ( sRGB in most cases). It\u2019s impossible to have variation at the time of the export, which will undergo exactly the same transformation. It should be noted that the majority of the screens (except HDR and other screens in P3 ) cannot display the colors used in HDR and cinema ; the problem is thus rather opposite: how to work on colors apart from the display space of the screen\u2026 However, what is true is that one must be careful in the selection of colors in the rendering engines (3D): the colors chosen on the screen are colors displayed in sRGB , and it\u2019s easy to select colors that are too intense or too saturated without realizing it, because they are actually outside the sRGB space in values. One way to overcome this problem is to make sure that the color pickers of the application are limited to sRGB for example (the application taking care of the conversion from sRGB to the workspace); in this case, the selection of an intense green for example will still be far from the extremes of the larger workspace, and the color will not risk to brighten and saturate the scene too much.","title":"Working in a color space larger than the output color space is dangerous because we might choose colors that cannot be reproduced."},{"location":"ZZ-erreurs.html#you-must-choose-a-rec709-display-space-because-the-video-output-will-be-rec709","text":"The display color space must be that of the connected screen ( sRGB in most cases in computing). This display area is used for conversion from the workspace to the screen display. When the video file is output, a conversion is made from the workspace to the video space, Rec.709 in this example. And it is when the video is played that a new conversion takes place again from the Rec.709 to the probable sRGB of the screen. Note However, some applications, especially compositing applications, allow simulation ( soft-proofing or proofing * ) of the conversions that the images undergo once exported; in this case, several conversions take place between the workspace and the display: \u2022 conversion from the workspace to the (simulated) export space ( Rec.709 in the example) \u2022 conversion from export to screen display ( sRGB in the example) But in no case is there a conversion to a Rec.709 display to be made. The activation/deactivation of this simulation should not, in theory, change the display of the image; the changes are only due to the loss of precision of the successive conversions that are simulated.","title":"You must choose a Rec.709 display space because the video output will be Rec.709."},{"location":"ZZ-erreurs.html#problem-solving","text":"","title":"Problem solving"},{"location":"ZZ-erreurs.html#when-playing-a-video-the-colors-are-not-the-same-as-in-the-software-that-was-used-to-create-it","text":"Here are the parameters to check: Display problem in the software: the display color space in the software must match that of the screen ( sRGB most often). Problem of output : the color space of output of the video is not standard ; check the space used in output of the application. Playback problem : the video playback is badly configured on the graphics card (see below When a video is played on the computer (\u2026) the colors appear \u201cdull\u201d. ) If a (very slight) variation persists, it may be due to the conversion from RGB to YUV , or the compression of the video format in particular. In this case, there is nothing to do\u2026","title":"When playing a video, the colors are not the same as in the software that was used to create it."},{"location":"ZZ-erreurs.html#banding-colored-bands-appear-in-the-fine-gradations","text":"Most often, this problem comes from the fact that the application workspace is too small and the depth * is too small compared to the output format. Here is what to check and what can correct the problem: The depth in the application should be higher than the output ( 16 bpc minimum to output in 8 bpc , etc.), especially if the workspace is linear. If it\u2019s not possible to increase the depth of the workspace, when possible a space with the same gamma * as the output one can help reduce the problem. Choose a workspace at gamut * that is larger than the output one. If the gradient is part of an imported file, check the above points in the application that created the file. Banding problems may occur due to compression; check if they are still visible in lossless formats ( openEXR , PNG , Quicktime Animation , etc.) Adding a very slight, imperceptible noise or grain to the image often \u201cfixes\u201d this problem.","title":"\"Banding\" : colored \"bands\" appear in the fine gradations"},{"location":"ZZ-erreurs.html#a-fringe-a-line-that-appears-blurred-appears-in-colored-or-high-contrast-areas-especially-on-vertical-or-horizontal-lines","text":"This problem is usually a consequence of chrominance subsampling , when switching from RGB to YUV . If possible, increase the subsampling ( 4:4:4 or 4:2:2 instead of 4:2:0 for example). If this is not possible, and the image is a still image, it is sometimes possible to move the image a pixel or two in the video to correct the problem.","title":"A fringe, a line that appears blurred, appears in colored or high contrast areas, especially on vertical or horizontal lines"},{"location":"ZZ-erreurs.html#when-you-play-a-video-on-your-computer-in-most-players-and-on-videos-on-websites-the-colors-look-dull-whites-are-light-gray-blacks-are-dark-gray","text":"There is probably a conversion error from Full to Limited or vice versa. In the parameters of the graphics card, a \u201cvideo\u201d option often allows to change this setting; you have to choose Full if it is a computer screen that is connected, or Limited if it is a TV. Sources and references )","title":"When you play a video on your computer, in most players and on videos on websites, the colors look \"dull\". Whites are light gray, blacks are dark gray."},{"location":"ZZ-vocabulaire.html","text":"Glossary \u00b6 Alphabetical listing. ACES : Academy Color Encoding System is a standard of exchange, of color management for digital imaging. It aims to simplify color management by maintaining color fidelity throughout the production pipeline. The standard is free and open source and many companies contribute to its development. It uses in particular the OCIO * software library. Alpha : a fourth channel is sometimes added to the pixels of a video to store transparency information in addition to color. The RGBA and YUVA channels are usually noted in this case, and in the case of YUVA a fourth value is sometimes added to the chroma subsampling acronym: 4:4:4:4 , 4:2:2:4*, etc. Bit : The basic unit of both binary computer calculation and storage. A bit is either 0 or 1. 8 bits make up a byte , a sequence of eight 0s or 1s. Not to be confused with the Byte . Noted b (whereas Byte is noted B and byte is noted o ). Byte : smallest unit of usable memory on a given system (pronounced /ba\u026at/); usually made up of 8 bits and (only in this case but not necessarily) equivalent to one byte Candela : Unit of the international system representing a luminous intensity, i.e. a quantity of light emitted by a light source in a given direction (whereas the lumen* represents this quantity of light in total, in all directions). Symbol : cd . Cf . Intensity . CIE : La Commission internationale de l\u2019\u00e9clairage is an international organization dedicated to light, lighting, color and color spaces. It was founded in Berlin in 1913 and is currently based in Vienna, Austria. Note: the French acronym CIE is the one used internationally although in English it is the International Commission on Illumination . Codec : abbreviation for encoder-decoder. Software used to encode and decode a video or audio stream into a certain standard* . By the way, the codec is often confused with the standard or format , but it is indeed software and not a format . For example, x264 or nvenc are codecs allowing to encode a video in the standard h.264 (in format mp4 for example). (Colors) complementary : set of colors that, when mixed, cancel the perception of color, producing a neutral gray. Two complementary colors are diametrically opposed on the color wheel. In the chromaticity diagram CIE XYZ , the points that represent them are aligned on either side of the white point. Container : synonym for file type . Defines how multimedia streams (audio, video, subtitles, metadata\u2026) should be saved together in a specific file. For example: Quicktime ( *.mov ), MP4 ( *.mp4 ), Matroska ( *.mkv ) are containers (but h.264 is a standard , and x264 is a codec* ). Some containers are specialized and impose a certain standard (e.g., a MP4 should always use the h.264 or h.265 standard ), while others allow a large number of different standards (e.g., Quicktime allows PNG , Prores , RLE/Animation , MJPEG*, etc.). Contrast : difference in value between the most intense and the least intense point in an image. Channel (color) : AIn a pixel, the color is described by several values; each value is a channel of the pixel (or image). In RGB the three channels are red, green, and blue, in YUV , the luminance and two channels of chrominance . Transfer curve : mathematical function used to convert the intensities of a given color space to a linear scale (and vice versa). It is also called gamma even if the name gamma should rather be limited to only those functions that consist of a mathematical function of power , while transfer functions can take more varied and complex forms; they can however all have a relatively accurate approximation in a simple gamma . see also gamma and the chapter entitled Transfer curves, linear space and gamma . Proofing or Softproofing: Simulation of color conversion in a specific space which will be that of the final export (followed by a conversion towards the space of the screen for a correct display). This technique makes it possible to check that the successive conversions of the colors do not degrade the image too much and that the new colors are close to what one seeks, by checking what will see the final spectator. It is important to note that softproofing can remain far from reality if the space of exit is larger than that of the screen or too different (for example CMJN on a screen sRGB or Rec.2020 on a screen sRGB ); it makes it possible however to preview what a user will see on a similar screen. Format : synonym for file type . See container . Not to be confused with standard and codec . Frequency : in the ondulatory representation of the light (and of all the range of electromagnetic waves), the frequency, measured in Hertz ( Hz ) is the number of undulations per second; it is the inverse of the wavelength* ( F = 1 / \u03bb with F for the frequency and \u03bb for the wavelength): when the frequency increases, the wavelength decreases. In the visible part of the electromagnetic waves (the light), towards 10 15 Hz ( 1000 TeraHertz ), each frequency corresponds to a precise color, a monochromatic light. Gamma : Mathematical function used as a transfer curve and for colorimetric retouching, consisting of a power; the gamma value is that of the power (or the inverse of the power for inverse retouching): x 1/2.4 for a gamma 2.4, for example. see also transfer curve and the chapter titled Transfer Curves, Linear Space, and Gamma . Gamut : extent of colors that can be represented by a color space, represented by a subsurface in the chromaticity diagram CIE XYZ , in triangle when the space uses three primary. The area then represents the gamut ; we also speak of the \u201cwidth\u201d of the gamut : the more colors the space contains, the more its gamut is \u201cwide\u201d. (Colors) Homochromes : cf. Metamers Illuminant : coordinates (in the CIE XYZ ) of the point representing conventionally the white in a given color space. It can also be referred to by a standardized name, such as D65 or D60 , or by a blackbody temperature, in Kelvin, 6500 K for example. See also White Temperature . Intensity : quantity of light received by a surface (reflecting or a sensor) or emitted by a light source, measured in number of photons received by a given surface in a given time (for example in photons per second per square centimeter). In the case of a light emitter (candle, lamp, screen), we speak in terms of a cone (a solid angle) of emission and not a receiving surface and the unit is the candela * noted cd 1 . Linear : graphic representation of a proportional function, a mathematical function known as affine , represented by a straight line. The term linear is used to speak about color spaces whose correspondences of values with the physical intensity are proportional and thus represented by a straight line on a graph. Wavelength : in the ondulatory representation of the light (and of all the range of electromagnetic waves), the wavelength, measured in nanometers ( nm ) is the \u201csize\u201d of the undulations ; it is the reverse of the frequency* ( \u03bb = 1 / F with F for the frequency and \u03bb for the wavelength) : when the frequency increases, the wavelength decreases. In the visible part of the electromagnetic waves (the light thus), in 400nm and 700nm approximately, each wavelength corresponds to a precise color, a *monochromatic* light. Luminance : representation of the physical intensity of light (or color) corresponding to a color on a linear scale. Different from luminosity or luma . Luminosity or Luma : representation of the intensity of the light (or the color), but on a scale adapted to the human perception, using a gamma (cf Transfer curve ), contrary to the luminance . LUT : from Lookup Table . A LUT allows in computer science (especially in its early days) to replace complex mathematical functions (and especially time-consuming to realize) by tables of values; instead of calculating correspondences and conversions, one finds the value in the table. They are still used a lot in colorimetry: they allow to convert colors between different color spaces without worrying about the corresponding mathematical formula (and are thus an easy way to make a conversion between spaces in a software which does not manage them natively). They can cause a (small) loss of information if they are not precise enough (do not contain enough values). They are also used as calibration or colorimetry presets and can allow to easily replicate an effect, even in a software that would not allow it with its native color tools (as long as it can apply a LUT anyway). See the section entitled The LUTs for more details. (Colors) Metamers : two (or more) colors are said metamers when they are of identical appearance (the eye and the brain do not make the difference) although they are composed in reality of a different mixture of monochromatic rays *. Lumen : Unit of the international system representing luminous flux, symbol: lm . Measured by the total amount of light emitted by a light source in a given time. (Light) Monochromatic : pure light, composed of rays of a single precise frequency , without mixing frequencies* , like the light generated by a laser . The color of such a light is part of the spectrum of visible electro-magnetic waves, of the rainbow, going from blue to pure red (passing by cyan, green, yellow, orange\u2026). Lux : Unit of the international system representing illumination (the amount of light received by a surface, which can be described as surface brightness), symbol: lx . 1 lux = 1 lm/m\u00b2 , 1 lux is equivalent to one lumen* per square meter. For example, a typical office lighting environment is between 300 and 500 lux , a cloudless sunset or sunrise around 400 lux , an overcast day at 1000 lux, a sunny day between 10,000 and 25,000 lux , with areas in full sunlight ranging from 32,000 to 100,000 lux . Black : The black is the color resulting from the absence of light. It is thus the color of weakest luminosity (null) and its saturation* cannot be defined. Standard (video and audio) : defines the way in which the audio and video data are encoded, the standard used, and therefore with which codec these data can be decoded. OCIO : OpenColorIO is a software library (a \u201cbrick\u201d/developer tool usable in other software/applications) dedicated to color management. It is free and open source, and has become standard with its inclusion and use in a large number of applications (natively on Maya, Arnold, Krita, Blender, Nuke \u2026 ; via plugins and add-ons on After Effects \u2026 ; via LUTs*/Colorimetric profiles on Photoshop and others). This inclusion in different applications allows to share a single configuration of color management throughout the production pipeline and ensure identical color reproduction at all stages. Byte : 8 bits . Not\u00e9 B . OpenColorIO : cf . OCIO OIIO : OpenImageIO mainly defines the standard for the openEXR image data storage format, and is a free and open source software library to handle this open format (and others), chosen as the default format by ACES*; it is interdependent with OCIO*. OpenEXR : Open and extremely versatile file format dedicated to images, and used in most production pipelines. Cf . OIIO . OpenImageIO : cf . OIIO Photon : Although the word dates from 1926, it was in 1905 that Albert Einstein theorized the photon as the quantum of light: the smallest indivisible quantity of light (of all electromagnetic waves in reality). The photon can be seen as the particular counterpart of the radius of light which would be the ondulatory counterpart. It is in reality both particle and wave. The intensity of light (and thus of colors as we perceive them) can be expressed in number of photons received by a given surface in a given time (thus for example in photons per square centimeter per second \u03b3/cm\u00b2/s ). We also measure the threshold of triggering (the lower level) and saturation (the upper level) of photo-sensitive sensors (such as retinal cells) in photons per second. White Point : cf . Illuminant Purple : The purples are the colors resulting from the mixture (only) of the two extremes of the spectrum of the visible monochromatic* lights: red and blue, in proportions which can vary. They are considered as saturated* colors. (Colors) Primaries : Colors chosen in a colorimetric space as basic colors for the representation, defining the gamut . Their mixture must make it possible to obtain white (in an additive system). In general, in digital imaging, a red, green and blue tint. Purity : cf . Saturation Colorimetric Purity : Ratio between the luminance of the monochromatic component of a light and the luminance of the total light. Purity of excitement : ratio between the amount of white and saturated color (monochromatic) in a light. Saturation : The most saturated colors possible are the monochromatic colors to which we add the purples (mixtures of red and blue). As soon as the colors are a mixture of several monochromatic * lights (except mixture of the blue and red extremes), the saturation decreases until the color becomes gray or white. The saturated colors go from black to the colors of the rainbow. We also speak of purity of light. cf . Colorimetric purity and Excitation purity . Tint : Color of the dominant wavelength of a light (of the pure equivalent of a given light), and thus the color of the pure or saturated equivalent of the light. We usually locate the colors on a circle by considering that the primary colors are equally spaced, with the primary red at 0 \u00b0, the primary green at 120 \u00b0, the primary blue at 240 \u00b0, and the mixtures, at intermediate angles, in proportion to the two primaries that compose them. Temperature of whites : Comparison of a \u201cwhite\u201d light to the color of a black body (a body that only emits light, and whose color is a direct result of its temperature, such as the sun, an ember, a flame, molten metal \u2026). It is a way to define precisely the hue of a white, which is then expressed in temperature, using the Kelvin as unit. The yellow-orange of sunlight is around 5800 K, the blue of an electric flash at 9000 K, the orange of a candle around 1850 K, etc. See the chapter Back to whites: temperature Violet : see Purplee . YCbCr or YCC : digital version of the YUV .. YPbPr : alternate name of YUV . YUV : System of coding of the colors in three channels: one of luminance or luminosity and two of chrominance . Several notations bring details on the system, although one uses very majority the term YUV : When a prime is added to the Y, it is specified as luminosity (with a gamma) and not luminance (linear). The correct terms in analog are: YUV or YPbPr with the luminance and Y\u2019UV or Y\u2019PbPr with the luminosity . The correct terms in digital are: YCbCr or YCC with the luminance and Y\u2019CbCr or Y\u2019CC with the luminosity . The lumen measures the general intensity (so in all directions) unlike the candela, but is not used in computing. \u21a9","title":"Glossary"},{"location":"ZZ-vocabulaire.html#glossary","text":"Alphabetical listing. ACES : Academy Color Encoding System is a standard of exchange, of color management for digital imaging. It aims to simplify color management by maintaining color fidelity throughout the production pipeline. The standard is free and open source and many companies contribute to its development. It uses in particular the OCIO * software library. Alpha : a fourth channel is sometimes added to the pixels of a video to store transparency information in addition to color. The RGBA and YUVA channels are usually noted in this case, and in the case of YUVA a fourth value is sometimes added to the chroma subsampling acronym: 4:4:4:4 , 4:2:2:4*, etc. Bit : The basic unit of both binary computer calculation and storage. A bit is either 0 or 1. 8 bits make up a byte , a sequence of eight 0s or 1s. Not to be confused with the Byte . Noted b (whereas Byte is noted B and byte is noted o ). Byte : smallest unit of usable memory on a given system (pronounced /ba\u026at/); usually made up of 8 bits and (only in this case but not necessarily) equivalent to one byte Candela : Unit of the international system representing a luminous intensity, i.e. a quantity of light emitted by a light source in a given direction (whereas the lumen* represents this quantity of light in total, in all directions). Symbol : cd . Cf . Intensity . CIE : La Commission internationale de l\u2019\u00e9clairage is an international organization dedicated to light, lighting, color and color spaces. It was founded in Berlin in 1913 and is currently based in Vienna, Austria. Note: the French acronym CIE is the one used internationally although in English it is the International Commission on Illumination . Codec : abbreviation for encoder-decoder. Software used to encode and decode a video or audio stream into a certain standard* . By the way, the codec is often confused with the standard or format , but it is indeed software and not a format . For example, x264 or nvenc are codecs allowing to encode a video in the standard h.264 (in format mp4 for example). (Colors) complementary : set of colors that, when mixed, cancel the perception of color, producing a neutral gray. Two complementary colors are diametrically opposed on the color wheel. In the chromaticity diagram CIE XYZ , the points that represent them are aligned on either side of the white point. Container : synonym for file type . Defines how multimedia streams (audio, video, subtitles, metadata\u2026) should be saved together in a specific file. For example: Quicktime ( *.mov ), MP4 ( *.mp4 ), Matroska ( *.mkv ) are containers (but h.264 is a standard , and x264 is a codec* ). Some containers are specialized and impose a certain standard (e.g., a MP4 should always use the h.264 or h.265 standard ), while others allow a large number of different standards (e.g., Quicktime allows PNG , Prores , RLE/Animation , MJPEG*, etc.). Contrast : difference in value between the most intense and the least intense point in an image. Channel (color) : AIn a pixel, the color is described by several values; each value is a channel of the pixel (or image). In RGB the three channels are red, green, and blue, in YUV , the luminance and two channels of chrominance . Transfer curve : mathematical function used to convert the intensities of a given color space to a linear scale (and vice versa). It is also called gamma even if the name gamma should rather be limited to only those functions that consist of a mathematical function of power , while transfer functions can take more varied and complex forms; they can however all have a relatively accurate approximation in a simple gamma . see also gamma and the chapter entitled Transfer curves, linear space and gamma . Proofing or Softproofing: Simulation of color conversion in a specific space which will be that of the final export (followed by a conversion towards the space of the screen for a correct display). This technique makes it possible to check that the successive conversions of the colors do not degrade the image too much and that the new colors are close to what one seeks, by checking what will see the final spectator. It is important to note that softproofing can remain far from reality if the space of exit is larger than that of the screen or too different (for example CMJN on a screen sRGB or Rec.2020 on a screen sRGB ); it makes it possible however to preview what a user will see on a similar screen. Format : synonym for file type . See container . Not to be confused with standard and codec . Frequency : in the ondulatory representation of the light (and of all the range of electromagnetic waves), the frequency, measured in Hertz ( Hz ) is the number of undulations per second; it is the inverse of the wavelength* ( F = 1 / \u03bb with F for the frequency and \u03bb for the wavelength): when the frequency increases, the wavelength decreases. In the visible part of the electromagnetic waves (the light), towards 10 15 Hz ( 1000 TeraHertz ), each frequency corresponds to a precise color, a monochromatic light. Gamma : Mathematical function used as a transfer curve and for colorimetric retouching, consisting of a power; the gamma value is that of the power (or the inverse of the power for inverse retouching): x 1/2.4 for a gamma 2.4, for example. see also transfer curve and the chapter titled Transfer Curves, Linear Space, and Gamma . Gamut : extent of colors that can be represented by a color space, represented by a subsurface in the chromaticity diagram CIE XYZ , in triangle when the space uses three primary. The area then represents the gamut ; we also speak of the \u201cwidth\u201d of the gamut : the more colors the space contains, the more its gamut is \u201cwide\u201d. (Colors) Homochromes : cf. Metamers Illuminant : coordinates (in the CIE XYZ ) of the point representing conventionally the white in a given color space. It can also be referred to by a standardized name, such as D65 or D60 , or by a blackbody temperature, in Kelvin, 6500 K for example. See also White Temperature . Intensity : quantity of light received by a surface (reflecting or a sensor) or emitted by a light source, measured in number of photons received by a given surface in a given time (for example in photons per second per square centimeter). In the case of a light emitter (candle, lamp, screen), we speak in terms of a cone (a solid angle) of emission and not a receiving surface and the unit is the candela * noted cd 1 . Linear : graphic representation of a proportional function, a mathematical function known as affine , represented by a straight line. The term linear is used to speak about color spaces whose correspondences of values with the physical intensity are proportional and thus represented by a straight line on a graph. Wavelength : in the ondulatory representation of the light (and of all the range of electromagnetic waves), the wavelength, measured in nanometers ( nm ) is the \u201csize\u201d of the undulations ; it is the reverse of the frequency* ( \u03bb = 1 / F with F for the frequency and \u03bb for the wavelength) : when the frequency increases, the wavelength decreases. In the visible part of the electromagnetic waves (the light thus), in 400nm and 700nm approximately, each wavelength corresponds to a precise color, a *monochromatic* light. Luminance : representation of the physical intensity of light (or color) corresponding to a color on a linear scale. Different from luminosity or luma . Luminosity or Luma : representation of the intensity of the light (or the color), but on a scale adapted to the human perception, using a gamma (cf Transfer curve ), contrary to the luminance . LUT : from Lookup Table . A LUT allows in computer science (especially in its early days) to replace complex mathematical functions (and especially time-consuming to realize) by tables of values; instead of calculating correspondences and conversions, one finds the value in the table. They are still used a lot in colorimetry: they allow to convert colors between different color spaces without worrying about the corresponding mathematical formula (and are thus an easy way to make a conversion between spaces in a software which does not manage them natively). They can cause a (small) loss of information if they are not precise enough (do not contain enough values). They are also used as calibration or colorimetry presets and can allow to easily replicate an effect, even in a software that would not allow it with its native color tools (as long as it can apply a LUT anyway). See the section entitled The LUTs for more details. (Colors) Metamers : two (or more) colors are said metamers when they are of identical appearance (the eye and the brain do not make the difference) although they are composed in reality of a different mixture of monochromatic rays *. Lumen : Unit of the international system representing luminous flux, symbol: lm . Measured by the total amount of light emitted by a light source in a given time. (Light) Monochromatic : pure light, composed of rays of a single precise frequency , without mixing frequencies* , like the light generated by a laser . The color of such a light is part of the spectrum of visible electro-magnetic waves, of the rainbow, going from blue to pure red (passing by cyan, green, yellow, orange\u2026). Lux : Unit of the international system representing illumination (the amount of light received by a surface, which can be described as surface brightness), symbol: lx . 1 lux = 1 lm/m\u00b2 , 1 lux is equivalent to one lumen* per square meter. For example, a typical office lighting environment is between 300 and 500 lux , a cloudless sunset or sunrise around 400 lux , an overcast day at 1000 lux, a sunny day between 10,000 and 25,000 lux , with areas in full sunlight ranging from 32,000 to 100,000 lux . Black : The black is the color resulting from the absence of light. It is thus the color of weakest luminosity (null) and its saturation* cannot be defined. Standard (video and audio) : defines the way in which the audio and video data are encoded, the standard used, and therefore with which codec these data can be decoded. OCIO : OpenColorIO is a software library (a \u201cbrick\u201d/developer tool usable in other software/applications) dedicated to color management. It is free and open source, and has become standard with its inclusion and use in a large number of applications (natively on Maya, Arnold, Krita, Blender, Nuke \u2026 ; via plugins and add-ons on After Effects \u2026 ; via LUTs*/Colorimetric profiles on Photoshop and others). This inclusion in different applications allows to share a single configuration of color management throughout the production pipeline and ensure identical color reproduction at all stages. Byte : 8 bits . Not\u00e9 B . OpenColorIO : cf . OCIO OIIO : OpenImageIO mainly defines the standard for the openEXR image data storage format, and is a free and open source software library to handle this open format (and others), chosen as the default format by ACES*; it is interdependent with OCIO*. OpenEXR : Open and extremely versatile file format dedicated to images, and used in most production pipelines. Cf . OIIO . OpenImageIO : cf . OIIO Photon : Although the word dates from 1926, it was in 1905 that Albert Einstein theorized the photon as the quantum of light: the smallest indivisible quantity of light (of all electromagnetic waves in reality). The photon can be seen as the particular counterpart of the radius of light which would be the ondulatory counterpart. It is in reality both particle and wave. The intensity of light (and thus of colors as we perceive them) can be expressed in number of photons received by a given surface in a given time (thus for example in photons per square centimeter per second \u03b3/cm\u00b2/s ). We also measure the threshold of triggering (the lower level) and saturation (the upper level) of photo-sensitive sensors (such as retinal cells) in photons per second. White Point : cf . Illuminant Purple : The purples are the colors resulting from the mixture (only) of the two extremes of the spectrum of the visible monochromatic* lights: red and blue, in proportions which can vary. They are considered as saturated* colors. (Colors) Primaries : Colors chosen in a colorimetric space as basic colors for the representation, defining the gamut . Their mixture must make it possible to obtain white (in an additive system). In general, in digital imaging, a red, green and blue tint. Purity : cf . Saturation Colorimetric Purity : Ratio between the luminance of the monochromatic component of a light and the luminance of the total light. Purity of excitement : ratio between the amount of white and saturated color (monochromatic) in a light. Saturation : The most saturated colors possible are the monochromatic colors to which we add the purples (mixtures of red and blue). As soon as the colors are a mixture of several monochromatic * lights (except mixture of the blue and red extremes), the saturation decreases until the color becomes gray or white. The saturated colors go from black to the colors of the rainbow. We also speak of purity of light. cf . Colorimetric purity and Excitation purity . Tint : Color of the dominant wavelength of a light (of the pure equivalent of a given light), and thus the color of the pure or saturated equivalent of the light. We usually locate the colors on a circle by considering that the primary colors are equally spaced, with the primary red at 0 \u00b0, the primary green at 120 \u00b0, the primary blue at 240 \u00b0, and the mixtures, at intermediate angles, in proportion to the two primaries that compose them. Temperature of whites : Comparison of a \u201cwhite\u201d light to the color of a black body (a body that only emits light, and whose color is a direct result of its temperature, such as the sun, an ember, a flame, molten metal \u2026). It is a way to define precisely the hue of a white, which is then expressed in temperature, using the Kelvin as unit. The yellow-orange of sunlight is around 5800 K, the blue of an electric flash at 9000 K, the orange of a candle around 1850 K, etc. See the chapter Back to whites: temperature Violet : see Purplee . YCbCr or YCC : digital version of the YUV .. YPbPr : alternate name of YUV . YUV : System of coding of the colors in three channels: one of luminance or luminosity and two of chrominance . Several notations bring details on the system, although one uses very majority the term YUV : When a prime is added to the Y, it is specified as luminosity (with a gamma) and not luminance (linear). The correct terms in analog are: YUV or YPbPr with the luminance and Y\u2019UV or Y\u2019PbPr with the luminosity . The correct terms in digital are: YCbCr or YCC with the luminance and Y\u2019CbCr or Y\u2019CC with the luminosity . The lumen measures the general intensity (so in all directions) unlike the candela, but is not used in computing. \u21a9","title":"Glossary"},{"location":"ae.html","text":"II.G - Color management: Adobe After Effects \u00b6 After Effects is one of the last software to not integrate OpenColorIO * ( cf . I.N - OpenColorIO and ACES ) natively for color management. However, the color management in After Effects is quite simple to set up, and it handles a wide range of color spaces, including ACEScg * , which may make it possible to do without OCIO . However, it is possible to use OCIO anyway via a dedicated plug-in, and thus take advantage of a unique configuration on the whole production pipeline of which After Effects would be part. Natively, After Effects can be complicated to integrate into a production pipeline: Footage interpretation, and color management in general, cannot be changed via a script. The default interpretation for each type of footage can be changed in a configuration text file, but it will have to be copied to all workstations. After Effects does not offer the ACEScc workspace for color correction. The output space is set by default to the workspace. These shortcomings make it impractical when working in a team with a precise and automated workflow. These problems can be overcome by using the OCIO plug-in, which, being an effect, can be scripted. For a freelancer or a small team, the management proposed by After Effects can however be sufficient. II.G - Color management: Adobe After Effects G.1 - Project Settings - Workspace G.2 - Footage Interpretation - Input spaces G.3 - View Options - Display Space and Simulations (Soft-proofing) G.4 - Export Options - Output Spaces G.5 - OCIO G.5.a - Introduction G.5.b - Install the OCIO plug-in G.5.c - Disable color management in After Effects G.5.d - Management G.5.e - Input and workspace G.5.f - Output G.5.g - Display G.1 - Project Settings - Workspace \u00b6 The workspace is set per project, in the project settings. This is where we set the color depth to be used with the workspace; unless you are working on low performance hardware, there is no reason to choose anything other than 32 bpc , especially if you enable color management. In any case, 16 bpc is essential to obtain correct colors. After Effects offers a long list of color spaces, and mixes both real workspaces and output/display and camera spaces not to be used as workspace . As with all software, it is advisable to choose a wide gamut space, such as Adobe RGB or ACEScg . Unfortunately, After Effects does not offer ACEScc for color correction natively, and you will have to use the OCIO plug-in or LUT * to use it. A check box allows you to linearize the workspaces which are not linear (like Adobe RGB for example) in order to better generate the colors. In the case of color correction work, common on After Effects , a non-linear space should be chosen and this box should be left unchecked. G.2 - Footage Interpretation - Input spaces \u00b6 The input space is defined on each footage, in the interpretation , accessible via a right click on the footage. It is necessary to check for each type of imported footage that After Effects selects the right space, or specify it manually via this dialog box. For example, when importing EXR files from a 3D renderer, After Effects considers them to be RGB linear by default; when working in ACEScg , you will have to select this space at this point for each imported EXR sequence. You can change the default interpretation for each file type by modifying the configuration text file named interpretation rules.txt , itself located in the folder containing the After Effects settings 1 . Once the file is modified, you have to restart After Effects for the changes to take effect. Warning If the color space of the imported files is different from the default one for the type of file in question, don\u2019t forget to select the right space at each import, or modify the default settings file. G.3 - View Options - Display Space and Simulations (Soft-proofing) \u00b6 The transformation from the workspace to the display space is set in the View ( Display ) menu. By simply checking the Use display color managament box, After Effects automatically converts the colors to the display space of the screen as specified by the operating system. The Simulate output submenu allows you to test output color spaces without having to render: an output space is automatically applied to the image, which is then converted back to the display space. This option can be useful for testing the result of a specific output and checking that the various conversions do not degrade the image, and to be certain of the colors that will be generated when output in a specific space, even if most of the time this simulation can be left disabled. You can then manually configure a simulation. The first color space option allows conversion to the output space of the image. The second option simulates a conversion or interpretation that a video player would make, and allows you to have a preview of what would happen in different scenarios. The Preserve RGB box is usually left checked, which disables any conversion on the image as it is output. The third part, which cannot be modified, recalls the color space of the screen and the final transformation applied to the image for display. G.4 - Export Options - Output Spaces \u00b6 Don\u2019t forget to apply the conversion to the output space when exporting. These options are found in the output module settings of the render queue. By default, After Effects sets the output to the workspace, which is not logical at all, as the workspace is rarely the output space! It is then necessary to specify the right space for each output\u2026 See II-B Some standards for files . Warning After Effects applies by default the workspace to all files\u2026 You have to think about changing it systematically or create preconfigurations that are correctly set. G.5 - OCIO \u00b6 G.5.a - Introduction \u00b6 Using OCIO on After Effects allows you to bypass many of the shortcomings and problems raised by native color management, but the use of the plug-in requires some precise organization. The ideal is to create a script to automate these tasks in the production pipeline. The first thing to do (after installing the plug-in) is to deactivate the color management of After Effects to be able to control everything via OCIO . G.5.b - Install the OCIO plug-in \u00b6 The updated plug-in is available here . Although the article dates from 2012, it is updated with each new version of OCIO . After downloading the .zip archive, you just have to copy the OpenColorIO.aex file in the plug-ins folder of After Effects , then restart the application. OpenColorIO is an effect available in the Utility section of the effects. G.5.c - Disable color management in After Effects \u00b6 Once the plug-in is installed, you have to take over the color management in After Effects to control it via the effect. Warning Contrary to what you might think, you must start by choosing a color space in the project settings, in order to then have access to the other color management options and be in a linear space\u2026 In the project parameters, choose a default space. sRGB does the trick, but you must check the box to linearize it and use 32 bpc . When importing all the footage, you must check Preserve RGB in the interpretation so that After Effects does not perform any conversion. It is possible to modify the configuration text file named interpretation rules.txt , itself located in the folder containing the settings of After Effects 1 in order to interpret all the footage by default in Preserve RGB mode. For each composition you must disable the display transformation by unchecking Use Display Color Management in the View ( Display ) menu. Finally, for all outputs, you must check the Preserve RGB box in the color parameters of the output module, so that After Effects does not perform any conversion. All these operations ensure that it is indeed OCIO that will take care of all the color conversions. G.5.d - Management \u00b6 Since OCIO for After Effects is an effect, its use requires some organization of the project to avoid mistakes. When importingf footage, it will be much more practical to systematically place it in a precomposition, and to use this precomposition instead of the footage itself in the other compositions. It is in this precomposition that the interpretation of the footage and the conversion to the workspace will be set up. G.5.e - Input and workspace \u00b6 In the precomposition of each footage, the OpenColorIO effect must be put on the layer of the footage. After selecting the correct OCIO configuration, you can choose the space corresponding to the footage in the first field (in this example, RGB linear , for a EXR sequence for example). In the second field, you choose the desired workspace for the project ( ACEScg in this example). This second field will be common to all footage, while the first field depends on the format of the imported file. G.5.f - Output \u00b6 The conversion for output is simply done via an effect layer with the OpenColorIO effect which converts the colors from the workspace to the output space. Choose the workspace in the input space field and the output space corresponding to the desired file in output space . G.5.g - Display \u00b6 To perform the display conversion, we use an adjustment layer at the top of the compositions in which we work. You can set this layer to guide layer mode to make sure that it is not active when rendering. We add the OpenColorIO effect on this layer. Two solutions are possible: Either we convert from the workspace to the display; this is the easiest course if we have several different outputs to do. In this case, the output layer is disabled while working, and you must remember to re-enable it to perform the rendering. Check the Display box, select the workspace first (field input space ), then the display space on the next field(s). Or we convert from the desired output space (see previous point), if we use only one. In this case the output layer is left active during the work. Check the Display button, select the output space first (field input space ), then the display space on the next field(s). This folder is easily found via the Reveal Preferences button in the After Effects general preferences panel \u21a9 \u21a9","title":"G - Adobe After Effects"},{"location":"ae.html#iig-color-management-adobe-after-effects","text":"After Effects is one of the last software to not integrate OpenColorIO * ( cf . I.N - OpenColorIO and ACES ) natively for color management. However, the color management in After Effects is quite simple to set up, and it handles a wide range of color spaces, including ACEScg * , which may make it possible to do without OCIO . However, it is possible to use OCIO anyway via a dedicated plug-in, and thus take advantage of a unique configuration on the whole production pipeline of which After Effects would be part. Natively, After Effects can be complicated to integrate into a production pipeline: Footage interpretation, and color management in general, cannot be changed via a script. The default interpretation for each type of footage can be changed in a configuration text file, but it will have to be copied to all workstations. After Effects does not offer the ACEScc workspace for color correction. The output space is set by default to the workspace. These shortcomings make it impractical when working in a team with a precise and automated workflow. These problems can be overcome by using the OCIO plug-in, which, being an effect, can be scripted. For a freelancer or a small team, the management proposed by After Effects can however be sufficient. II.G - Color management: Adobe After Effects G.1 - Project Settings - Workspace G.2 - Footage Interpretation - Input spaces G.3 - View Options - Display Space and Simulations (Soft-proofing) G.4 - Export Options - Output Spaces G.5 - OCIO G.5.a - Introduction G.5.b - Install the OCIO plug-in G.5.c - Disable color management in After Effects G.5.d - Management G.5.e - Input and workspace G.5.f - Output G.5.g - Display","title":"II.G - Color management: Adobe After Effects"},{"location":"ae.html#g1-project-settings-workspace","text":"The workspace is set per project, in the project settings. This is where we set the color depth to be used with the workspace; unless you are working on low performance hardware, there is no reason to choose anything other than 32 bpc , especially if you enable color management. In any case, 16 bpc is essential to obtain correct colors. After Effects offers a long list of color spaces, and mixes both real workspaces and output/display and camera spaces not to be used as workspace . As with all software, it is advisable to choose a wide gamut space, such as Adobe RGB or ACEScg . Unfortunately, After Effects does not offer ACEScc for color correction natively, and you will have to use the OCIO plug-in or LUT * to use it. A check box allows you to linearize the workspaces which are not linear (like Adobe RGB for example) in order to better generate the colors. In the case of color correction work, common on After Effects , a non-linear space should be chosen and this box should be left unchecked.","title":"G.1 - Project Settings - Workspace"},{"location":"ae.html#g2-footage-interpretation-input-spaces","text":"The input space is defined on each footage, in the interpretation , accessible via a right click on the footage. It is necessary to check for each type of imported footage that After Effects selects the right space, or specify it manually via this dialog box. For example, when importing EXR files from a 3D renderer, After Effects considers them to be RGB linear by default; when working in ACEScg , you will have to select this space at this point for each imported EXR sequence. You can change the default interpretation for each file type by modifying the configuration text file named interpretation rules.txt , itself located in the folder containing the After Effects settings 1 . Once the file is modified, you have to restart After Effects for the changes to take effect. Warning If the color space of the imported files is different from the default one for the type of file in question, don\u2019t forget to select the right space at each import, or modify the default settings file.","title":"G.2 - Footage Interpretation - Input spaces"},{"location":"ae.html#g3-view-options-display-space-and-simulations-soft-proofing","text":"The transformation from the workspace to the display space is set in the View ( Display ) menu. By simply checking the Use display color managament box, After Effects automatically converts the colors to the display space of the screen as specified by the operating system. The Simulate output submenu allows you to test output color spaces without having to render: an output space is automatically applied to the image, which is then converted back to the display space. This option can be useful for testing the result of a specific output and checking that the various conversions do not degrade the image, and to be certain of the colors that will be generated when output in a specific space, even if most of the time this simulation can be left disabled. You can then manually configure a simulation. The first color space option allows conversion to the output space of the image. The second option simulates a conversion or interpretation that a video player would make, and allows you to have a preview of what would happen in different scenarios. The Preserve RGB box is usually left checked, which disables any conversion on the image as it is output. The third part, which cannot be modified, recalls the color space of the screen and the final transformation applied to the image for display.","title":"G.3 - View Options - Display Space and Simulations (Soft-proofing)"},{"location":"ae.html#g4-export-options-output-spaces","text":"Don\u2019t forget to apply the conversion to the output space when exporting. These options are found in the output module settings of the render queue. By default, After Effects sets the output to the workspace, which is not logical at all, as the workspace is rarely the output space! It is then necessary to specify the right space for each output\u2026 See II-B Some standards for files . Warning After Effects applies by default the workspace to all files\u2026 You have to think about changing it systematically or create preconfigurations that are correctly set.","title":"G.4 - Export Options - Output Spaces"},{"location":"ae.html#g5-ocio","text":"","title":"G.5 - OCIO"},{"location":"ae.html#g5a-introduction","text":"Using OCIO on After Effects allows you to bypass many of the shortcomings and problems raised by native color management, but the use of the plug-in requires some precise organization. The ideal is to create a script to automate these tasks in the production pipeline. The first thing to do (after installing the plug-in) is to deactivate the color management of After Effects to be able to control everything via OCIO .","title":"G.5.a - Introduction"},{"location":"ae.html#g5b-install-the-ocio-plug-in","text":"The updated plug-in is available here . Although the article dates from 2012, it is updated with each new version of OCIO . After downloading the .zip archive, you just have to copy the OpenColorIO.aex file in the plug-ins folder of After Effects , then restart the application. OpenColorIO is an effect available in the Utility section of the effects.","title":"G.5.b - Install the OCIO plug-in"},{"location":"ae.html#g5c-disable-color-management-in-after-effects","text":"Once the plug-in is installed, you have to take over the color management in After Effects to control it via the effect. Warning Contrary to what you might think, you must start by choosing a color space in the project settings, in order to then have access to the other color management options and be in a linear space\u2026 In the project parameters, choose a default space. sRGB does the trick, but you must check the box to linearize it and use 32 bpc . When importing all the footage, you must check Preserve RGB in the interpretation so that After Effects does not perform any conversion. It is possible to modify the configuration text file named interpretation rules.txt , itself located in the folder containing the settings of After Effects 1 in order to interpret all the footage by default in Preserve RGB mode. For each composition you must disable the display transformation by unchecking Use Display Color Management in the View ( Display ) menu. Finally, for all outputs, you must check the Preserve RGB box in the color parameters of the output module, so that After Effects does not perform any conversion. All these operations ensure that it is indeed OCIO that will take care of all the color conversions.","title":"G.5.c - Disable color management in After Effects"},{"location":"ae.html#g5d-management","text":"Since OCIO for After Effects is an effect, its use requires some organization of the project to avoid mistakes. When importingf footage, it will be much more practical to systematically place it in a precomposition, and to use this precomposition instead of the footage itself in the other compositions. It is in this precomposition that the interpretation of the footage and the conversion to the workspace will be set up.","title":"G.5.d - Management"},{"location":"ae.html#g5e-input-and-workspace","text":"In the precomposition of each footage, the OpenColorIO effect must be put on the layer of the footage. After selecting the correct OCIO configuration, you can choose the space corresponding to the footage in the first field (in this example, RGB linear , for a EXR sequence for example). In the second field, you choose the desired workspace for the project ( ACEScg in this example). This second field will be common to all footage, while the first field depends on the format of the imported file.","title":"G.5.e - Input and workspace"},{"location":"ae.html#g5f-output","text":"The conversion for output is simply done via an effect layer with the OpenColorIO effect which converts the colors from the workspace to the output space. Choose the workspace in the input space field and the output space corresponding to the desired file in output space .","title":"G.5.f - Output"},{"location":"ae.html#g5g-display","text":"To perform the display conversion, we use an adjustment layer at the top of the compositions in which we work. You can set this layer to guide layer mode to make sure that it is not active when rendering. We add the OpenColorIO effect on this layer. Two solutions are possible: Either we convert from the workspace to the display; this is the easiest course if we have several different outputs to do. In this case, the output layer is disabled while working, and you must remember to re-enable it to perform the rendering. Check the Display box, select the workspace first (field input space ), then the display space on the next field(s). Or we convert from the desired output space (see previous point), if we use only one. In this case the output layer is left active during the work. Check the Display button, select the output space first (field input space ), then the display space on the next field(s). This folder is easily found via the Reveal Preferences button in the After Effects general preferences panel \u21a9 \u21a9","title":"G.5.g - Display"},{"location":"blender.html","text":"II.K - Gestion des couleurs : BlenderColor management: Blender \u00b6 Blender use natively OpenColorIO * ( cf . I.N - OpenColorIO et ACES ) for color management, and the parameters are very simple to set up. An excellent default OCIO configuration comes with Blender , which includes spaces for importing image and video files, exporting in several formats, and a Filmic workspace that works very well for 3D rendering. However, we can easily use a configuration with ACES if we want to take advantage of it or integrate Blender to a production pipeline in ACES , the ACES workspace being slightly different from Filmic . II.K - Gestion des couleurs : BlenderColor management: Blender K.1 - Default configuration: Filmic K.1.a - Rendering (scene referred) and display K.1.b - Output K.1.c - Input (textures) K.2 - OCIO pipeline K.2.a - Changing the OCIO configuration K.3 - Using ACES K.3.a - Rendering (scene referred) K.3.b - Input (textures) K.1 - Default configuration: Filmic \u00b6 a. Render without color management, with sRGB standard . workspace b. Color managed render in the Filmic . workspace. With the default configuration and the Filmic space for rendering ( scene referred ), here are the few color settings in Blender. K.1.a - Rendering (scene referred) and display \u00b6 These settings are located in the rendering settings, section Color Management . Note With OpenEXR output, only the raw (linear) data is saved. For other formats***, the settings are applied during the conversion to the color space of the output format. It is important to note that in a color managed workflow, and using the OpenEXR * format as an intermediate format (and therefore for saving at the output of Blender ), all these options only influence the display of the rendering in Blender but not the data saved in the OpenEXR file ! So you have to re-apply the same settings in the following applications of the chain (compositing applications like Nuke , After Effects , etc. for example). If the other applications do not use OpenColorIO * (and we cannot therefore give the OCIO configuration of Blender ), we can however find these settings via LUT * exported from the OCIO configuration of Blender . Cf . I.2 - Manufacturing chain OCIO . However, this is not the case when images are saved in other formats ( PNG , video, etc.), in which case Blender applies all the settings to the image which, unlike OpenEXR , is saved in a specific color space ( sRGB for images, Rec.709 for videos, etc.). Display Device : choose the display space. In general, leave on sRGB . View Transform : this is the workspace setting (scene referred) that transforms the raw linear data from the renderer to the display. Filmic ( fig. a ) : This is the default space, very efficient to soften the highlights that burn easily and keep details in the dark areas. It actually simulates the printing of images on film and allows a fairly fine and detailed rendering. Standard ( fig. b ) : Displays the data in the display space; this option allows you to see how an application would display the image without applying the Filmic space. It is of little use, as highlights become burned too easily, except possibly when using the curve tools below. Filmic Log : a logarithmic version of Filmic , mainly intended to perform important color adjustments in compositing. Raw : displays the raw data without transformation, as they are recorded in OpenEXR files if this is the format chosen for output. False Color : a contrasted display in various colors to be able to analyze the image content in detail. Look : applies in addition to the workspace ( View Transform ) an additional adjustment according to a purely artistic and creative choice. Each of the options is a choice of image contrast. Medium and None* are exactly the same options. Exposure et Gamma : change the exposure and apply a possible gamma * correction to the image. It is better not to change these settings if the workflow involves other applications after the Blender output and the OpenEXR format is used. They can however be useful to simply retouch the contrast of the output image, if you export in a final image or video format other than EXR . Since these settings affect the workspace, they are applied after the compositing nodes. Sequencer : settings of the sequencer workspace (the video editing module). Does not influence 3D images or compositing. Blender also offers an image adjustment tool via curves, which allows fine retouching of the image. This example pretty much replicates what the Filmic space does. If this tool can be useful to correct images coming out of Blender , it is not recommended if you work on a color-managed pipeline using the OpenEXR format: it would be too complicated to reproduce exactly the same effect of curves in the applications that follow Blender in the production pipeline. K.1.b - Output \u00b6 Blender does not display color options (except for some modes and depth) for image output; it uses the standard space of the output format ( sRGB for image formats, linear raw data in OpenEXR ) In OpenEXR the two depths * are : Float (Half) : 16 bpc float. Float (Full) : 32 bpc float. Depending on the compositing to be done in the following applications or the final formats, we may prefer Float (Full) keeping in mind that the files will sometimes be much larger and heavier in calculations. Cf . chapter I.K - Pixels formats for more information. Things to remember: In OpenEXR (and OpenEXR MultiLayer ): the raw and linear data are saved. It will be necessary to re-apply the same settings as those chosen in the rendering parameters and color management when using the files in other applications to find the same image. This is the format that allows the best control and quality. In the other image and video formats, it is the data converted via the workspace to the standard space of the file that is saved ( sRGB or Rec.709 in general). The images displayed as they are will therefore automatically be the same as in Blender (but you lose quality with the loss of linearity in particular, and of color depth ). K.1.c - Input (textures) \u00b6 When adding images to Blender , the Color Space selector allows you to specify the color space used by the file in question. Filmic Log : to be used (only) in case the file is an OpenEXR file previously exported or rendered by Blender with the Filmic Log workspace. Linear : for standard OpenEXR files (or other possible RGB linear formats). Linear ACES : for OpenEXR files rendered with the ACEScg workspace. Non-Color et Raw : the file does not contain an image but data (like a normal map , a displacement map , a metalness , etc.) sRGB : for all standard image and video files. XYZ : in case of a production pipeline in the XYZ space. Note The presence of the Linear ACES space allows you to import images that have used ACES , but Blender does not fully integrate into a production pipeline that uses ACES by default; indeed, it does not offer a ACES workspace. See . I.3 - Using ACES K.2 - OCIO pipeline \u00b6 Cf. II.D - Design a production pipeline with OpenColorIO . . The OCIO * configuration provided by default with Blender , Filmic , is available in the Blender configuration folder, datafiles/colormanagement/config.ocio . It is therefore possible to use this configuration throughout the production pipeline by pointing to this configuration in other applications, or by setting the OCIO environment variable to this file. To include applications not using OpenColorIO in the pipeline (like Adobe After Effects or Adobe Photoshop ), it\u2019s always possible to generate the necessary LUT * , especially the LUT that converts linear images (like in openEXR * ) to sRGB or other common spaces. We offer several of these LUT for download here , in different formats. You just have to apply the desired LUT to find the Filmic space of Blender in another application. K.2.a - Changing the OCIO configuration \u00b6 To use a configuration different from the default Filmic , Blender does not allow to select a configuration via the interface, but two solutions are possible. Using the environment variable It\u2019s possible to simply change the OCIO system environment variable to point to the path of the configuration to be used. See II.D - Designing a production line with OpenColorIO . By replacing the file in the Blender folder You can also delete the existing configuration in the Blender folder and copy the new configuration (the ocio.config file and its associated subfolders). Just remember to keep a copy of the original configuration! It is of course also possible to edit the ocio.config file of Blender yourself, for advanced users. K.3 - Using ACES \u00b6 It may be interesting to use the ACEScg workspace rather than the default Filmic of Blender for a slightly different look, or to integrate Blender into a production line using ACES . Here is a comparison of the results of the same rendering in different spaces: a. Render without color management, with the sRGB workspace standard . b. Render with color management, in the workspace Filmic ( \u201cMedium-Contrast\u201d setting). c. Render with color management, in the workspace ACEScg . As you can see on these images, ACES keeps the saturation of very bright colors better, and a little more contrast, while Filmic tends to dull the images a little. To use ACES * with Blender and take advantage of its powerful workspace, simply use the corresponding OCIO * configuration. However, the configuration provided by OCIO for ACES includes a long list of color spaces corresponding to various camera models, which is very useful in cinema for working on filmed images, but which complicates the task in animation and especially in Blender . The list of color spaces displayed in Blender with the original ACES configuration! We offer for download here a modified version of ACES specifically for animation, notably by limiting the number of color spaces included to those that are potentially really necessary in animation. This work done from Sony Pictures ImageWorks sources is available here on Github . The list of color spaces displayed in Blender with the configuration ACES adjusted by us. Here are the new settings available once this configuration is installed. K.3.a - Rendering (scene referred) \u00b6 Here are the settings for the rendering parameters, color management , once ACES * is available. The Display Device now contains only one option, ACES . This is not very standard in the way it works, but ACES is made like that\u2026 View Transform contains the display spaces, to which the ACEScg workspace is applied. There is no creative Look provided with ACES . The Looks of Filmic are not very useful anyway (they can easily be reproduced in compositing). Simply choose the display space corresponding to your screen (usually sRGB ). Raw displays the raw data without transformation, as they are saved in the OpenEXR files if this is the format chosen for output. Log is the ACES equivalent of Filmic Log , useful in case of strong color adjustement in compositing. Several spaces offer a variation of the white point * D60 instead of D65 . Indeed, D60 (about 6000 K ) is the white point of ACES and choosing one of these options allows to see the image without the white point* conversion. K.3.b - Input (textures) \u00b6 For the import of images, a large list of different color spaces is available. Here are the most useful and common ones. The ACES spaces are the usual ACES spaces: ACES2065-1 : color space for storage recommended by the ACES standard in OpenEXR files, but still quite little used. ACEScc and ACES - ACEScct : ACES color space dedicated to color correction, in OpenEXR files in general. It is unlikely that you will have to import this kind of file into Blender , except possibly an OpenEXR file from a color grading or compositing software. ACEScg : ACES workspace for rendering and compositing. In a pipeline using ACES , textures can be made in this space and imported into Blender via OpenEXR files. Input - Generic - sRGB - Texture et Utility - sRGB - Texture are identical. For standard sRGB image files ( PNG , TGA , etc). The Output spaces should not be used as input\u2026 The Role spaces are aliases for other spaces depending on their usage, and are not very useful here. The Utility spaces contain several important elements for importing images into Blender : Curve converts only the transfer curve of the imported files, but not the primaries. Linear convert only the primaries but not the transfer curve. Some are common and useful: Linear - sRGB is the default space for OpenEXR files. Linear - Rec.709 is perfectly identical to Linear - sRGB . Raw is the space to be used for files that do not contain images but other data (like normal map , displacement map , metalness , etc.) Rec.709 - Display is the space for standard HD videos (e.g. MP4 ) Rec.2020 - Display is a UHD (4K) video space In a nutshell : OpenEXR files containing images: ACES - ACEScg if the file comes from an application working in ACES , Utility - Linear - sRGB in other cases. OpenEXR files containing data other than color: Utility - Raw . Image files ( PNG , JPG , TGA*, etc.): Utility - sRGB - Texture **. Video files ( MP4*): Utility - Rec.709 - Texture in most cases, Utility - Rec.2020 - Display ** in case of UHD video. Sources et r\u00e9f\u00e9rences","title":"K - Blender"},{"location":"blender.html#iik-gestion-des-couleurs-blendercolor-management-blender","text":"Blender use natively OpenColorIO * ( cf . I.N - OpenColorIO et ACES ) for color management, and the parameters are very simple to set up. An excellent default OCIO configuration comes with Blender , which includes spaces for importing image and video files, exporting in several formats, and a Filmic workspace that works very well for 3D rendering. However, we can easily use a configuration with ACES if we want to take advantage of it or integrate Blender to a production pipeline in ACES , the ACES workspace being slightly different from Filmic . II.K - Gestion des couleurs : BlenderColor management: Blender K.1 - Default configuration: Filmic K.1.a - Rendering (scene referred) and display K.1.b - Output K.1.c - Input (textures) K.2 - OCIO pipeline K.2.a - Changing the OCIO configuration K.3 - Using ACES K.3.a - Rendering (scene referred) K.3.b - Input (textures)","title":"II.K - Gestion des couleurs : BlenderColor management: Blender"},{"location":"blender.html#k1-default-configuration-filmic","text":"a. Render without color management, with sRGB standard . workspace b. Color managed render in the Filmic . workspace. With the default configuration and the Filmic space for rendering ( scene referred ), here are the few color settings in Blender.","title":"K.1 - Default configuration: Filmic"},{"location":"blender.html#k1a-rendering-scene-referred-and-display","text":"These settings are located in the rendering settings, section Color Management . Note With OpenEXR output, only the raw (linear) data is saved. For other formats***, the settings are applied during the conversion to the color space of the output format. It is important to note that in a color managed workflow, and using the OpenEXR * format as an intermediate format (and therefore for saving at the output of Blender ), all these options only influence the display of the rendering in Blender but not the data saved in the OpenEXR file ! So you have to re-apply the same settings in the following applications of the chain (compositing applications like Nuke , After Effects , etc. for example). If the other applications do not use OpenColorIO * (and we cannot therefore give the OCIO configuration of Blender ), we can however find these settings via LUT * exported from the OCIO configuration of Blender . Cf . I.2 - Manufacturing chain OCIO . However, this is not the case when images are saved in other formats ( PNG , video, etc.), in which case Blender applies all the settings to the image which, unlike OpenEXR , is saved in a specific color space ( sRGB for images, Rec.709 for videos, etc.). Display Device : choose the display space. In general, leave on sRGB . View Transform : this is the workspace setting (scene referred) that transforms the raw linear data from the renderer to the display. Filmic ( fig. a ) : This is the default space, very efficient to soften the highlights that burn easily and keep details in the dark areas. It actually simulates the printing of images on film and allows a fairly fine and detailed rendering. Standard ( fig. b ) : Displays the data in the display space; this option allows you to see how an application would display the image without applying the Filmic space. It is of little use, as highlights become burned too easily, except possibly when using the curve tools below. Filmic Log : a logarithmic version of Filmic , mainly intended to perform important color adjustments in compositing. Raw : displays the raw data without transformation, as they are recorded in OpenEXR files if this is the format chosen for output. False Color : a contrasted display in various colors to be able to analyze the image content in detail. Look : applies in addition to the workspace ( View Transform ) an additional adjustment according to a purely artistic and creative choice. Each of the options is a choice of image contrast. Medium and None* are exactly the same options. Exposure et Gamma : change the exposure and apply a possible gamma * correction to the image. It is better not to change these settings if the workflow involves other applications after the Blender output and the OpenEXR format is used. They can however be useful to simply retouch the contrast of the output image, if you export in a final image or video format other than EXR . Since these settings affect the workspace, they are applied after the compositing nodes. Sequencer : settings of the sequencer workspace (the video editing module). Does not influence 3D images or compositing. Blender also offers an image adjustment tool via curves, which allows fine retouching of the image. This example pretty much replicates what the Filmic space does. If this tool can be useful to correct images coming out of Blender , it is not recommended if you work on a color-managed pipeline using the OpenEXR format: it would be too complicated to reproduce exactly the same effect of curves in the applications that follow Blender in the production pipeline.","title":"K.1.a - Rendering (scene referred) and display"},{"location":"blender.html#k1b-output","text":"Blender does not display color options (except for some modes and depth) for image output; it uses the standard space of the output format ( sRGB for image formats, linear raw data in OpenEXR ) In OpenEXR the two depths * are : Float (Half) : 16 bpc float. Float (Full) : 32 bpc float. Depending on the compositing to be done in the following applications or the final formats, we may prefer Float (Full) keeping in mind that the files will sometimes be much larger and heavier in calculations. Cf . chapter I.K - Pixels formats for more information. Things to remember: In OpenEXR (and OpenEXR MultiLayer ): the raw and linear data are saved. It will be necessary to re-apply the same settings as those chosen in the rendering parameters and color management when using the files in other applications to find the same image. This is the format that allows the best control and quality. In the other image and video formats, it is the data converted via the workspace to the standard space of the file that is saved ( sRGB or Rec.709 in general). The images displayed as they are will therefore automatically be the same as in Blender (but you lose quality with the loss of linearity in particular, and of color depth ).","title":"K.1.b - Output"},{"location":"blender.html#k1c-input-textures","text":"When adding images to Blender , the Color Space selector allows you to specify the color space used by the file in question. Filmic Log : to be used (only) in case the file is an OpenEXR file previously exported or rendered by Blender with the Filmic Log workspace. Linear : for standard OpenEXR files (or other possible RGB linear formats). Linear ACES : for OpenEXR files rendered with the ACEScg workspace. Non-Color et Raw : the file does not contain an image but data (like a normal map , a displacement map , a metalness , etc.) sRGB : for all standard image and video files. XYZ : in case of a production pipeline in the XYZ space. Note The presence of the Linear ACES space allows you to import images that have used ACES , but Blender does not fully integrate into a production pipeline that uses ACES by default; indeed, it does not offer a ACES workspace. See . I.3 - Using ACES","title":"K.1.c - Input (textures)"},{"location":"blender.html#k2-ocio-pipeline","text":"Cf. II.D - Design a production pipeline with OpenColorIO . . The OCIO * configuration provided by default with Blender , Filmic , is available in the Blender configuration folder, datafiles/colormanagement/config.ocio . It is therefore possible to use this configuration throughout the production pipeline by pointing to this configuration in other applications, or by setting the OCIO environment variable to this file. To include applications not using OpenColorIO in the pipeline (like Adobe After Effects or Adobe Photoshop ), it\u2019s always possible to generate the necessary LUT * , especially the LUT that converts linear images (like in openEXR * ) to sRGB or other common spaces. We offer several of these LUT for download here , in different formats. You just have to apply the desired LUT to find the Filmic space of Blender in another application.","title":"K.2 - OCIO pipeline"},{"location":"blender.html#k2a-changing-the-ocio-configuration","text":"To use a configuration different from the default Filmic , Blender does not allow to select a configuration via the interface, but two solutions are possible. Using the environment variable It\u2019s possible to simply change the OCIO system environment variable to point to the path of the configuration to be used. See II.D - Designing a production line with OpenColorIO . By replacing the file in the Blender folder You can also delete the existing configuration in the Blender folder and copy the new configuration (the ocio.config file and its associated subfolders). Just remember to keep a copy of the original configuration! It is of course also possible to edit the ocio.config file of Blender yourself, for advanced users.","title":"K.2.a - Changing the OCIO configuration"},{"location":"blender.html#k3-using-aces","text":"It may be interesting to use the ACEScg workspace rather than the default Filmic of Blender for a slightly different look, or to integrate Blender into a production line using ACES . Here is a comparison of the results of the same rendering in different spaces: a. Render without color management, with the sRGB workspace standard . b. Render with color management, in the workspace Filmic ( \u201cMedium-Contrast\u201d setting). c. Render with color management, in the workspace ACEScg . As you can see on these images, ACES keeps the saturation of very bright colors better, and a little more contrast, while Filmic tends to dull the images a little. To use ACES * with Blender and take advantage of its powerful workspace, simply use the corresponding OCIO * configuration. However, the configuration provided by OCIO for ACES includes a long list of color spaces corresponding to various camera models, which is very useful in cinema for working on filmed images, but which complicates the task in animation and especially in Blender . The list of color spaces displayed in Blender with the original ACES configuration! We offer for download here a modified version of ACES specifically for animation, notably by limiting the number of color spaces included to those that are potentially really necessary in animation. This work done from Sony Pictures ImageWorks sources is available here on Github . The list of color spaces displayed in Blender with the configuration ACES adjusted by us. Here are the new settings available once this configuration is installed.","title":"K.3 - Using ACES"},{"location":"blender.html#k3a-rendering-scene-referred","text":"Here are the settings for the rendering parameters, color management , once ACES * is available. The Display Device now contains only one option, ACES . This is not very standard in the way it works, but ACES is made like that\u2026 View Transform contains the display spaces, to which the ACEScg workspace is applied. There is no creative Look provided with ACES . The Looks of Filmic are not very useful anyway (they can easily be reproduced in compositing). Simply choose the display space corresponding to your screen (usually sRGB ). Raw displays the raw data without transformation, as they are saved in the OpenEXR files if this is the format chosen for output. Log is the ACES equivalent of Filmic Log , useful in case of strong color adjustement in compositing. Several spaces offer a variation of the white point * D60 instead of D65 . Indeed, D60 (about 6000 K ) is the white point of ACES and choosing one of these options allows to see the image without the white point* conversion.","title":"K.3.a - Rendering (scene referred)"},{"location":"blender.html#k3b-input-textures","text":"For the import of images, a large list of different color spaces is available. Here are the most useful and common ones. The ACES spaces are the usual ACES spaces: ACES2065-1 : color space for storage recommended by the ACES standard in OpenEXR files, but still quite little used. ACEScc and ACES - ACEScct : ACES color space dedicated to color correction, in OpenEXR files in general. It is unlikely that you will have to import this kind of file into Blender , except possibly an OpenEXR file from a color grading or compositing software. ACEScg : ACES workspace for rendering and compositing. In a pipeline using ACES , textures can be made in this space and imported into Blender via OpenEXR files. Input - Generic - sRGB - Texture et Utility - sRGB - Texture are identical. For standard sRGB image files ( PNG , TGA , etc). The Output spaces should not be used as input\u2026 The Role spaces are aliases for other spaces depending on their usage, and are not very useful here. The Utility spaces contain several important elements for importing images into Blender : Curve converts only the transfer curve of the imported files, but not the primaries. Linear convert only the primaries but not the transfer curve. Some are common and useful: Linear - sRGB is the default space for OpenEXR files. Linear - Rec.709 is perfectly identical to Linear - sRGB . Raw is the space to be used for files that do not contain images but other data (like normal map , displacement map , metalness , etc.) Rec.709 - Display is the space for standard HD videos (e.g. MP4 ) Rec.2020 - Display is a UHD (4K) video space In a nutshell : OpenEXR files containing images: ACES - ACEScg if the file comes from an application working in ACES , Utility - Linear - sRGB in other cases. OpenEXR files containing data other than color: Utility - Raw . Image files ( PNG , JPG , TGA*, etc.): Utility - sRGB - Texture **. Video files ( MP4*): Utility - Rec.709 - Texture in most cases, Utility - Rec.2020 - Display ** in case of UHD video. Sources et r\u00e9f\u00e9rences","title":"K.3.b - Input (textures)"},{"location":"calibration.html","text":"II.C - Screen calibration \u00b6 The very important first stage of color management in a production pipeline of digital images is to ensure the fidelity of the screens, that the colors displayed are the closest possible to what the images contain. It is the calibration process. Calibration, by greatly improving the fidelity of the screens, makes it possible to have a correct display, even very good by using a probe, even on screens of lesser quality 1 . II.C - Screen calibration C.1 - Introduction C.2 - Environment C.3 - Calibration C.3.a - Choice of the screen color space and calibration C.3.b - White point and brightness C.3.c - Calibration and application of the color profile C.3.c.1 - Calibration C.3.c.2 - Colorimetric profile C.1 - Introduction \u00b6 The purpose of screen calibration is multiple: To ensure that for any given color value, all screens in the production pipeline display the same color. To ensure that for any given color value, the color displayed is the one recommended by the color space used by the screen. To ensure that the people using the screens perceive the correct color (influence of the environment). The obstacles to achieving this are also multiple: Displays in their factory configuration are rarely true to the advertised color space. Displays age, and their colors can therefore vary over time. Not to mention that even the temperature can influence the electronic components, and therefore the displayed colors. It is complicated to have an objective reference where we are sure that the colors are correct, which can be used as a point of comparison. The environment (ambient light, color of the walls\u2026) influences the perception of the colors. The most reliable way to do this is to rely on a calibration probe . A probe is in fact a light sensor that works in tandem with the computer to apply color corrections, and its role is to measure the colors displayed by the screen, to compare with reference values. Most of the probes are also able to measure the ambient light to take into account the variations due to the environment and the human perception. The use of a probe thus makes it possible to overcome all these obstacles: They allow to measure the colors of the screen and their shift compared to the reference values of the colorimetric space of the screen. By recalibrating the screens regularly, they make it possible to maintain a sufficient fidelity throughout the life of the screen and the conditions. They avoid the need for a reference: they can objectively measure the colors, whose values can be compared directly with the values expected by the color space of the screen. Most of the probes can remain connected to the computer, and regularly measure the ambient light, in order to compensate for the variations in realtime. However, in the absence of a probe, it is also possible to improve the display of colors with another sensor: the eye. The results will not be as precise as with a probe, but it\u2019s possible to improve the display of the worst screens. The main problem of eyeballing is the lack of an objective reference to compare colors: it is impossible to be sure that the colors displayed are the right ones. The influence of the environment is also a problem, but can be compensated with good planning. C.2 - Environment \u00b6 First of all, a relatively easy parameter to control is the working environment and the ambient light in which the screen colors are seen. A few simple rules can ensure that you always see the same colors, and as objectively as possible. Avoid as much as possible natural lighting, which varies at every moment; if the work on colors is essential, a room without windows is ideal. Without going that far, the possibility of dimming the light from the windows as much as possible, or shutters to cut it off completely when needed are simple but effective solutions. Control the lighting and the color of the walls (which make the general color of the ambient light). The colorimetric spaces generally define the conditions in which their colors are supposed to be seen; the ideal situation is thus to respect the recommendations of the space used by the ecan. In all cases, neutral light (white, of a defined temperature * but not colored), and of good quality should be preferred; walls should be as neutral as possible, and ideally gray rather than white. Control the brightness of the screens, according to the ambient light: the screen must be sufficiently bright in relation to the environment (but not to the point of appearing overly bright: it must not be a source of light). Here are the parameters of the environment such as it is recommended by the color space sRGB , that of the majority of the screens, and applicable to the other spaces (P3, Rec.2020\u2026): Ambient light : 64 lux to 200 lux * . This is low to very low light, lower than typical office light (which is between 300 and 500 lux). Lighting temperature : 5003 K , D50 . It is a relatively orange light, close to that of the sun. It is voluntarily a little far from the white of the sRGB and therefore of the screens (which is 6500 K , D65 ). Wall color : Medium gray (20% reflectance). The walls should absorb 80% of the light rays they receive, making them medium gray. Screen brightness : 80 cd/m\u00b2 The sRGB (just like the P3 for the display) recommends a relatively low luminosity of the screens (if the environment is not very luminous). A slightly higher brightness may be necessary if the environment is not dark enough. It should be noted that the spaces dedicated to video like Rec.709 or Rec.2020 are adapted to TV screens with a screen brightness of 100 cd/m\u00b2 , and that it can thus be useful to increase the brightness of the screens towards this value for broadcast television (or cinema). C.3 - Calibration \u00b6 Once the environment has been controlled, or in any case set in the working conditions, we can proceed to the calibration of the screens. Before starting, it is necessary to make sure of the parameters of the calibration to be done. C.3.a - Choice of the screen color space and calibration \u00b6 We choose default one, in general sRGB or sometimes Display P3 or more rarely others. You can also decide to calibrate to a different space than the one intended for the screen, provided that the two spaces are compatible . This can be useful when all manufactured images are intended for a specific use (TV, cinema\u2026) in order to work in the same or close to the space used for the broadcasting of the finished images. For example, screens can be calibrated in Rec.709 to make HD videos for TV and watch them in broadcast conditions, without having to convert the videos at playback time to the screen space 2 . It is necessary for that that the primaries * of the two spaces are the same, or that the gamut * of the space of the screen is larger and contains the gamut of the space in which one calibrates. In any case, you should remember to change the display settings in all applications and the operating system so that the images are displayed in the space in which you have calibrated the screen. The choice of space will define the primaries * and gama * used; the white point * can often be set separately. C.3.b - White point and brightness \u00b6 The white point is often adjustable when using a probe; unless there are specific needs such as work for printing (in which case the white point can be that of the paper), one chooses that of the colorimetric space of the screen, i.e. D65 , 6500 K in the majority of the cases. The luminosity can also often be measured and adjusted. When working for images in sRGB intended to be seen on a computer screen, the reference is 80 cd/m\u00b2 . For television work (or cinema), the luminosity can be that of Rec.709 , that is 100 cd/m\u00b2 . It may be useful to choose a slightly higher value if the working environment is too bright. C.3.c - Calibration and application of the color profile \u00b6 C.3.c.1 - Calibration \u00b6 If you use a probe, the process is usually quite simple: you connect the probe to the computer and place it flat on the screen (or facing the projector screen) 3 . A software provided with the probe (or the very powerful free software DisplayCal 4 ) then performs a series of measurements of the colors displayed by the screen. The result of these measurements allows to calculate the shift between the displayed colors and the expected colors. Calibration parameters of DisplayCAL If you calibrate by eye, the idea is to display a series of test cards , reference images, and try to adjust screen settings by hand to approximate the description of a reference image. It is completely impossible to correctly adjust the primaries * and the white point * by eye, but one can try to get close by displaying different images. On the other hand, the brightness * , the contrast * , the gama * can be relatively well approached thanks to comparisons of grey values between them (and a lot of patience). To calibrate by eye several methods are possible, but everything always starts with the display of test patterns and reference images. Some operating systems offer them, but you can also find them on the net like the ones we propose here , carefully made to accompany this document. These test cards can also be used to check the calibration result with a probe. It is best to display these test patterns in full screen mode and with the environment controlled, as explained in chapter C.2 - Environment . Then, by displaying the images, we retouch the screen settings. There are three possibilities: Tinker with the screen settings; but they are often quite limited and not very precise. Adjust the settings of the graphics card via its drivers, if they allow it. The advantage is that the correction will be correctly applied in all situations; but it is possible that the parameters will be re-initialized when the system or the drivers are updated. If the operating system offers it, make the adjustments via its color utility and save then apply a color profile to the screen. It is then necessary to make sure at the time of updates that the system does not re-select another profile. The interest of this method is to be able to keep the settings in a file that can be saved and transported, the profile, generally in .icc format. It is not advisable to combine these different methods, as the result quickly becomes rather random, and especially difficult to retouch or correct afterwards. Warning At the time of writing, during our tests on Mac OS , the screen calibration tool in the system settings has a mediocre method (and results) (except perhaps on Mac screens) and does not offer test patterns worthy of the name, it should be avoided. It is better to calibrate directly with good test patterns such as the ones we propose, using the screen settings or other utilities. Tip Windows offers a good tool for calibrating screens by eye with rather good results. You can find it via the Control Panel , in Color Management then the Advanced tab. Then click on Screen calibration and let yourself be guided! It is in these options of Color Management that you can apply and choose the spaces and profiles (calibration result) of the screens. Example of parameters via the settings of a Nvidia graphics card (under Linux ). Note especially here the color range parameter, to be set to Full if the screen is a computer screen, and Limited if it is a television. Example of parameters via the settings of a Nvidia graphics card (under Linux ). Here, different sliders allow to adjust the display of the colors (for each screen). Each slider can act on the three channels * at the same time, or separately on the red, green or blue. C.3.c.2 - Colorimetric profile \u00b6 The result of the calibration of the probe, or sometimes of a calibration with the eye, is recorded in a colorimetric profile , a LUT * . It is the operating system which will apply this profile or this LUT to all the colors at the time of the posting on the corresponding screen and thus to convert these colors so that they are displayed exactly as they must. Choice of color profiles under Ubuntu Linux in the Color section of the settings. A button allows you to add profiles (predefined or imported via a file, for example generated by a probe) to the list under each screen to easily change profile later. Once the calibration is finished and the profile applied, you can check the result by displaying different test patterns (see above, C.3.c.1 - Calibration by eye), and also check the quality of the screen after calibration. Note that if you calibrate with a probe but the display of the test patterns is not perfect, it is probably more related to the quality of the screen than to a calibration defect. Sources & References As long as the colors of the screen are more or less uniform and identical in the center as well as on the edges and in the corners\u2026 \u21a9 It should be noted that it is not at all necessary for the screens to use the broadcast color space: thanks to color management, the displayed images are converted to the screen space so that the displayed colors are exactly those of the broadcast no matter what; calibrating the screens in the broadcast space just allows to avoid this conversion, and to avoid the common configuration errors by systematically choosing the broadcast space, whether for display, file output, etc. A concrete example: when working in video and for television, the images will be in Rec.709 , but the working screens are by default in sRGB . When making the images, it is therefore necessary to specify (but this is often automatic), that the images are displayed in sRGB , whereas when exporting they will be chosen Rec.709 . If we calibrate the screens in Rec.709 , we can simply choose Rec.709 everywhere without making a mistake (but it is less standard in computing, and the default or automatic configurations may be wrong\u2026). \u21a9 It is advisable to start by restoring the screen settings to as neutral as possible, disabling power saving, the various modes (game, office, video \u2026), to let the sensor and the system control the colors. \u21a9 DisplayCAL is free and available on Linux, MacOS, Windows and supports a wide range of probe makes and models: https://displaycal.net/ As always, we strongly encourage you to make a donation if you decide to use it! \u21a9","title":"C - Screen calibration"},{"location":"calibration.html#iic-screen-calibration","text":"The very important first stage of color management in a production pipeline of digital images is to ensure the fidelity of the screens, that the colors displayed are the closest possible to what the images contain. It is the calibration process. Calibration, by greatly improving the fidelity of the screens, makes it possible to have a correct display, even very good by using a probe, even on screens of lesser quality 1 . II.C - Screen calibration C.1 - Introduction C.2 - Environment C.3 - Calibration C.3.a - Choice of the screen color space and calibration C.3.b - White point and brightness C.3.c - Calibration and application of the color profile C.3.c.1 - Calibration C.3.c.2 - Colorimetric profile","title":"II.C - Screen calibration"},{"location":"calibration.html#c1-introduction","text":"The purpose of screen calibration is multiple: To ensure that for any given color value, all screens in the production pipeline display the same color. To ensure that for any given color value, the color displayed is the one recommended by the color space used by the screen. To ensure that the people using the screens perceive the correct color (influence of the environment). The obstacles to achieving this are also multiple: Displays in their factory configuration are rarely true to the advertised color space. Displays age, and their colors can therefore vary over time. Not to mention that even the temperature can influence the electronic components, and therefore the displayed colors. It is complicated to have an objective reference where we are sure that the colors are correct, which can be used as a point of comparison. The environment (ambient light, color of the walls\u2026) influences the perception of the colors. The most reliable way to do this is to rely on a calibration probe . A probe is in fact a light sensor that works in tandem with the computer to apply color corrections, and its role is to measure the colors displayed by the screen, to compare with reference values. Most of the probes are also able to measure the ambient light to take into account the variations due to the environment and the human perception. The use of a probe thus makes it possible to overcome all these obstacles: They allow to measure the colors of the screen and their shift compared to the reference values of the colorimetric space of the screen. By recalibrating the screens regularly, they make it possible to maintain a sufficient fidelity throughout the life of the screen and the conditions. They avoid the need for a reference: they can objectively measure the colors, whose values can be compared directly with the values expected by the color space of the screen. Most of the probes can remain connected to the computer, and regularly measure the ambient light, in order to compensate for the variations in realtime. However, in the absence of a probe, it is also possible to improve the display of colors with another sensor: the eye. The results will not be as precise as with a probe, but it\u2019s possible to improve the display of the worst screens. The main problem of eyeballing is the lack of an objective reference to compare colors: it is impossible to be sure that the colors displayed are the right ones. The influence of the environment is also a problem, but can be compensated with good planning.","title":"C.1 - Introduction"},{"location":"calibration.html#c2-environment","text":"First of all, a relatively easy parameter to control is the working environment and the ambient light in which the screen colors are seen. A few simple rules can ensure that you always see the same colors, and as objectively as possible. Avoid as much as possible natural lighting, which varies at every moment; if the work on colors is essential, a room without windows is ideal. Without going that far, the possibility of dimming the light from the windows as much as possible, or shutters to cut it off completely when needed are simple but effective solutions. Control the lighting and the color of the walls (which make the general color of the ambient light). The colorimetric spaces generally define the conditions in which their colors are supposed to be seen; the ideal situation is thus to respect the recommendations of the space used by the ecan. In all cases, neutral light (white, of a defined temperature * but not colored), and of good quality should be preferred; walls should be as neutral as possible, and ideally gray rather than white. Control the brightness of the screens, according to the ambient light: the screen must be sufficiently bright in relation to the environment (but not to the point of appearing overly bright: it must not be a source of light). Here are the parameters of the environment such as it is recommended by the color space sRGB , that of the majority of the screens, and applicable to the other spaces (P3, Rec.2020\u2026): Ambient light : 64 lux to 200 lux * . This is low to very low light, lower than typical office light (which is between 300 and 500 lux). Lighting temperature : 5003 K , D50 . It is a relatively orange light, close to that of the sun. It is voluntarily a little far from the white of the sRGB and therefore of the screens (which is 6500 K , D65 ). Wall color : Medium gray (20% reflectance). The walls should absorb 80% of the light rays they receive, making them medium gray. Screen brightness : 80 cd/m\u00b2 The sRGB (just like the P3 for the display) recommends a relatively low luminosity of the screens (if the environment is not very luminous). A slightly higher brightness may be necessary if the environment is not dark enough. It should be noted that the spaces dedicated to video like Rec.709 or Rec.2020 are adapted to TV screens with a screen brightness of 100 cd/m\u00b2 , and that it can thus be useful to increase the brightness of the screens towards this value for broadcast television (or cinema).","title":"C.2 - Environment"},{"location":"calibration.html#c3-calibration","text":"Once the environment has been controlled, or in any case set in the working conditions, we can proceed to the calibration of the screens. Before starting, it is necessary to make sure of the parameters of the calibration to be done.","title":"C.3 - Calibration"},{"location":"calibration.html#c3a-choice-of-the-screen-color-space-and-calibration","text":"We choose default one, in general sRGB or sometimes Display P3 or more rarely others. You can also decide to calibrate to a different space than the one intended for the screen, provided that the two spaces are compatible . This can be useful when all manufactured images are intended for a specific use (TV, cinema\u2026) in order to work in the same or close to the space used for the broadcasting of the finished images. For example, screens can be calibrated in Rec.709 to make HD videos for TV and watch them in broadcast conditions, without having to convert the videos at playback time to the screen space 2 . It is necessary for that that the primaries * of the two spaces are the same, or that the gamut * of the space of the screen is larger and contains the gamut of the space in which one calibrates. In any case, you should remember to change the display settings in all applications and the operating system so that the images are displayed in the space in which you have calibrated the screen. The choice of space will define the primaries * and gama * used; the white point * can often be set separately.","title":"C.3.a - Choice of the screen color space and calibration"},{"location":"calibration.html#c3b-white-point-and-brightness","text":"The white point is often adjustable when using a probe; unless there are specific needs such as work for printing (in which case the white point can be that of the paper), one chooses that of the colorimetric space of the screen, i.e. D65 , 6500 K in the majority of the cases. The luminosity can also often be measured and adjusted. When working for images in sRGB intended to be seen on a computer screen, the reference is 80 cd/m\u00b2 . For television work (or cinema), the luminosity can be that of Rec.709 , that is 100 cd/m\u00b2 . It may be useful to choose a slightly higher value if the working environment is too bright.","title":"C.3.b - White point and brightness"},{"location":"calibration.html#c3c-calibration-and-application-of-the-color-profile","text":"","title":"C.3.c - Calibration and application of the color profile"},{"location":"calibration.html#c3c1-calibration","text":"If you use a probe, the process is usually quite simple: you connect the probe to the computer and place it flat on the screen (or facing the projector screen) 3 . A software provided with the probe (or the very powerful free software DisplayCal 4 ) then performs a series of measurements of the colors displayed by the screen. The result of these measurements allows to calculate the shift between the displayed colors and the expected colors. Calibration parameters of DisplayCAL If you calibrate by eye, the idea is to display a series of test cards , reference images, and try to adjust screen settings by hand to approximate the description of a reference image. It is completely impossible to correctly adjust the primaries * and the white point * by eye, but one can try to get close by displaying different images. On the other hand, the brightness * , the contrast * , the gama * can be relatively well approached thanks to comparisons of grey values between them (and a lot of patience). To calibrate by eye several methods are possible, but everything always starts with the display of test patterns and reference images. Some operating systems offer them, but you can also find them on the net like the ones we propose here , carefully made to accompany this document. These test cards can also be used to check the calibration result with a probe. It is best to display these test patterns in full screen mode and with the environment controlled, as explained in chapter C.2 - Environment . Then, by displaying the images, we retouch the screen settings. There are three possibilities: Tinker with the screen settings; but they are often quite limited and not very precise. Adjust the settings of the graphics card via its drivers, if they allow it. The advantage is that the correction will be correctly applied in all situations; but it is possible that the parameters will be re-initialized when the system or the drivers are updated. If the operating system offers it, make the adjustments via its color utility and save then apply a color profile to the screen. It is then necessary to make sure at the time of updates that the system does not re-select another profile. The interest of this method is to be able to keep the settings in a file that can be saved and transported, the profile, generally in .icc format. It is not advisable to combine these different methods, as the result quickly becomes rather random, and especially difficult to retouch or correct afterwards. Warning At the time of writing, during our tests on Mac OS , the screen calibration tool in the system settings has a mediocre method (and results) (except perhaps on Mac screens) and does not offer test patterns worthy of the name, it should be avoided. It is better to calibrate directly with good test patterns such as the ones we propose, using the screen settings or other utilities. Tip Windows offers a good tool for calibrating screens by eye with rather good results. You can find it via the Control Panel , in Color Management then the Advanced tab. Then click on Screen calibration and let yourself be guided! It is in these options of Color Management that you can apply and choose the spaces and profiles (calibration result) of the screens. Example of parameters via the settings of a Nvidia graphics card (under Linux ). Note especially here the color range parameter, to be set to Full if the screen is a computer screen, and Limited if it is a television. Example of parameters via the settings of a Nvidia graphics card (under Linux ). Here, different sliders allow to adjust the display of the colors (for each screen). Each slider can act on the three channels * at the same time, or separately on the red, green or blue.","title":"C.3.c.1 - Calibration"},{"location":"calibration.html#c3c2-colorimetric-profile","text":"The result of the calibration of the probe, or sometimes of a calibration with the eye, is recorded in a colorimetric profile , a LUT * . It is the operating system which will apply this profile or this LUT to all the colors at the time of the posting on the corresponding screen and thus to convert these colors so that they are displayed exactly as they must. Choice of color profiles under Ubuntu Linux in the Color section of the settings. A button allows you to add profiles (predefined or imported via a file, for example generated by a probe) to the list under each screen to easily change profile later. Once the calibration is finished and the profile applied, you can check the result by displaying different test patterns (see above, C.3.c.1 - Calibration by eye), and also check the quality of the screen after calibration. Note that if you calibrate with a probe but the display of the test patterns is not perfect, it is probably more related to the quality of the screen than to a calibration defect. Sources & References As long as the colors of the screen are more or less uniform and identical in the center as well as on the edges and in the corners\u2026 \u21a9 It should be noted that it is not at all necessary for the screens to use the broadcast color space: thanks to color management, the displayed images are converted to the screen space so that the displayed colors are exactly those of the broadcast no matter what; calibrating the screens in the broadcast space just allows to avoid this conversion, and to avoid the common configuration errors by systematically choosing the broadcast space, whether for display, file output, etc. A concrete example: when working in video and for television, the images will be in Rec.709 , but the working screens are by default in sRGB . When making the images, it is therefore necessary to specify (but this is often automatic), that the images are displayed in sRGB , whereas when exporting they will be chosen Rec.709 . If we calibrate the screens in Rec.709 , we can simply choose Rec.709 everywhere without making a mistake (but it is less standard in computing, and the default or automatic configurations may be wrong\u2026). \u21a9 It is advisable to start by restoring the screen settings to as neutral as possible, disabling power saving, the various modes (game, office, video \u2026), to let the sensor and the system control the colors. \u21a9 DisplayCAL is free and available on Linux, MacOS, Windows and supports a wide range of probe makes and models: https://displaycal.net/ As always, we strongly encourage you to make a donation if you decide to use it! \u21a9","title":"C.3.c.2 - Colorimetric profile"},{"location":"darktable.html","text":"II.L - Color management: Darktable \u00b6 Darktable is a free and open source photo editing software, extremely complete and professional; the color management is very complete. L.1 - Input and workspace \u00b6 The color workspace is set per file, at the same time as the input, via the module \u201c input color profile \u201d. The configuration is very simple: input profile is used to define the profile of the image on which we work; in the case of an image jpeg for example, we will choose sRGB . working profile is the workspace; we prefer a space with a wide gamut * , and linear. Darktable proposes the most useful and common ones, like the Rec. 2020 , Adobe RGB or the prophoto dedicated to photo editing. gamut clipping is an interesting option for displaying on the image the areas whose colors are outside the gamut of the given space; the output space is usually chosen here. To activate the gamut clipping alerts you have to activate the option via the button under the image: L.2 - Output \u00b6 The output is easily adjusted via the \u201coutput color profile\u201d module. L.3 - Soft-Proofing \u00b6 Soft-proofing can be activated and regulated with the buttons under the image. A right click gives access to the settings. softproof profile defines the simulated/activated space for the soft-proofing. It is there that one finds the specific profiles ICC for the impression which one would have added (see below). display profile allows you to modify the screen space; in general, you can leave \u201c system display profile \u201d (this is the only one that will take into account the possible calibration of the screen). Otherwise you should always choose the normal screen space ( sRGB in general). preview display profile is the space of the possible second image preview window, in case it is on a second different screen. histogram profile defines the space used by the histogram and the eyedropper of Darktable . You can add your own output and proofing profiles by adding the ICC files in the color/out subfolder of the Darktable installation (or the $HOME/.config/darktable/ folder under linux).","title":"L - Darktable"},{"location":"darktable.html#iil-color-management-darktable","text":"Darktable is a free and open source photo editing software, extremely complete and professional; the color management is very complete.","title":"II.L - Color management: Darktable"},{"location":"darktable.html#l1-input-and-workspace","text":"The color workspace is set per file, at the same time as the input, via the module \u201c input color profile \u201d. The configuration is very simple: input profile is used to define the profile of the image on which we work; in the case of an image jpeg for example, we will choose sRGB . working profile is the workspace; we prefer a space with a wide gamut * , and linear. Darktable proposes the most useful and common ones, like the Rec. 2020 , Adobe RGB or the prophoto dedicated to photo editing. gamut clipping is an interesting option for displaying on the image the areas whose colors are outside the gamut of the given space; the output space is usually chosen here. To activate the gamut clipping alerts you have to activate the option via the button under the image:","title":"L.1 - Input and workspace"},{"location":"darktable.html#l2-output","text":"The output is easily adjusted via the \u201coutput color profile\u201d module.","title":"L.2 - Output"},{"location":"darktable.html#l3-soft-proofing","text":"Soft-proofing can be activated and regulated with the buttons under the image. A right click gives access to the settings. softproof profile defines the simulated/activated space for the soft-proofing. It is there that one finds the specific profiles ICC for the impression which one would have added (see below). display profile allows you to modify the screen space; in general, you can leave \u201c system display profile \u201d (this is the only one that will take into account the possible calibration of the screen). Otherwise you should always choose the normal screen space ( sRGB in general). preview display profile is the space of the possible second image preview window, in case it is on a second different screen. histogram profile defines the space used by the histogram and the eyedropper of Darktable . You can add your own output and proofing profiles by adding the ICC files in the color/out subfolder of the Darktable installation (or the $HOME/.config/darktable/ folder under linux).","title":"L.3 - Soft-Proofing"},{"location":"dume.html","text":"II.M - Color management: Duduf Media Encoder (DuME) \u00b6 DuME is a free and open source media encoder/converter based on ffmpeg and OpenColorIO . M.1 - Workspace \u00b6 In general, to encode media, you don\u2019t necessarily need to define a workspace, but since DuME also allows you to edit videos, it can be useful to choose a specific workspace for it; it\u2019s also useful to encode openEXR files directly from a render in the same space as the render. The space is simply selected in the output panel. M.2 - Input \u00b6 The interpretation of the input colors is done by adding the block Interpret Colors in the input panel. Hint In the absence of the Interpret Colors block, DuME uses its own rules to \u201cguess\u201d the space to use, respecting the most common standards, which makes this block optional if the input files are standard. The characteristics of the color space of the input file can be set individually. Primaries & White point : primaries * and white point ( illuminant * )to use, defined by the name of the corresponding space or standard. Transfer curve : transfer curve * to use, defined by the name of the corresponding space or standard. YUV Range : allows you to choose between Full and Limited depending on the file. YUV to RGB Matrix : defines the mathematical method used for conversions between YUV and RGB . In case of doubt, it is always possible to leave one of the parameters on \u201c Auto \u201d. Tip DuME offers a list of presets for all the most common spaces, see below. M.3 - Output \u00b6 As for the input, the output settings are made via a Color management block in the output panel. The parameters are the same as for the input. Hint As for the input, in the absence of the Color management block, DuME uses its own rules to \u201cguess\u201d the space to use, respecting the most common standards, which makes this block optional if the input files are standard. Tip DuME offers a list of presets for all the most common spaces, see below. M.4 - Presets \u00b6 To select more quickly and without error the different input and output parameters, DuME offers several presets, accessible by clicking on the block options button. It contains the most common spaces, including ACES in the latest versions. M.5 - LUT \u00b6 It is also possible to apply a LUT * when encoding media, via the Apply LUT block in the output panel. DuME offers several LUTs , including one to convert from or to ACES or Blender Filmic , and other LUT files can be loaded. DuME also offers in its tools a LUT conversion tool, which allows to convert between many formats. M.6 - OCIO \u00b6 There is no direct support for OCIO * when encoding media, but DuME provides a tool for generating LUT from a OCIO config, which can then be used during encoding (see above). The different parameters can be found by reading the config file OCIO , cf . section II.F - Designing a production pipeline with OCIO","title":"M - DuME"},{"location":"dume.html#iim-color-management-duduf-media-encoder-dume","text":"DuME is a free and open source media encoder/converter based on ffmpeg and OpenColorIO .","title":"II.M - Color management: Duduf Media Encoder (DuME)"},{"location":"dume.html#m1-workspace","text":"In general, to encode media, you don\u2019t necessarily need to define a workspace, but since DuME also allows you to edit videos, it can be useful to choose a specific workspace for it; it\u2019s also useful to encode openEXR files directly from a render in the same space as the render. The space is simply selected in the output panel.","title":"M.1 - Workspace"},{"location":"dume.html#m2-input","text":"The interpretation of the input colors is done by adding the block Interpret Colors in the input panel. Hint In the absence of the Interpret Colors block, DuME uses its own rules to \u201cguess\u201d the space to use, respecting the most common standards, which makes this block optional if the input files are standard. The characteristics of the color space of the input file can be set individually. Primaries & White point : primaries * and white point ( illuminant * )to use, defined by the name of the corresponding space or standard. Transfer curve : transfer curve * to use, defined by the name of the corresponding space or standard. YUV Range : allows you to choose between Full and Limited depending on the file. YUV to RGB Matrix : defines the mathematical method used for conversions between YUV and RGB . In case of doubt, it is always possible to leave one of the parameters on \u201c Auto \u201d. Tip DuME offers a list of presets for all the most common spaces, see below.","title":"M.2 - Input"},{"location":"dume.html#m3-output","text":"As for the input, the output settings are made via a Color management block in the output panel. The parameters are the same as for the input. Hint As for the input, in the absence of the Color management block, DuME uses its own rules to \u201cguess\u201d the space to use, respecting the most common standards, which makes this block optional if the input files are standard. Tip DuME offers a list of presets for all the most common spaces, see below.","title":"M.3 - Output"},{"location":"dume.html#m4-presets","text":"To select more quickly and without error the different input and output parameters, DuME offers several presets, accessible by clicking on the block options button. It contains the most common spaces, including ACES in the latest versions.","title":"M.4 - Presets"},{"location":"dume.html#m5-lut","text":"It is also possible to apply a LUT * when encoding media, via the Apply LUT block in the output panel. DuME offers several LUTs , including one to convert from or to ACES or Blender Filmic , and other LUT files can be loaded. DuME also offers in its tools a LUT conversion tool, which allows to convert between many formats.","title":"M.5 - LUT"},{"location":"dume.html#m6-ocio","text":"There is no direct support for OCIO * when encoding media, but DuME provides a tool for generating LUT from a OCIO config, which can then be used during encoding (see above). The different parameters can be found by reading the config file OCIO , cf . section II.F - Designing a production pipeline with OCIO","title":"M.6 - OCIO"},{"location":"epreuvage.html","text":"II.D - Soft-Proofing and simulation \u00b6 The proofing , or simulation or soft-proofing , consists in simulating the various conversions of colorimetric spaces which can intervene on the image before reaching the final user, in order to check the final conformity of the colors without having to export, print, calculate the project to test, but directly on the screen. Indeed, if the display space of the screen can not change and must be the one provided by the manufacturer ( sRGB in most cases), the project delivered will not necessarily be in this space. Let\u2019s take the example of a project delivered in standard HD video which will finally be exported in Rec. 709 space, the work on the image being done in RGB linear space. In this case, when working on the image, the display is done after a conversion from RGB Linear (working space) to sRGB (screen space). Work: RGB Linear \u2192 Screen: sRGB It can be useful in this case to introduce an intermediate conversion in Rec. 709 , space of the final output file, to control the result. In this case, a second conversion intervenes then to return to space sRGB of the screen This conversion would also take place when reading the exported file, in the same way, soft-proofing thus makes it possible to test the colors in real conditions of viewing. Work: Linear RGB \u2192 Output simulation: Rec.709 \u2192 Display: sRGB In the same way, it\u2019s possible to use this technique to preview and simulate the transformations which the colors will undergo at the time of much more radical changes of spaces of colors, for example when printing, and thus of transition towards a subtractive space like the CMYK ; in this case, when dealing with a simulation, the space of exit is not able to be reproduced exactly by the sRGB of the screen. Work: Linear RGB \u2192 Output simulation: CMYK \u2192 Display: sRGB","title":"D - Soft-Proofing"},{"location":"epreuvage.html#iid-soft-proofing-and-simulation","text":"The proofing , or simulation or soft-proofing , consists in simulating the various conversions of colorimetric spaces which can intervene on the image before reaching the final user, in order to check the final conformity of the colors without having to export, print, calculate the project to test, but directly on the screen. Indeed, if the display space of the screen can not change and must be the one provided by the manufacturer ( sRGB in most cases), the project delivered will not necessarily be in this space. Let\u2019s take the example of a project delivered in standard HD video which will finally be exported in Rec. 709 space, the work on the image being done in RGB linear space. In this case, when working on the image, the display is done after a conversion from RGB Linear (working space) to sRGB (screen space). Work: RGB Linear \u2192 Screen: sRGB It can be useful in this case to introduce an intermediate conversion in Rec. 709 , space of the final output file, to control the result. In this case, a second conversion intervenes then to return to space sRGB of the screen This conversion would also take place when reading the exported file, in the same way, soft-proofing thus makes it possible to test the colors in real conditions of viewing. Work: Linear RGB \u2192 Output simulation: Rec.709 \u2192 Display: sRGB In the same way, it\u2019s possible to use this technique to preview and simulate the transformations which the colors will undergo at the time of much more radical changes of spaces of colors, for example when printing, and thus of transition towards a subtractive space like the CMYK ; in this case, when dealing with a simulation, the space of exit is not able to be reproduced exactly by the sRGB of the screen. Work: Linear RGB \u2192 Output simulation: CMYK \u2192 Display: sRGB","title":"II.D - Soft-Proofing and simulation"},{"location":"krita.html","text":"II.N - Color management : Krita \u00b6 Krita is very complete in the management of colors, and uses in particular OCIO * . Its only drawback is that you can\u2019t specify the color space of the images you import individually. This can be a problem especially when importing openEXR * files which do not come from an application sharing the same OCIO * configuration or if you mix files using different color spaces. LThe Krita documentation includes interesting and detailed explanations about colors and color management. N.1 - Workspace \u00b6 The workspace is defined when creating a Krita document. You can choose the space from the list in the Color tab. Model allows you to choose the format in which the colors will be recorded in the document. We generally stay on RGB/Alpha if we work in video, or possibly CMYK/Alpha in case of a document intended for printing. Depth defines the depth of the colors. We will generally choose 16-bit float/channel when working for video or animation. Profile allows to select the working space, often scRGB (Linear) or ACEScg for video. The Color Space Browser button opens a more complete dialog box describing the different color spaces available and facilitating the choice. You can also browse the files to choose a space in ICC format. It is possible to change the default profile used in new documents via the Krita preferences. You can also set some other options. To change the workspace of an already opened document, go to the menu Image then Properties . It contains the same settings as when the document was created. N.2 - Display \u00b6 N.2.a - Screen \u00b6 You can specify the display color space for each screen connected to the computer, in the Krita preferences. In general, we leave sRGB except in the case where the screen uses a different space. N.2.b - Soft-proofing \u00b6 In the third tab of the preferences, it is possible to configure the simulation of the display according to the final output space, which is particularly interesting in Krita when working for printing. The color selectable in Gamut Warning is the one used to display alerts showing areas of the image outside the output gamut * , and which will therefore be changed during output. Once the proofing is configured, it can be activated and deactivated for the display of the documents via the menu View ( Display ) or with the keyboard shortcut [CTRL] + [Y] . Krita can also display an alert for areas of the image whose colors are outside the output gamut, also via the View ( Display ) menu or with the keyboard shortcut [CTRL] + [SHIFT] + [Y] . You can also define these parameters of proofing only for the opened document, via the menu Image then Properties . N.3 - Color picker \u00b6 Krita allows you to adjust the display space of the color pickers, which is very practical. You can find the setting in a tab of the preferences. In general, we check the box allowing to choose a specific space for the color picker (in particular a non-linear space, like the simple sRGB used by the screens). When working in 16 or 32 bpc floating and a linear space, selector types other than HSV (hue, saturation, value) - i.e. HSL , HSI and HSY - may not work properly (because they must have a maximum white which no longer exists in linear). N.4 - Output \u00b6 When saving files, Krita displays a dialog box with the appropriate options. When saving in native .kra or openEXR * format, the workspace is used; otherwise Krita will convert to the standard space of the saved file. Example for a PNG output and its conversion to sRGB, or optionally to Rec.2020. N.5 - OCIO with Krita \u00b6 Krita uses OCIO which is simply configured via the Docker (panel) called LUT Management . An example with the OCIO Filmic configuration of Blender","title":"N - Krita"},{"location":"krita.html#iin-color-management-krita","text":"Krita is very complete in the management of colors, and uses in particular OCIO * . Its only drawback is that you can\u2019t specify the color space of the images you import individually. This can be a problem especially when importing openEXR * files which do not come from an application sharing the same OCIO * configuration or if you mix files using different color spaces. LThe Krita documentation includes interesting and detailed explanations about colors and color management.","title":"II.N - Color management : Krita"},{"location":"krita.html#n1-workspace","text":"The workspace is defined when creating a Krita document. You can choose the space from the list in the Color tab. Model allows you to choose the format in which the colors will be recorded in the document. We generally stay on RGB/Alpha if we work in video, or possibly CMYK/Alpha in case of a document intended for printing. Depth defines the depth of the colors. We will generally choose 16-bit float/channel when working for video or animation. Profile allows to select the working space, often scRGB (Linear) or ACEScg for video. The Color Space Browser button opens a more complete dialog box describing the different color spaces available and facilitating the choice. You can also browse the files to choose a space in ICC format. It is possible to change the default profile used in new documents via the Krita preferences. You can also set some other options. To change the workspace of an already opened document, go to the menu Image then Properties . It contains the same settings as when the document was created.","title":"N.1 - Workspace"},{"location":"krita.html#n2-display","text":"","title":"N.2 - Display"},{"location":"krita.html#n2a-screen","text":"You can specify the display color space for each screen connected to the computer, in the Krita preferences. In general, we leave sRGB except in the case where the screen uses a different space.","title":"N.2.a - Screen"},{"location":"krita.html#n2b-soft-proofing","text":"In the third tab of the preferences, it is possible to configure the simulation of the display according to the final output space, which is particularly interesting in Krita when working for printing. The color selectable in Gamut Warning is the one used to display alerts showing areas of the image outside the output gamut * , and which will therefore be changed during output. Once the proofing is configured, it can be activated and deactivated for the display of the documents via the menu View ( Display ) or with the keyboard shortcut [CTRL] + [Y] . Krita can also display an alert for areas of the image whose colors are outside the output gamut, also via the View ( Display ) menu or with the keyboard shortcut [CTRL] + [SHIFT] + [Y] . You can also define these parameters of proofing only for the opened document, via the menu Image then Properties .","title":"N.2.b - Soft-proofing"},{"location":"krita.html#n3-color-picker","text":"Krita allows you to adjust the display space of the color pickers, which is very practical. You can find the setting in a tab of the preferences. In general, we check the box allowing to choose a specific space for the color picker (in particular a non-linear space, like the simple sRGB used by the screens). When working in 16 or 32 bpc floating and a linear space, selector types other than HSV (hue, saturation, value) - i.e. HSL , HSI and HSY - may not work properly (because they must have a maximum white which no longer exists in linear).","title":"N.3 - Color picker"},{"location":"krita.html#n4-output","text":"When saving files, Krita displays a dialog box with the appropriate options. When saving in native .kra or openEXR * format, the workspace is used; otherwise Krita will convert to the standard space of the saved file. Example for a PNG output and its conversion to sRGB, or optionally to Rec.2020.","title":"N.4 - Output"},{"location":"krita.html#n5-ocio-with-krita","text":"Krita uses OCIO which is simply configured via the Docker (panel) called LUT Management . An example with the OCIO Filmic configuration of Blender","title":"N.5 - OCIO with Krita"},{"location":"ocio.html","text":"II.F - Designing a production pipeline with OCIO \u00b6 The interest of OCIO[ ](ZZ-vocabulaire.md) lies in the fact that, by using it, you can configure the entire production pipeline at once, for all the software in the pipeline (as long as they are compatible with OCIO*). In general, when you configure a production pipeline using OCIO , you start with an existing config that you modify according to the needs of the project and the artistic and manufacturing choices. A set of configurations is available in the [ OpenColorIO ] downloads (http://opencolorio.org), including ACES , spi-anim and spi-vfx , developed by Sony Pictures Imageworks . Another configuration is available with Blender . F.1 - Setting up \u00b6 The implementation of OCIO on a production pipeline is relatively simple. First of all, you have to choose a configuration to use, and eventually modify it and adapt it to your needs (see next section \u201c OCIO config anatomy\u201d). The principle is then to give to each application the path to the configuration file config.ocio . This can be done easily for all of them by modifying the OCIO environment variable, giving it the path to the configuration file. It is also possible to give a specific path to each application; in this case the method varies according to the application. F.2 - OCIO configuratin anatomy \u00b6 An OCIO configuration actually consists of a set of LUT * accompanied by a simple text file containing the actual configuration, always named config.ocio . Here is for example the Filmic config used by Blender : The config file is divided into several sections, usually in the following order: Metadata , including various information. Roles , defining in which cases which color spaces should be used. Displays , defining the spaces to be used for the display according to the screens. Colorspaces , containing the list of color spaces used. Looks (optional), containing the list of LUT and transformations that can be added to the image. Other sections can be added, see the official documentation for details on opencolorio.org . Comments that do not affect the configuration can also be added to the file. A line containing a comment must simply begin with the character # . Hint The syntax actually follows the standard YAML . We explain here only the different sections that a current user can modify, but all sections can be manipulated by more advanced users. You should refer to the official documentation of OpenColorIO . F.2.a - Metadata \u00b6 Example : ocio_profile_version: 1 search_path: \"luts:filmic\" strictparsing: true luma: [0.2126, 0.7152, 0.0722] description: RRT version ut33 Here is a description of some common parameters: ocio_profile_version : either 1 or 2 , depending on the version of OCIO to be used with this config. description (optional) : a brief description name (optional) : an unique name search_path (optional) : defines where the LUT needed for the config are. In this example, they are in two subfolders (separated by : ): luts and filmic . luma (optional and discontinued) : old value used in particular by OCIO for the conversions in black and white of the colors. This value is actually no longer used anywhere, so it can be omitted. By default, it corresponds to the luminance of the Rec. 709 primaries. F.2.b - Roles \u00b6 The roles section is the one that will be modified most often; it is the one that defines how the different color spaces available are used, what the default spaces should be during the different manufacturing stages, etc. Example (commented) of Blender . : roles: reference: Linear # Internal scene linear space scene_linear: Linear rendering: Linear # Default color space for byte image default_byte: sRGB # Default color space for float images default_float: Linear # Default color space sequencer is working in default_sequencer: sRGB # Color spaces for color picking and texture painting (not internally supported yet) color_picking: sRGB texture_paint: Raw # Non-color data data: Non-Color # CIE XYZ color space XYZ: XYZ On each line, the role is defined, then the color space to be used. The name of the space must be that of one of the spaces available in the list in the following section. ACES examples: roles: color_picking: Output - sRGB color_timing: ACES - ACEScc compositing_linear: ACES - ACEScg compositing_log: Input - ADX - ADX10 data: Utility - Raw default: ACES - ACES2065-1 matte_paint: Utility - sRGB - Texture reference: Utility - Raw rendering: ACES - ACEScg scene_linear: ACES - ACEScg texture_paint: ACES - ACEScc Here are some explanations on the different roles (alphabetical sorting): color_picking : space for color selectors. color_timing : space for colorimetric corrections. compositing_linear : space for compositing. compositing_log : alternative space for some compositing operations. data : space for files ( EXR ) containing something other than color (normal maps, specs, etc.). matte_paint : space for images used in matte painting. reference : space used as a base for all the others ; the other spaces are defined according to this one. rendering : space to use for rendering. scene_linear : main working space : the one for 3D rendering in particular, the one for the generation of the image. texture_paint : similar to matte_paint but for textures used on 3D objects. Note Not all applications will respect all roles: some are not supported, or the application does not allow to change them (for example, Blender does not allow to change its color picker space) Some applications may also include other additional roles that are not listed here. This is the case for Blender with roles starting with default_ for example. F.2.c - Displays \u00b6 The displays section defines the list of different possible displays, for example work screens and projection room projectors. Each display has a list of possible views, allowing to modify the display, for example for a technical validation, to better discern dark colors, etc. Example of Blender : displays: sRGB: - !<View> {name: Standard, colorspace: sRGB} - !<View> {name: Filmic, colorspace: Filmic sRGB} - !<View> {name: Filmic Log, colorspace: Filmic Log} - !<View> {name: Raw, colorspace: Raw} - !<View> {name: False Color, colorspace: False Color} XYZ: - !<View> {name: Standard, colorspace: XYZ} - !<View> {name: DCI, colorspace: dci_xyz} - !<View> {name: Raw, colorspace: Raw} None: - !<View> {name: Standard, colorspace: Raw} The standard view displays sRGB values directly; Filmic Log can be used to check dark areas. Raw displays the data without conversion, for example to perform a technical check of a data layer (normal map, occlusion, etc.). False Color is also used for technical checks of the image. Example of ACES : displays: ACES: - !<View> {name: sRGB, colorspace: Output - sRGB} - !<View> {name: DCDM, colorspace: Output - DCDM} - !<View> {name: DCDM P3 gamut clip, colorspace: Output - DCDM (P3 gamut clip)} - !<View> {name: P3-D60, colorspace: Output - P3-D60} - !<View> {name: P3-D60 ST2084 1000 nits, colorspace: Output - P3-D60 ST2084 (1000 nits)} - !<View> {name: P3-D60 ST2084 2000 nits, colorspace: Output - P3-D60 ST2084 (2000 nits)} - !<View> {name: P3-D60 ST2084 4000 nits, colorspace: Output - P3-D60 ST2084 (4000 nits)} - !<View> {name: P3-DCI, colorspace: Output - P3-DCI} - !<View> {name: Rec.2020, colorspace: Output - Rec.2020} - !<View> {name: Rec.2020 ST2084 1000 nits, colorspace: Output - Rec.2020 ST2084 (1000 nits)} - !<View> {name: Rec.709, colorspace: Output - Rec.709} - !<View> {name: Rec.709 D60 sim., colorspace: Output - Rec.709 (D60 sim.)} - !<View> {name: sRGB D60 sim., colorspace: Output - sRGB (D60 sim.)} - !<View> {name: Raw, colorspace: Utility - Raw} - !<View> {name: Log, colorspace: Input - ADX - ADX10} ACES does not sort by different displays, and everything is gathered under one display. Sources & References Documentation on OCIO configurations","title":"F - Designing a production pipeline with OCIO"},{"location":"ocio.html#iif-designing-a-production-pipeline-with-ocio","text":"The interest of OCIO[ ](ZZ-vocabulaire.md) lies in the fact that, by using it, you can configure the entire production pipeline at once, for all the software in the pipeline (as long as they are compatible with OCIO*). In general, when you configure a production pipeline using OCIO , you start with an existing config that you modify according to the needs of the project and the artistic and manufacturing choices. A set of configurations is available in the [ OpenColorIO ] downloads (http://opencolorio.org), including ACES , spi-anim and spi-vfx , developed by Sony Pictures Imageworks . Another configuration is available with Blender .","title":"II.F - Designing a production pipeline with OCIO"},{"location":"ocio.html#f1-setting-up","text":"The implementation of OCIO on a production pipeline is relatively simple. First of all, you have to choose a configuration to use, and eventually modify it and adapt it to your needs (see next section \u201c OCIO config anatomy\u201d). The principle is then to give to each application the path to the configuration file config.ocio . This can be done easily for all of them by modifying the OCIO environment variable, giving it the path to the configuration file. It is also possible to give a specific path to each application; in this case the method varies according to the application.","title":"F.1 - Setting up"},{"location":"ocio.html#f2-ocio-configuratin-anatomy","text":"An OCIO configuration actually consists of a set of LUT * accompanied by a simple text file containing the actual configuration, always named config.ocio . Here is for example the Filmic config used by Blender : The config file is divided into several sections, usually in the following order: Metadata , including various information. Roles , defining in which cases which color spaces should be used. Displays , defining the spaces to be used for the display according to the screens. Colorspaces , containing the list of color spaces used. Looks (optional), containing the list of LUT and transformations that can be added to the image. Other sections can be added, see the official documentation for details on opencolorio.org . Comments that do not affect the configuration can also be added to the file. A line containing a comment must simply begin with the character # . Hint The syntax actually follows the standard YAML . We explain here only the different sections that a current user can modify, but all sections can be manipulated by more advanced users. You should refer to the official documentation of OpenColorIO .","title":"F.2 - OCIO configuratin anatomy"},{"location":"ocio.html#f2a-metadata","text":"Example : ocio_profile_version: 1 search_path: \"luts:filmic\" strictparsing: true luma: [0.2126, 0.7152, 0.0722] description: RRT version ut33 Here is a description of some common parameters: ocio_profile_version : either 1 or 2 , depending on the version of OCIO to be used with this config. description (optional) : a brief description name (optional) : an unique name search_path (optional) : defines where the LUT needed for the config are. In this example, they are in two subfolders (separated by : ): luts and filmic . luma (optional and discontinued) : old value used in particular by OCIO for the conversions in black and white of the colors. This value is actually no longer used anywhere, so it can be omitted. By default, it corresponds to the luminance of the Rec. 709 primaries.","title":"F.2.a - Metadata"},{"location":"ocio.html#f2b-roles","text":"The roles section is the one that will be modified most often; it is the one that defines how the different color spaces available are used, what the default spaces should be during the different manufacturing stages, etc. Example (commented) of Blender . : roles: reference: Linear # Internal scene linear space scene_linear: Linear rendering: Linear # Default color space for byte image default_byte: sRGB # Default color space for float images default_float: Linear # Default color space sequencer is working in default_sequencer: sRGB # Color spaces for color picking and texture painting (not internally supported yet) color_picking: sRGB texture_paint: Raw # Non-color data data: Non-Color # CIE XYZ color space XYZ: XYZ On each line, the role is defined, then the color space to be used. The name of the space must be that of one of the spaces available in the list in the following section. ACES examples: roles: color_picking: Output - sRGB color_timing: ACES - ACEScc compositing_linear: ACES - ACEScg compositing_log: Input - ADX - ADX10 data: Utility - Raw default: ACES - ACES2065-1 matte_paint: Utility - sRGB - Texture reference: Utility - Raw rendering: ACES - ACEScg scene_linear: ACES - ACEScg texture_paint: ACES - ACEScc Here are some explanations on the different roles (alphabetical sorting): color_picking : space for color selectors. color_timing : space for colorimetric corrections. compositing_linear : space for compositing. compositing_log : alternative space for some compositing operations. data : space for files ( EXR ) containing something other than color (normal maps, specs, etc.). matte_paint : space for images used in matte painting. reference : space used as a base for all the others ; the other spaces are defined according to this one. rendering : space to use for rendering. scene_linear : main working space : the one for 3D rendering in particular, the one for the generation of the image. texture_paint : similar to matte_paint but for textures used on 3D objects. Note Not all applications will respect all roles: some are not supported, or the application does not allow to change them (for example, Blender does not allow to change its color picker space) Some applications may also include other additional roles that are not listed here. This is the case for Blender with roles starting with default_ for example.","title":"F.2.b - Roles"},{"location":"ocio.html#f2c-displays","text":"The displays section defines the list of different possible displays, for example work screens and projection room projectors. Each display has a list of possible views, allowing to modify the display, for example for a technical validation, to better discern dark colors, etc. Example of Blender : displays: sRGB: - !<View> {name: Standard, colorspace: sRGB} - !<View> {name: Filmic, colorspace: Filmic sRGB} - !<View> {name: Filmic Log, colorspace: Filmic Log} - !<View> {name: Raw, colorspace: Raw} - !<View> {name: False Color, colorspace: False Color} XYZ: - !<View> {name: Standard, colorspace: XYZ} - !<View> {name: DCI, colorspace: dci_xyz} - !<View> {name: Raw, colorspace: Raw} None: - !<View> {name: Standard, colorspace: Raw} The standard view displays sRGB values directly; Filmic Log can be used to check dark areas. Raw displays the data without conversion, for example to perform a technical check of a data layer (normal map, occlusion, etc.). False Color is also used for technical checks of the image. Example of ACES : displays: ACES: - !<View> {name: sRGB, colorspace: Output - sRGB} - !<View> {name: DCDM, colorspace: Output - DCDM} - !<View> {name: DCDM P3 gamut clip, colorspace: Output - DCDM (P3 gamut clip)} - !<View> {name: P3-D60, colorspace: Output - P3-D60} - !<View> {name: P3-D60 ST2084 1000 nits, colorspace: Output - P3-D60 ST2084 (1000 nits)} - !<View> {name: P3-D60 ST2084 2000 nits, colorspace: Output - P3-D60 ST2084 (2000 nits)} - !<View> {name: P3-D60 ST2084 4000 nits, colorspace: Output - P3-D60 ST2084 (4000 nits)} - !<View> {name: P3-DCI, colorspace: Output - P3-DCI} - !<View> {name: Rec.2020, colorspace: Output - Rec.2020} - !<View> {name: Rec.2020 ST2084 1000 nits, colorspace: Output - Rec.2020 ST2084 (1000 nits)} - !<View> {name: Rec.709, colorspace: Output - Rec.709} - !<View> {name: Rec.709 D60 sim., colorspace: Output - Rec.709 (D60 sim.)} - !<View> {name: sRGB D60 sim., colorspace: Output - sRGB (D60 sim.)} - !<View> {name: Raw, colorspace: Utility - Raw} - !<View> {name: Log, colorspace: Input - ADX - ADX10} ACES does not sort by different displays, and everything is gathered under one display. Sources & References Documentation on OCIO configurations","title":"F.2.c - Displays"},{"location":"pratique.html","text":"II.A - Practical application: choose your color spaces and formats \u00b6 Working within an digital image application, it is important to know and master how the colors are managed in the various functionalities of the application in question, as well as throughout the production pipeline in which it is inserted. This second part is intended to explain concretely how to organize this production pipeline as regards the color, and to help to parameterize it. A.1 - Theory \u00b6 Before setting up the various software programs, it is important to understand that colors are regularly converted during the production of images. Indeed, at each stage since the generation of the color until its posting on a device, each end of software and hardware intervening works in its own space. To control the production pipeline of the colors thus does not imply to choose well a single colorimetric space, but rather to be aware of the various conversions and the various colorimetric spaces coming into play at each stage, within an application as well as between the applications. This control will not guarantee that the colors will be correctly reproduced on the viewer\u2019s device (TV, computer screen, telephone, cinema screen\u2026) but at least allows to have control throughout the production while ensuring to deliver images that respect the standards in force. It is then up to the broadcaster to take over at the end of the pipeline with a correctly parameterized pipeline. A.1.a - Journey of a color \u00b6 Let\u2019s follow the journey that a color has to go through before it is correctly displayed. A color generated by one application and to be exported to another application, through an intermediate file, is likely to undergo two conversions: from the first application to the file, and from the file to the second application. This operation is repeated at each stage of the production pipeline, until broadcast, where the broadcast application (and hardware) convert what they receive into the broadcast color space. Each of the applications must thus imperatively be correctly informed on the colorimetric space of the files which it imports, in order to be able to carry out the good conversion towards its own colorimetric space. In the same way, at the time of export, the good conversion since the colorimetric space of the application towards that of the exported file must be carried out. But within an application itself, many conversions can take place: From the color space of the imported file to the color space of the application. From the color space of the application towards that of the screen of the computer, for the preview. From the color space of the application towards that of the output file. Indeed, all these colorimetric spaces are not necessarily the same\u2026 With each \u201cbrick\u201d of the application using colors, a color space is associated. Let us see these various bricks and some recommendations. Hint All applications do not necessarily allow access to all the settings of all the spaces for these different elements. The \u201cimposed\u201d settings can be more or less practical and intelligent depending on the application\u2026 In the applications, we will have to adjust the different stages of color management: Workspace Input (imports) Display Color pickers Output ( intermediate et final ) Note All the following explanations apply to design applications (3D, drawing, compositing, retouching\u2026) as well as to players (image display, video players\u2026) B - Workspace (scene referred) \u00b6 The most important space to know is the one in which the application works, in which it performs the color synthesis and the associated calculations. This space is the one noted scene referred . Here are the most commonly used characteristics for a workspace : Linear in the great majority of the cases, to improve the blending of the colors, it can be useful for the colorimetric corrections to use a nonlinear space. Gamut large in order to work in a space capable of faithfully reproducing a large number of colors and gain in precision. Some applications allow you to change the workspace, which is particularly interesting in the case of 3D renderers: the workspace influences the rendering of colors and allows you to adjust the way you work with lights and materials. Warning The workspace is chosen before starting to work ; indeed, it is once the space is chosen that you work the colors in the space in question. Changing the workspace once the work has progressed makes no sense ; it will be necessary to adjust all the lights, all the color settings\u2026 It is in any case imperative that the workspace of the application is larger than that of the final output, both in gamut * and for the pixel format . C - Input \u00b6 Each time a file or other external element is imported, the application must interpret (know) the element\u2019s color space in order to convert it to its workspace. There are then several possibilities: Either the files respect the most common standards (when they exist\u2026), and the application interprets the files correctly by default. Either the files, or the application itself, do not respect these standards, or no standard exists, and the application must then allow to modify the interpretation of the data to specify manually which is the imported color space 1 . In any case, in order to control the production, it is imperative to control the interpretation of the colors by the applications during the import; some will systematically \u201cmake mistakes\u201d on certain files, and it will then be necessary to think of correcting the interpretation at each import (or automate it) 2 . See * A.6 - Intermediate Output and A.7 - Final Output for more information on file-specific color spaces, and II.B - Some Standards * for a list of the most common standards. D - Display \u00b6 It is important to keep in mind that the working color space, scene referred , is most often different from the display color space ! The application thus carries out a conversion of the colors such as it calculates them towards the display space of the screen. There are several elements to take into account for this display: The color space of the screen itself The adjustments of the screen which can deteriorate the colors The colorimetric profile applied to the screen by the operating system The conversion carried out by the application since its workspace towards that of the display. See section 2C - Screen Calibration for more details on the subject. D.1 - Screen space \u00b6 Each screen displays colors in a predefined color space chosen by the manufacturer for the particular model of screen. There are three main categories of screens: Computer screens (and projectors) Televisions Phones, tablets, etc. Following these categories, most displays use these color spaces: Computer: sRGB , although some displays (often called HDR ) are also capable of displaying P3 colors; P3 displays also display sRGB , which is contained entirely \u201cwithin\u201d P3 . Computer monitors display the full/pc range of colors ( cf. K.4 - Full range / Limited / TV / PC ? ). Televisions: Rec.709 , or sometimes sRGB (adjustable), or sometimes other spaces when they are HDR . TVs display the limited/tv range of colors ( cf. K.4 - Full range / Limited / TV / PC ? ). Phones, tablets, etc. : sRGB , although some (rare) phones and tablets are also able to display P3 colors. These devices display the full range (full/pc) of colors ( cf. K.4 - Full range / Limited / TV / PC ? ). It should be noted that the screens displaying exactly the announced color space are rare, and most generate (more or less) small variations; these variations are in general largely corrected by a controlled calibration. Cf. II.C - screen calibration . D.2 - Settings and color profiles \u00b6 The great majority of the screens propose several adjustments of the colors on the screen itself, in particular via the parameters of brightness and contrast, complemented, according to the screens, by the gammas * red, green and blue, and sometimes still other adjustments. These settings can sometimes correct the biggest defects of the screens as they are delivered from the factory (provided you have an efficient calibration method), and can be completed by finer adjustments, both via the colorimetric profile applied by the operating system, and possibly adjustments at the level of the graphics card driver. Warning Many displays offer \u201ceco\u201d, \u201cauto\u201d, \u201cgaming\u201d modes, etc., which adapt their settings automatically depending on the activity, the type of signal received, etc. In a production pipeline where colors are managed, it is imperative to disable these different modes that change the display in an unpredictable way. Knowing these settings is important for controlling the correct display of colors on the workstation. It should also be noted that these settings should be checked (and adjusted) regularly; the color display may vary with the aging of the screen, the ambient temperature, etc. Cf. II.C - screen calibration for detailed explanations of the screen settings and how to adjust them. D.3 - Within the application \u00b6 Once the screen is installed and properly set up (or as best as possible), all that is left to do is to select the correct display profile in the application. Most of the time, a simple display option allows you to specify whether the screen is sRGB , Rec.709 or P3 or whatever; sometimes no setting is available and the application relies on the operating system. It should be kept in mind that the application continues to work in its own space, which does not depend on the one in which the colors are displayed, and that the file outputs do not depend on this display space either; on the other hand, a bad choice of display leads to bad choices of colors and thus unexpected and non-standard variations in the output! The worst mistake is for example to choose the wrong display space and then believe that it is the output space that is different from what was expected. This error then leads to changing the interpretation of the colors when importing into the next application to try to compensate, and introduce bad corrections while completely losing control of the production pipeline. D.4 Soft-Proofing \u00b6 Some applications offer, in addition to controlling the conversions to display space, a simulation or screen proofing * , which consists of performing an intermediate conversion to the output space intended for the current job, before finally converting to display space. When working for specific outputs, it can be useful to activate this kind of tool and thus check the result after the multiple conversions that the colors will undergo until the final format. This method is in particular very useful to simulate the result of a printing in a space CMJN for example, but also the posting of a video in its format of exit. However the soft-proofing is only a method of checking and one can often do without it (especially in video). Cf. Soft-Proofing) for more details on the subject. E - Color pickers \u00b6 In an application the color pickers can have their own color space. Most often, they are either in the workspace of the application, which makes them difficult to use when the space is linear, or in the space of the display, more practical. We prefer non-linear spaces to facilitate the choice of colors; having color selectors in sRGB also allows to easily recover colors from other applications, from images, etc. A conversion is then carried out after the selection of the color towards the working space of the application. F - Intermediate output \u00b6 When exporting intermediate files, which will be used in the rest of the production, the aim is to lose as little information as possible, to keep a maximum of data for the rest of the work. In this case, the easiest thing to do is, as far as possible, to export files in the application\u2019s workspace. To do this, the file format best able to store any color information is openEXR (which is supposed to use linear spaces). It is quite possible to use other formats, but in this case, either the choice of space will not be standard and may be misinterpreted later, or unnecessary conversions are introduced, or a loss of data by having to use a smaller or non-linear space. If it is impossible to export in the workspace and in openEXR (or other format allowing to keep the right space), you should prefer RGB formats (and avoid YUV , or at least use the 4:4:4 subsampling). When the working space is linear but the output space is not (and vice versa), it is important to know that a loss of precision and quality occurs, and that in this case it is absolutely necessary that the depth of the working space is greater than that of the output space (working in 32 bpc linear to output in 16 bpc non-linear for example). G - Final output \u00b6 During the final output, it is of course necessary to try to respect as well as possible the standard corresponding to the delivery, or to refer to the request of the broadcaster. Cf. II.B - A few standards for a list of the most common standards. Most of the final outputs will be in color spaces dedicated to the display, and therefore with a non-linear transfer; a loss of precision and quality taking place when passing from a linear workspace to a non-linear display space, it is important in this case that the workspace has a depth higher than that of the final output (work in 16 bpc for an output 8 bpc for example) Sources & References If an application does not allow you to change the color space during import, be prepared to have unexpected color variations during import. You will then have to guess where the application is \u201cwrong\u201d in order to manually perform a color correction to restore the original colors (most often simply via a gamma * correction or the application of a LUT * ). Note that such an application does not really have its place in a production pipeline where one seeks to control the color\u2026 \u21a9 Not all applications allow the automation of color management (for example Adobe After Effects does not have an API for this precise point). In the rest of this document, we will note for the applications explained the errors of interpretation that they make, that we have been able to spot, and the other specificities to know. \u21a9","title":"A - Choosing your color spaces and formats"},{"location":"pratique.html#iia-practical-application-choose-your-color-spaces-and-formats","text":"Working within an digital image application, it is important to know and master how the colors are managed in the various functionalities of the application in question, as well as throughout the production pipeline in which it is inserted. This second part is intended to explain concretely how to organize this production pipeline as regards the color, and to help to parameterize it.","title":"II.A - Practical application: choose your color spaces and formats"},{"location":"pratique.html#a1-theory","text":"Before setting up the various software programs, it is important to understand that colors are regularly converted during the production of images. Indeed, at each stage since the generation of the color until its posting on a device, each end of software and hardware intervening works in its own space. To control the production pipeline of the colors thus does not imply to choose well a single colorimetric space, but rather to be aware of the various conversions and the various colorimetric spaces coming into play at each stage, within an application as well as between the applications. This control will not guarantee that the colors will be correctly reproduced on the viewer\u2019s device (TV, computer screen, telephone, cinema screen\u2026) but at least allows to have control throughout the production while ensuring to deliver images that respect the standards in force. It is then up to the broadcaster to take over at the end of the pipeline with a correctly parameterized pipeline.","title":"A.1 - Theory"},{"location":"pratique.html#a1a-journey-of-a-color","text":"Let\u2019s follow the journey that a color has to go through before it is correctly displayed. A color generated by one application and to be exported to another application, through an intermediate file, is likely to undergo two conversions: from the first application to the file, and from the file to the second application. This operation is repeated at each stage of the production pipeline, until broadcast, where the broadcast application (and hardware) convert what they receive into the broadcast color space. Each of the applications must thus imperatively be correctly informed on the colorimetric space of the files which it imports, in order to be able to carry out the good conversion towards its own colorimetric space. In the same way, at the time of export, the good conversion since the colorimetric space of the application towards that of the exported file must be carried out. But within an application itself, many conversions can take place: From the color space of the imported file to the color space of the application. From the color space of the application towards that of the screen of the computer, for the preview. From the color space of the application towards that of the output file. Indeed, all these colorimetric spaces are not necessarily the same\u2026 With each \u201cbrick\u201d of the application using colors, a color space is associated. Let us see these various bricks and some recommendations. Hint All applications do not necessarily allow access to all the settings of all the spaces for these different elements. The \u201cimposed\u201d settings can be more or less practical and intelligent depending on the application\u2026 In the applications, we will have to adjust the different stages of color management: Workspace Input (imports) Display Color pickers Output ( intermediate et final ) Note All the following explanations apply to design applications (3D, drawing, compositing, retouching\u2026) as well as to players (image display, video players\u2026)","title":"A.1.a - Journey of a color"},{"location":"pratique.html#b-workspace-scene-referred","text":"The most important space to know is the one in which the application works, in which it performs the color synthesis and the associated calculations. This space is the one noted scene referred . Here are the most commonly used characteristics for a workspace : Linear in the great majority of the cases, to improve the blending of the colors, it can be useful for the colorimetric corrections to use a nonlinear space. Gamut large in order to work in a space capable of faithfully reproducing a large number of colors and gain in precision. Some applications allow you to change the workspace, which is particularly interesting in the case of 3D renderers: the workspace influences the rendering of colors and allows you to adjust the way you work with lights and materials. Warning The workspace is chosen before starting to work ; indeed, it is once the space is chosen that you work the colors in the space in question. Changing the workspace once the work has progressed makes no sense ; it will be necessary to adjust all the lights, all the color settings\u2026 It is in any case imperative that the workspace of the application is larger than that of the final output, both in gamut * and for the pixel format .","title":"B - Workspace (scene referred)"},{"location":"pratique.html#c-input","text":"Each time a file or other external element is imported, the application must interpret (know) the element\u2019s color space in order to convert it to its workspace. There are then several possibilities: Either the files respect the most common standards (when they exist\u2026), and the application interprets the files correctly by default. Either the files, or the application itself, do not respect these standards, or no standard exists, and the application must then allow to modify the interpretation of the data to specify manually which is the imported color space 1 . In any case, in order to control the production, it is imperative to control the interpretation of the colors by the applications during the import; some will systematically \u201cmake mistakes\u201d on certain files, and it will then be necessary to think of correcting the interpretation at each import (or automate it) 2 . See * A.6 - Intermediate Output and A.7 - Final Output for more information on file-specific color spaces, and II.B - Some Standards * for a list of the most common standards.","title":"C - Input"},{"location":"pratique.html#d-display","text":"It is important to keep in mind that the working color space, scene referred , is most often different from the display color space ! The application thus carries out a conversion of the colors such as it calculates them towards the display space of the screen. There are several elements to take into account for this display: The color space of the screen itself The adjustments of the screen which can deteriorate the colors The colorimetric profile applied to the screen by the operating system The conversion carried out by the application since its workspace towards that of the display. See section 2C - Screen Calibration for more details on the subject.","title":"D - Display"},{"location":"pratique.html#d1-screen-space","text":"Each screen displays colors in a predefined color space chosen by the manufacturer for the particular model of screen. There are three main categories of screens: Computer screens (and projectors) Televisions Phones, tablets, etc. Following these categories, most displays use these color spaces: Computer: sRGB , although some displays (often called HDR ) are also capable of displaying P3 colors; P3 displays also display sRGB , which is contained entirely \u201cwithin\u201d P3 . Computer monitors display the full/pc range of colors ( cf. K.4 - Full range / Limited / TV / PC ? ). Televisions: Rec.709 , or sometimes sRGB (adjustable), or sometimes other spaces when they are HDR . TVs display the limited/tv range of colors ( cf. K.4 - Full range / Limited / TV / PC ? ). Phones, tablets, etc. : sRGB , although some (rare) phones and tablets are also able to display P3 colors. These devices display the full range (full/pc) of colors ( cf. K.4 - Full range / Limited / TV / PC ? ). It should be noted that the screens displaying exactly the announced color space are rare, and most generate (more or less) small variations; these variations are in general largely corrected by a controlled calibration. Cf. II.C - screen calibration .","title":"D.1 - Screen space"},{"location":"pratique.html#d2-settings-and-color-profiles","text":"The great majority of the screens propose several adjustments of the colors on the screen itself, in particular via the parameters of brightness and contrast, complemented, according to the screens, by the gammas * red, green and blue, and sometimes still other adjustments. These settings can sometimes correct the biggest defects of the screens as they are delivered from the factory (provided you have an efficient calibration method), and can be completed by finer adjustments, both via the colorimetric profile applied by the operating system, and possibly adjustments at the level of the graphics card driver. Warning Many displays offer \u201ceco\u201d, \u201cauto\u201d, \u201cgaming\u201d modes, etc., which adapt their settings automatically depending on the activity, the type of signal received, etc. In a production pipeline where colors are managed, it is imperative to disable these different modes that change the display in an unpredictable way. Knowing these settings is important for controlling the correct display of colors on the workstation. It should also be noted that these settings should be checked (and adjusted) regularly; the color display may vary with the aging of the screen, the ambient temperature, etc. Cf. II.C - screen calibration for detailed explanations of the screen settings and how to adjust them.","title":"D.2 - Settings and color profiles"},{"location":"pratique.html#d3-within-the-application","text":"Once the screen is installed and properly set up (or as best as possible), all that is left to do is to select the correct display profile in the application. Most of the time, a simple display option allows you to specify whether the screen is sRGB , Rec.709 or P3 or whatever; sometimes no setting is available and the application relies on the operating system. It should be kept in mind that the application continues to work in its own space, which does not depend on the one in which the colors are displayed, and that the file outputs do not depend on this display space either; on the other hand, a bad choice of display leads to bad choices of colors and thus unexpected and non-standard variations in the output! The worst mistake is for example to choose the wrong display space and then believe that it is the output space that is different from what was expected. This error then leads to changing the interpretation of the colors when importing into the next application to try to compensate, and introduce bad corrections while completely losing control of the production pipeline.","title":"D.3 - Within the application"},{"location":"pratique.html#d4-soft-proofing","text":"Some applications offer, in addition to controlling the conversions to display space, a simulation or screen proofing * , which consists of performing an intermediate conversion to the output space intended for the current job, before finally converting to display space. When working for specific outputs, it can be useful to activate this kind of tool and thus check the result after the multiple conversions that the colors will undergo until the final format. This method is in particular very useful to simulate the result of a printing in a space CMJN for example, but also the posting of a video in its format of exit. However the soft-proofing is only a method of checking and one can often do without it (especially in video). Cf. Soft-Proofing) for more details on the subject.","title":"D.4 Soft-Proofing"},{"location":"pratique.html#e-color-pickers","text":"In an application the color pickers can have their own color space. Most often, they are either in the workspace of the application, which makes them difficult to use when the space is linear, or in the space of the display, more practical. We prefer non-linear spaces to facilitate the choice of colors; having color selectors in sRGB also allows to easily recover colors from other applications, from images, etc. A conversion is then carried out after the selection of the color towards the working space of the application.","title":"E - Color pickers"},{"location":"pratique.html#f-intermediate-output","text":"When exporting intermediate files, which will be used in the rest of the production, the aim is to lose as little information as possible, to keep a maximum of data for the rest of the work. In this case, the easiest thing to do is, as far as possible, to export files in the application\u2019s workspace. To do this, the file format best able to store any color information is openEXR (which is supposed to use linear spaces). It is quite possible to use other formats, but in this case, either the choice of space will not be standard and may be misinterpreted later, or unnecessary conversions are introduced, or a loss of data by having to use a smaller or non-linear space. If it is impossible to export in the workspace and in openEXR (or other format allowing to keep the right space), you should prefer RGB formats (and avoid YUV , or at least use the 4:4:4 subsampling). When the working space is linear but the output space is not (and vice versa), it is important to know that a loss of precision and quality occurs, and that in this case it is absolutely necessary that the depth of the working space is greater than that of the output space (working in 32 bpc linear to output in 16 bpc non-linear for example).","title":"F - Intermediate output"},{"location":"pratique.html#g-final-output","text":"During the final output, it is of course necessary to try to respect as well as possible the standard corresponding to the delivery, or to refer to the request of the broadcaster. Cf. II.B - A few standards for a list of the most common standards. Most of the final outputs will be in color spaces dedicated to the display, and therefore with a non-linear transfer; a loss of precision and quality taking place when passing from a linear workspace to a non-linear display space, it is important in this case that the workspace has a depth higher than that of the final output (work in 16 bpc for an output 8 bpc for example) Sources & References If an application does not allow you to change the color space during import, be prepared to have unexpected color variations during import. You will then have to guess where the application is \u201cwrong\u201d in order to manually perform a color correction to restore the original colors (most often simply via a gamma * correction or the application of a LUT * ). Note that such an application does not really have its place in a production pipeline where one seeks to control the color\u2026 \u21a9 Not all applications allow the automation of color management (for example Adobe After Effects does not have an API for this precise point). In the rest of this document, we will note for the applications explained the errors of interpretation that they make, that we have been able to spot, and the other specificities to know. \u21a9","title":"G - Final output"},{"location":"premiere.html","text":"II.G - Color Management : Adobe Premiere and Media Encoder \u00b6 The color management options are quite limited (almost non-existent) in Adobe Premiere , and it does not support OCIO * . There is no setting to set up the input color interpretation , which implies that all media (other than After Effects compositions) must use standard color spaces, and that an openEXR file will always be read as RGB Linear for example. There is no configurable workspace which is not mandatory when editing, as long as you don\u2019t use too many effects. The choices are very limited on the output possibilities, everything is automatic. The output is necessarily done via Adobe Media Encoder . H.1 - Output space \u00b6 For the vast majority of formats, no setting is available. Only for encoding in h.264 or h.265 / hevc is it possible to use Rec.2020 , if and only if the high10 codec profile is selected. Primaries Colors Rec.2020 : Encoding is done in Rec.709 if unchecked. High dynamic range allows 12-bit rendering instead of 10-bit (see I.K - Pixel Formats ), which is also called HDR . *Include HDR10 Metadata unlocks the following options\u2026 Primaries Colors allows to change the primaries, and strangely enough, to choose the primaries of Display P3 or Rec.709 while the Rec.2020 box above must be checked, and are only available in HDR mode. The other options are metadata specific to the HDR video format for display purposes. H.2 - Applying LUT \u00b6 It is however possible to apply a LUT on the clips, which remains a way to theoretically do everything, provided you have the adequate LUT , which is not always obvious\u2026 You can find the option to add a LUT in the Lumetri effect or the panel of the same name. You can also apply a LUT via Media Encoder . The option to add a LUT can be found in the Effects tab of the output parameters. H.3 - Workspace and display \u00b6 It is important to know that the First sequences are always in Rec.709 during the editing; all the imported media are converted in Rec.709 and the effects, the editing is done in this color space. By default , Premiere does not convert the colors back to the sRGB of the screen , they remain in Rec.709 , which makes the colors look bad on standard sRGB screens (and even worse on P3 screens). This also means that by default the colors of the same footage on After Effects and Premiere are rendered differently . You can activate this missing conversion in the preferences , via the box Enable display color management , a useful option as soon as you don\u2019t mount on a Rec.709 screen but sRGB or P3 (which is the majority of cases). This option only influences the display to automatically convert from Rec.709 of Premiere to the screen space; in any case, it doesn\u2019t change anything to the final file output. This option is also available in Media Encoder although it is less important: it only affects the display of previews.","title":"H - Color management - Adobe Premiere et Media Encoder"},{"location":"premiere.html#iig-color-management-adobe-premiere-and-media-encoder","text":"The color management options are quite limited (almost non-existent) in Adobe Premiere , and it does not support OCIO * . There is no setting to set up the input color interpretation , which implies that all media (other than After Effects compositions) must use standard color spaces, and that an openEXR file will always be read as RGB Linear for example. There is no configurable workspace which is not mandatory when editing, as long as you don\u2019t use too many effects. The choices are very limited on the output possibilities, everything is automatic. The output is necessarily done via Adobe Media Encoder .","title":"II.G - Color Management : Adobe Premiere and Media Encoder"},{"location":"premiere.html#h1-output-space","text":"For the vast majority of formats, no setting is available. Only for encoding in h.264 or h.265 / hevc is it possible to use Rec.2020 , if and only if the high10 codec profile is selected. Primaries Colors Rec.2020 : Encoding is done in Rec.709 if unchecked. High dynamic range allows 12-bit rendering instead of 10-bit (see I.K - Pixel Formats ), which is also called HDR . *Include HDR10 Metadata unlocks the following options\u2026 Primaries Colors allows to change the primaries, and strangely enough, to choose the primaries of Display P3 or Rec.709 while the Rec.2020 box above must be checked, and are only available in HDR mode. The other options are metadata specific to the HDR video format for display purposes.","title":"H.1 - Output space"},{"location":"premiere.html#h2-applying-lut","text":"It is however possible to apply a LUT on the clips, which remains a way to theoretically do everything, provided you have the adequate LUT , which is not always obvious\u2026 You can find the option to add a LUT in the Lumetri effect or the panel of the same name. You can also apply a LUT via Media Encoder . The option to add a LUT can be found in the Effects tab of the output parameters.","title":"H.2 - Applying LUT"},{"location":"premiere.html#h3-workspace-and-display","text":"It is important to know that the First sequences are always in Rec.709 during the editing; all the imported media are converted in Rec.709 and the effects, the editing is done in this color space. By default , Premiere does not convert the colors back to the sRGB of the screen , they remain in Rec.709 , which makes the colors look bad on standard sRGB screens (and even worse on P3 screens). This also means that by default the colors of the same footage on After Effects and Premiere are rendered differently . You can activate this missing conversion in the preferences , via the box Enable display color management , a useful option as soon as you don\u2019t mount on a Rec.709 screen but sRGB or P3 (which is the majority of cases). This option only influences the display to automatically convert from Rec.709 of Premiere to the screen space; in any case, it doesn\u2019t change anything to the final file output. This option is also available in Media Encoder although it is less important: it only affects the display of previews.","title":"H.3 - Workspace and display"},{"location":"preparer.html","text":") II.E - Preparation of the color management of a production pipeline \u00b6 Now that we know the different types of color spaces and at what points in the production process they come into play, let\u2019s see how to prepare a complete production pipeline to effectively manage color from design to delivery. The first step is to list all the formats used at the input of the production pipeline (videos or images supplied and imported, manufactured images) as well as all the formats to be delivered at the end of the production process (videos and final images). Depending on these formats, and therefore the associated color spaces, we can choose the different spaces to be used during production, keeping in mind that \u201cwho can do more can do less\u201d: the easiest thing is to set up a high-performance chain that can deliver in all formats. It will also be necessary to take care that all the stages which modify the image use a colorimetric space larger than that of delivery: as well with a gamut * wider as with a larger depth. We will also try to limit the number of spaces used (and thus limit the possible errors and shifts due to conversions). The preparation of the pipeline consists therefore most often in choosing the most adequate workspace according to the input and especially output formats, and supported by the various applications. Here are some examples of possible choices, consistent with a cinema quality (and therefore all types of production). E.1 - 3D animation with Blender, Filmic \u00b6 Blender provides an efficient default workspace for 3D rendering: Filmic . This space being configured using OCIO , it is quite easy to create a production pipeline around it. For simplicity, the textures can be made in a standard RGB Linear space. As in most pipelines, we can use openEXR as an intermediate image format, both for textures and for intermediate renders or the final master . Step Type Color Space / Format Notes All Color Selectors sRGB (screen) It is always more convenient, and safer, to choose your colors in the non-linear screen space. All Display Screen Space ( sRGB ) Colors are always displayed in the color screen space. Textures Workspace RGB Linear / 16 bpc If the application does not support OCIO * Textures Workspace Filmic / 16 bpc Textures Output Workspace / openEXR 16 bpc 3D Workspace Filmic 3D Output Filmic / openEXR 16 bpc Compositing Workspace Filmic / 32 bpc In case of performance problems and if there is no color correction step following compositing, we may work with 16 bpc . If the application does not support OCIO , then a LUT * Filmic must be applied to the imported footage. Compositing Softproofing Final output space ( Rec.709 for example) Compositing Final Output Standard depending on the format: Rec.709 , Rec.2020 \u2026 Compositing Master / Archiving RGB Linear / openEXR 16 bpc It is probably safest to archive the images in as standard a space as possible, but without losing quality. Compositing Output for color correction RGB Linear / openEXR 32bpc 32bpc is necessary to avoid loss of quality when converting to a possibly non-linear space for color correction. E.1.a - Textures and other 2D images \u00b6 Whatever the application ( Substance Painter , Substance Designer , Krita , Adobe Photoshop , The Foundry Mari \u2026), the color textures can be created in RGB Linear ; just be careful to have at least 16 bpc . When importing in Blender , the images will be interpreted in RGB Linear , the data textures ( normal maps , metalness , etc) in Raw , without conversion. It is also possible to work directly with textures in Filmic space if the application supports OCIO * by simply loading the OCIO configuration from Blender , or by using an ICC profile corresponding to Filmic , such as the ones we propose download here . E.1.b - 3D Render \u00b6 3D rendering in Blender will be done in Filmic by default. You can choose different levels of contrast; it\u2019s an artistic choice to make when working on lookdev, you\u2019ll just have to re-apply the same one when compositing. E.1.c - Compositing \u00b6 If the compositing is done in Blender , it automatically uses the same workspace, i.e. Filmic (with the choice of contrast), and there is nothing to configure. In case of compositing in a third party application, the easiest way is to use OCIO and load the OCIO configuration from Blender to be able to apply again the Filmic workspace (and the choice of contrast) on the openEXR files rendered by Blender and imported into the compositing application. Warning When rendering openEXR , the images are linear and do not include color space information: the Filmic space must be reapplied in the compositing software. When rendering in any other format, Blender converts to the default space of the format ( sRGB to PNG for example). E.1.c.a - Filmic with After Effects \u00b6 For the particular case of After Effects , where setting up OCIO is tedious, it may be simpler to use a LUT * for Filmic conversion\u2026 We provide a variation of this LUT here (for each Filmic contrast variation available). You will then have to choose another workspace in the After Effects project, and use a space conversion effect from RGB Linear to the project space after applying the LUT . Example of effects to use on an openEXR Filmic file in After Effects with an ACEScg workspace*. E.1.d - Exports \u00b6 From the compositing software, we can then simply export openEXR RGB Linear files as master or backup , and directly video files in the standard color space that corresponds to them for the final exports. E.2 - 3D animation with ACES . \u00b6 Since most 3D software uses OCIO * , all of them will be able to use ACEScg * as their rendering space. As in most pipelines, we can use openEXR as an intermediate image format, both for textures and for intermediate renders or the final master . It should be noted that the ACES * configuration usually used, provided by OCIO , includes a very long list of color spaces, notably those of various camera manufacturers. If you don\u2019t need all these formats, on an animation production that doesn\u2019t include shooting, you can quite easily remove all these spaces from the setup. We provide here such an adapted version of ACES , which is more convenient in animation productions. Step Type Color Space / Format Notes All Color Selectors sRGB (from the screen) It is always more convenient, and safer, to choose your colors in the non-linear screen space. All Display Screen Space ( sRGB ) The colors are always displayed in the color screen space. Textures Workspace RGB Linear / 16 bpc If the application does not manage OCIO * Textures Workspace ACEScg / 16 bpc Textures Output Workspace / openEXR 16 bpc 3D Workspace ACEScg 3D Output ACEScg / openEXR 16 bpc Compositing Workspace ACEScg / 32 bpc In case of performance problems and if there is no color correction step following the compositing, we can eventually work with 16 bpc . Compositing Softproofing Final output space ( Rec.709 for example) In doubt, it is better not to configure softproofing and to remain on the color space of the screen for display. Compositing Final output Standard depending on the format : Rec.709 , Rec.2020 \u2026 Compositing Master / Archiving RGB Linear / openEXR 16 bpc It is probably safer to archive the images in as standard a space as possible, but without losing quality. Compositing Output for color correction ACEScg / openEXR 32bpc The 32 bpc are necessary to avoid loss of quality when converting to a probably non-linear space for color correction (which would be done with ACEScc in the case of a chain using ACES all along). E.2.a - Textures and other 2D images \u00b6 Whatever the application ( Substance Painter , Substance Designer , Krita , Adobe Photoshop , The Foundry Mari \u2026), the color textures can be created in RGB Linear ; just be careful to have at least 16 bpc . It is also possible, but not for all applications, to work directly in ACEScg ; in this case, it will be necessary to be vigilant when importing into the 3D software to specify the right space, RGB linear or ACEScg (or raw/data for data maps such as normal maps , specular , etc.) The interest of working directly with textures in ACEScg is to limit conversions and avoid the risks of color shift that result. E.2.b - 3D Rendering \u00b6 The 3D rendering is done with ACEScg E.2.c - Compositing \u00b6 For compositing, the easiest way is to use OCIO and load the ACES configuration so that you can reapply the ACEScg space to the rendered openEXR files and as a workspace. For the particular case of Adobe After Effects , there is no need to use OCIO , ACEScg is provided natively. E.2.d - Exports \u00b6 From the compositing software, you can then simply export openEXR RGB Linear files as master or backup , and directly video files in the corresponding standard color space. E.3 - 2D Animation \u00b6 In 2D animation, it is less important to use a gamut * wide workspace. Although it is recommended to follow the same methods as in 3D, some animation softwares will not allow it and we can therefore fall back on a RGB Linear pipeline for example. It will be necessary to use as much as possible openEXR files all along the pipeline to be sure to keep a sufficient quality. Here again, if for detailed scenery it is important to stay on this format and a linear space with at least 16 bpc , in the case of animation if it is less detailed or uses simple flat colors, we can fall back on more common spaces sRGB and PNG formats (with 16 bpc if possible) or vectors. Step Type Color Space / Format Notes All Color Selectors sRGB (from the screen) It is always more convenient, and safer, to choose your colors in the non-linear screen space. All Display Screen Space ( sRGB ) The colors are always displayed in the color screen space. Backgrounds Workspace RGB Linear / 16 bpc Backgrounds Output RGB Linear / openEXR 16 bpc Animation Workspace RGB Lin\u00e9aire / 16 bpc Animation Workspace sRGB / 16 bpc If the application does not allow you to work in linear. Animation Workspace sRGB / 8 bpc In case of vector output. Animation Output RGB Linear / openEXR 16 bpc Animation Output sRGB / PNG 16 bpc If the application does not allow exporting in EXR . Animation Output sRGB / 8 bpc In case of vector output. Compositing Workspace ACEScg / 32 bpc Or any other space at gamut * wide so that you can export in all types of formats, including film. Compositing Softproofing Final output space ( Rec.709 for example) In doubt, it is better not to configure softproofing and to remain on the color space of the screen for display. Compositing Final output Standard depending on the format : Rec.709 , Rec.2020 \u2026 Compositing Master / Archiving RGB Linear / openEXR 16 bpc It is probably safer to archive the images in as standard a space as possible, but without losing quality. Compositing Output for color correction RGB Linear / openEXR 32bpc 32 bpc are necessary to avoid losing quality when converting to a probably non-linear space for color correction. E.4 - Videos, live action and VFX \u00b6 A pipeline that includes video can be very similar to a 3D production pipeline, especially if it involves 3D renderings. Simply, you will have to be careful and import the photos and videos by specifying the correct color space, that of the camera that captured the images, as soon as they are imported into the software that uses them. OCIO and ACES contain a large selection of color spaces from various camera models, so it is often easiest to use a chain with ACES for production, as described above.","title":"E - Setting up the production pipeline"},{"location":"preparer.html#iie-preparation-of-the-color-management-of-a-production-pipeline","text":"Now that we know the different types of color spaces and at what points in the production process they come into play, let\u2019s see how to prepare a complete production pipeline to effectively manage color from design to delivery. The first step is to list all the formats used at the input of the production pipeline (videos or images supplied and imported, manufactured images) as well as all the formats to be delivered at the end of the production process (videos and final images). Depending on these formats, and therefore the associated color spaces, we can choose the different spaces to be used during production, keeping in mind that \u201cwho can do more can do less\u201d: the easiest thing is to set up a high-performance chain that can deliver in all formats. It will also be necessary to take care that all the stages which modify the image use a colorimetric space larger than that of delivery: as well with a gamut * wider as with a larger depth. We will also try to limit the number of spaces used (and thus limit the possible errors and shifts due to conversions). The preparation of the pipeline consists therefore most often in choosing the most adequate workspace according to the input and especially output formats, and supported by the various applications. Here are some examples of possible choices, consistent with a cinema quality (and therefore all types of production).","title":"II.E - Preparation of the color management of a production pipeline"},{"location":"preparer.html#e1-3d-animation-with-blender-filmic","text":"Blender provides an efficient default workspace for 3D rendering: Filmic . This space being configured using OCIO , it is quite easy to create a production pipeline around it. For simplicity, the textures can be made in a standard RGB Linear space. As in most pipelines, we can use openEXR as an intermediate image format, both for textures and for intermediate renders or the final master . Step Type Color Space / Format Notes All Color Selectors sRGB (screen) It is always more convenient, and safer, to choose your colors in the non-linear screen space. All Display Screen Space ( sRGB ) Colors are always displayed in the color screen space. Textures Workspace RGB Linear / 16 bpc If the application does not support OCIO * Textures Workspace Filmic / 16 bpc Textures Output Workspace / openEXR 16 bpc 3D Workspace Filmic 3D Output Filmic / openEXR 16 bpc Compositing Workspace Filmic / 32 bpc In case of performance problems and if there is no color correction step following compositing, we may work with 16 bpc . If the application does not support OCIO , then a LUT * Filmic must be applied to the imported footage. Compositing Softproofing Final output space ( Rec.709 for example) Compositing Final Output Standard depending on the format: Rec.709 , Rec.2020 \u2026 Compositing Master / Archiving RGB Linear / openEXR 16 bpc It is probably safest to archive the images in as standard a space as possible, but without losing quality. Compositing Output for color correction RGB Linear / openEXR 32bpc 32bpc is necessary to avoid loss of quality when converting to a possibly non-linear space for color correction.","title":"E.1 - 3D animation with Blender, Filmic"},{"location":"preparer.html#e1a-textures-and-other-2d-images","text":"Whatever the application ( Substance Painter , Substance Designer , Krita , Adobe Photoshop , The Foundry Mari \u2026), the color textures can be created in RGB Linear ; just be careful to have at least 16 bpc . When importing in Blender , the images will be interpreted in RGB Linear , the data textures ( normal maps , metalness , etc) in Raw , without conversion. It is also possible to work directly with textures in Filmic space if the application supports OCIO * by simply loading the OCIO configuration from Blender , or by using an ICC profile corresponding to Filmic , such as the ones we propose download here .","title":"E.1.a - Textures and other 2D images"},{"location":"preparer.html#e1b-3d-render","text":"3D rendering in Blender will be done in Filmic by default. You can choose different levels of contrast; it\u2019s an artistic choice to make when working on lookdev, you\u2019ll just have to re-apply the same one when compositing.","title":"E.1.b - 3D Render"},{"location":"preparer.html#e1c-compositing","text":"If the compositing is done in Blender , it automatically uses the same workspace, i.e. Filmic (with the choice of contrast), and there is nothing to configure. In case of compositing in a third party application, the easiest way is to use OCIO and load the OCIO configuration from Blender to be able to apply again the Filmic workspace (and the choice of contrast) on the openEXR files rendered by Blender and imported into the compositing application. Warning When rendering openEXR , the images are linear and do not include color space information: the Filmic space must be reapplied in the compositing software. When rendering in any other format, Blender converts to the default space of the format ( sRGB to PNG for example).","title":"E.1.c - Compositing"},{"location":"preparer.html#e1ca-filmic-with-after-effects","text":"For the particular case of After Effects , where setting up OCIO is tedious, it may be simpler to use a LUT * for Filmic conversion\u2026 We provide a variation of this LUT here (for each Filmic contrast variation available). You will then have to choose another workspace in the After Effects project, and use a space conversion effect from RGB Linear to the project space after applying the LUT . Example of effects to use on an openEXR Filmic file in After Effects with an ACEScg workspace*.","title":"E.1.c.a - Filmic with After Effects"},{"location":"preparer.html#e1d-exports","text":"From the compositing software, we can then simply export openEXR RGB Linear files as master or backup , and directly video files in the standard color space that corresponds to them for the final exports.","title":"E.1.d - Exports"},{"location":"preparer.html#e2-3d-animation-with-aces","text":"Since most 3D software uses OCIO * , all of them will be able to use ACEScg * as their rendering space. As in most pipelines, we can use openEXR as an intermediate image format, both for textures and for intermediate renders or the final master . It should be noted that the ACES * configuration usually used, provided by OCIO , includes a very long list of color spaces, notably those of various camera manufacturers. If you don\u2019t need all these formats, on an animation production that doesn\u2019t include shooting, you can quite easily remove all these spaces from the setup. We provide here such an adapted version of ACES , which is more convenient in animation productions. Step Type Color Space / Format Notes All Color Selectors sRGB (from the screen) It is always more convenient, and safer, to choose your colors in the non-linear screen space. All Display Screen Space ( sRGB ) The colors are always displayed in the color screen space. Textures Workspace RGB Linear / 16 bpc If the application does not manage OCIO * Textures Workspace ACEScg / 16 bpc Textures Output Workspace / openEXR 16 bpc 3D Workspace ACEScg 3D Output ACEScg / openEXR 16 bpc Compositing Workspace ACEScg / 32 bpc In case of performance problems and if there is no color correction step following the compositing, we can eventually work with 16 bpc . Compositing Softproofing Final output space ( Rec.709 for example) In doubt, it is better not to configure softproofing and to remain on the color space of the screen for display. Compositing Final output Standard depending on the format : Rec.709 , Rec.2020 \u2026 Compositing Master / Archiving RGB Linear / openEXR 16 bpc It is probably safer to archive the images in as standard a space as possible, but without losing quality. Compositing Output for color correction ACEScg / openEXR 32bpc The 32 bpc are necessary to avoid loss of quality when converting to a probably non-linear space for color correction (which would be done with ACEScc in the case of a chain using ACES all along).","title":"E.2 - 3D animation with ACES."},{"location":"preparer.html#e2a-textures-and-other-2d-images","text":"Whatever the application ( Substance Painter , Substance Designer , Krita , Adobe Photoshop , The Foundry Mari \u2026), the color textures can be created in RGB Linear ; just be careful to have at least 16 bpc . It is also possible, but not for all applications, to work directly in ACEScg ; in this case, it will be necessary to be vigilant when importing into the 3D software to specify the right space, RGB linear or ACEScg (or raw/data for data maps such as normal maps , specular , etc.) The interest of working directly with textures in ACEScg is to limit conversions and avoid the risks of color shift that result.","title":"E.2.a - Textures and other 2D images"},{"location":"preparer.html#e2b-3d-rendering","text":"The 3D rendering is done with ACEScg","title":"E.2.b - 3D Rendering"},{"location":"preparer.html#e2c-compositing","text":"For compositing, the easiest way is to use OCIO and load the ACES configuration so that you can reapply the ACEScg space to the rendered openEXR files and as a workspace. For the particular case of Adobe After Effects , there is no need to use OCIO , ACEScg is provided natively.","title":"E.2.c - Compositing"},{"location":"preparer.html#e2d-exports","text":"From the compositing software, you can then simply export openEXR RGB Linear files as master or backup , and directly video files in the corresponding standard color space.","title":"E.2.d - Exports"},{"location":"preparer.html#e3-2d-animation","text":"In 2D animation, it is less important to use a gamut * wide workspace. Although it is recommended to follow the same methods as in 3D, some animation softwares will not allow it and we can therefore fall back on a RGB Linear pipeline for example. It will be necessary to use as much as possible openEXR files all along the pipeline to be sure to keep a sufficient quality. Here again, if for detailed scenery it is important to stay on this format and a linear space with at least 16 bpc , in the case of animation if it is less detailed or uses simple flat colors, we can fall back on more common spaces sRGB and PNG formats (with 16 bpc if possible) or vectors. Step Type Color Space / Format Notes All Color Selectors sRGB (from the screen) It is always more convenient, and safer, to choose your colors in the non-linear screen space. All Display Screen Space ( sRGB ) The colors are always displayed in the color screen space. Backgrounds Workspace RGB Linear / 16 bpc Backgrounds Output RGB Linear / openEXR 16 bpc Animation Workspace RGB Lin\u00e9aire / 16 bpc Animation Workspace sRGB / 16 bpc If the application does not allow you to work in linear. Animation Workspace sRGB / 8 bpc In case of vector output. Animation Output RGB Linear / openEXR 16 bpc Animation Output sRGB / PNG 16 bpc If the application does not allow exporting in EXR . Animation Output sRGB / 8 bpc In case of vector output. Compositing Workspace ACEScg / 32 bpc Or any other space at gamut * wide so that you can export in all types of formats, including film. Compositing Softproofing Final output space ( Rec.709 for example) In doubt, it is better not to configure softproofing and to remain on the color space of the screen for display. Compositing Final output Standard depending on the format : Rec.709 , Rec.2020 \u2026 Compositing Master / Archiving RGB Linear / openEXR 16 bpc It is probably safer to archive the images in as standard a space as possible, but without losing quality. Compositing Output for color correction RGB Linear / openEXR 32bpc 32 bpc are necessary to avoid losing quality when converting to a probably non-linear space for color correction.","title":"E.3 - 2D Animation"},{"location":"preparer.html#e4-videos-live-action-and-vfx","text":"A pipeline that includes video can be very similar to a 3D production pipeline, especially if it involves 3D renderings. Simply, you will have to be careful and import the photos and videos by specifying the correct color space, that of the camera that captured the images, as soon as they are imported into the software that uses them. OCIO and ACES contain a large selection of color spaces from various camera models, so it is often easiest to use a chain with ACES for production, as described above.","title":"E.4 - Videos, live action and VFX"},{"location":"standards.html","text":"II.B - A few standards for files \u00b6 Here is a list of some standards associated with common files. Not all the technical possibilities for each type of file are listed here, only the most common (and standard) are indicated. While these lists are not necessarily official standards, these correspondences are standardized by usage. II.B - A few standards for files Image formats (Jpeg, PNG, TGA, TIFF\u2026) OpenEXR MP4 h.264 / h.265 / AVCHD / HEVC and other HD videos (2K) or UHD (4K) SWF (and others web formats) QuickTime Animation / RLE Image formats (Jpeg, PNG, TGA, TIFF\u2026) \u00b6 Space : sRGB Pixel format : RGB , Full Range Depth : Jpeg : 8 bpc PNG : 8 bpc , 16 bpc TGA : 8 bpc TIFF : 8 bpc , 16 bpc 1 Hint It\u2019s possible to see PNG HDR which use the color space Rec.2020 . OpenEXR \u00b6 Space: RGB linear , but it is also the official format for ACES , or Filmic from Blender . It is the only format that actually allows to store any existing color independently of the color space used to represent it, and therefore the preferred format to work with. Pixel format : RGB , Full Range YUV 4:4:4 or equivalent to YUV 4:2:0 with Luma/Chroma option, Full Range Depth : 8 bpc 16 bpc (integer or float) 32 bpc (integer or float) Hint The beauty of the EXR is that it can store colors in a floating format instead of the usual integer format, so there is no need to use integer versions of the depth. It is because of this \u201cfloat\u201d format that the EXR can store any color data without loss: it also stores colors outside the workspace (whose values are greater than 1 ). Note It should actually be understood that the color space in an EXR file doesn\u2019t matter: the file stores raw data, which isn\u2019t \u201cclamped\u201d (values above 1 , the theoretical maximum of a color space, are possible); an EXR file can therefore store colors outside the gamut * and of the luminosity * of its color space. MP4 h.264 / h.265 / AVCHD / HEVC and other HD videos (2K) or UHD (4K) \u00b6 Space : Rec.709 , Rec.2020 possible in UHD and more ( 4K ). Pixel format : YUV 4:2:0 ; although 4:2:2 and 4:4:4 , even RGB , are possible, they are not very standard and many players will not be able to read them. Limited Range Depth : 8 bits in Rec.709 10 bits in Rec.2020 UHD/4K 12 bits in Rec.2020 4K HDR SWF (and others web formats) \u00b6 Space : sRGB Pixel formats : RGB , Full Range Depth : 8 bpc Be careful, Adobe After Effects (at least some versions), imports it in Rec.709 instead of sRGB . It is then necessary to change the interpretation manually. QuickTime Animation / RLE \u00b6 Dpace: Rec.709 Pixel format : YUV 4:4:4 , Full Range Depth : 8 bpc Sources & References The TIFF allows many different color spaces, including CMYK and CIE Lab for example, and also the YUV format in a less standard way. \u21a9","title":"B - A few standards for files"},{"location":"standards.html#iib-a-few-standards-for-files","text":"Here is a list of some standards associated with common files. Not all the technical possibilities for each type of file are listed here, only the most common (and standard) are indicated. While these lists are not necessarily official standards, these correspondences are standardized by usage. II.B - A few standards for files Image formats (Jpeg, PNG, TGA, TIFF\u2026) OpenEXR MP4 h.264 / h.265 / AVCHD / HEVC and other HD videos (2K) or UHD (4K) SWF (and others web formats) QuickTime Animation / RLE","title":"II.B - A few standards for files"},{"location":"standards.html#image-formats-jpeg-png-tga-tiff","text":"Space : sRGB Pixel format : RGB , Full Range Depth : Jpeg : 8 bpc PNG : 8 bpc , 16 bpc TGA : 8 bpc TIFF : 8 bpc , 16 bpc 1 Hint It\u2019s possible to see PNG HDR which use the color space Rec.2020 .","title":"Image formats (Jpeg, PNG, TGA, TIFF...)"},{"location":"standards.html#openexr","text":"Space: RGB linear , but it is also the official format for ACES , or Filmic from Blender . It is the only format that actually allows to store any existing color independently of the color space used to represent it, and therefore the preferred format to work with. Pixel format : RGB , Full Range YUV 4:4:4 or equivalent to YUV 4:2:0 with Luma/Chroma option, Full Range Depth : 8 bpc 16 bpc (integer or float) 32 bpc (integer or float) Hint The beauty of the EXR is that it can store colors in a floating format instead of the usual integer format, so there is no need to use integer versions of the depth. It is because of this \u201cfloat\u201d format that the EXR can store any color data without loss: it also stores colors outside the workspace (whose values are greater than 1 ). Note It should actually be understood that the color space in an EXR file doesn\u2019t matter: the file stores raw data, which isn\u2019t \u201cclamped\u201d (values above 1 , the theoretical maximum of a color space, are possible); an EXR file can therefore store colors outside the gamut * and of the luminosity * of its color space.","title":"OpenEXR"},{"location":"standards.html#mp4-h264-h265-avchd-hevc-and-other-hd-videos-2k-or-uhd-4k","text":"Space : Rec.709 , Rec.2020 possible in UHD and more ( 4K ). Pixel format : YUV 4:2:0 ; although 4:2:2 and 4:4:4 , even RGB , are possible, they are not very standard and many players will not be able to read them. Limited Range Depth : 8 bits in Rec.709 10 bits in Rec.2020 UHD/4K 12 bits in Rec.2020 4K HDR","title":"MP4 h.264 / h.265 / AVCHD / HEVC and other HD videos (2K) or UHD (4K)"},{"location":"standards.html#swf-and-others-web-formats","text":"Space : sRGB Pixel formats : RGB , Full Range Depth : 8 bpc Be careful, Adobe After Effects (at least some versions), imports it in Rec.709 instead of sRGB . It is then necessary to change the interpretation manually.","title":"SWF (and others web formats)"},{"location":"standards.html#quicktime-animation-rle","text":"Dpace: Rec.709 Pixel format : YUV 4:4:4 , Full Range Depth : 8 bpc Sources & References The TIFF allows many different color spaces, including CMYK and CIE Lab for example, and also the YUV format in a less standard way. \u21a9","title":"QuickTime Animation / RLE"}]}